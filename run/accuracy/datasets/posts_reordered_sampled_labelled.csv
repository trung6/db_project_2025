,Body_PostId,PostDate,Text,manual_labels
4530,"Body: <p>I am trying to put together a data-mining package for StackExchange sites and in particular, I am stuck in trying to determine the ""most interesting"" questions. I would like to use the question score, but remove the bias due to the number of views, but I don't know how to approach this rigorously.</p>

<p>In the ideal world, I could sort the questions by calculating $\\frac{v}{n}$, where $v$ is the votes total and $n$ is the number of views. After all it would measure the percentage of people that upvote the question, minus the percentage of people that downvote the question.</p>

<p>Unfortunately, the voting pattern is much more complicated. Votes tend to ""plateau"" to a certain level and this has the effect of drastically underestimating wildly popular questions. In practice, a question with 1 view and 1 upvote would certainly score and be sorted higher than any other question with 10,000 views, but less than 10,000 votes.</p>

<p>I am currently using $\\frac{v}{\\log{n}+1}$ as an empirical formula, but I would like to be precise. How can I approach this problem with mathematical rigorousness?</p>

<p>In order to address some of the comments, I'll try to restate the problem in a better way:</p>

<p>Let's say I have a question with $v_0$ votes total and $n_0$ views. I would like to be able to estimate what votes total $v_1$ is most likely when the views reach $n_1$.</p>

<p>In this way I could simply choose a nominal value for $n_1$ and order all the question according to the expected $v_1$ total.</p>

<hr>

<p>I've created two queries on the SO datadump to show better the effect I am talking about:</p>

<p><a href=""http://data.stackexchange.com/stackoverflow/q/99605/"">Average Views by Score</a></p>

<p>Result:</p>

<p><img src=""http://i.stack.imgur.com/AjwgO.png"" alt=""Views by Score""></p>

<p><a href=""http://data.stackexchange.com/stackoverflow/q/99606/"">Average Score by Views (100-views buckets)</a></p>

<p>Result:</p>

<p><img src=""http://i.stack.imgur.com/2dVzr.png"" alt=""Score by Views""></p>

<hr>

<p><a href=""http://data.stackexchange.com/stackoverflow/q/99608/"">The two formulas compared</a> </p>

<p>Results, not sure if straighter is better: ($\\frac{v}{n}$ in blue, $\\frac{v}{log{n}+1}$ in red)</p>

<p><img src=""http://i.stack.imgur.com/v8RVc.png"" alt=""Formulas""></p>
PostId: 10295",PostDate: 2011-05-03 21:53:26.0,"Text: @Theo You may be right, actually. I'll flag for the mods to migrate if they think it's best.",NO
11984,"Body: <p>The <a href=""https://en.wikipedia.org/wiki/Standard_error"" rel=""nofollow"">Standard error</a> is an estimate of the variance of the strength of the effect, or the strength of the relationship between each causal variable and the predicted variable. If it's high, then the effect size will have to be stronger for us to be able to be sure that it's a real effect, and not just an artefact of randomness.</p>

<p>The <a href=""https://en.wikipedia.org/wiki/T-statistic"" rel=""nofollow"">t-statistic</a> is an estimate of how extreme the value you see is, relative to the standard error (assuming a normal distribution, centred on the null hypothesis).</p>

<p>The <a href=""https://en.wikipedia.org/wiki/P-value"" rel=""nofollow"">p-value</a> is an estimate of the probability of seeing a t-value as extreme, or more extreme the one you got, <em>if you assume that the null hypothesis is true</em>  (the null hypothesis is usually ""no effect"", unless something else is specified). So if the p-value is very low, then there is a higher probability that you're seeing data that is counter-indicative of zero effect. In other situations, you can get a p-value based on other statistics and variables. </p>

<p>Unfortunately, if that explanation of the p-value is confusing, that's because the entire concept is confusing. It's important to note that technically a low p-value does <em>not</em> show high probability of an effect, although it may indicate that. Have a read of some of the <a href=""http://stats.stackexchange.com/questions/tagged/p-value?sort=votes&amp;pagesize=50"">high-voted p-value questions</a>, to get an idea about what's going on here.</p>
PostId: 59252",PostDate: 2013-05-17 00:36:36.0,"Text: @godzilla For t-values, the most simple explanation is that you can use 2 (as a rule of thumb) as the threshold to decide whether or not a variable is statistically significant. Above two and the variable is statistically significant and below zero is not statistically significant. For an easy treatment of this material see Chapter 5 of Gujarati's Basic Econometrics. The ucla link I provided in another comment explains how interpret the p value. I assume its the interpretation of the output for practical use that you want rather than the actual underlying theory hence my oversimplification.",YES
12322,"Body: <p>The given data below shows the time in intervals in seconds between successive white cars in flowing traffic in an open road.Can these be modeled by an exponential distribution.</p>

<pre><code>Time        0-     20-      40-     60-      90-     120-180
Frequency   41     19       16      13        9        2
</code></pre>

<p>My question is when I calculate Expected frequencies though the total should add up to 100 it does not do so because of <strong>rounding off</strong> errors right?<br>
I get a value of 98.89.<br>
Here should I need to add another category myself as the interval being <strong>180-infinity</strong> and get its expected value as 100-98.89=1.11.
Is it necessary to add this category as this is an <strong>exponential distribution</strong> which goes for infinity?<br>
Of course as the Expected value of the created category is less than 5, in this case I have to add this up with the upper category.<br>
But if  I do not consider this new category then the degrees of freedom changes.Is it necessary to add this new category as 180-infinity</p>
PostId: 78765",PostDate: 2013-12-06 14:50:37.0,"Text: The final question about whether to add the $(180,\\infty)$ bin is interesting. It turns out not to matter much, because the $\\chi^2$ statistic barely changes. It creates problems, though, because the expected count in that bin is only $0.96$: too low for the $\\chi^2$ distribution to be a good approximation to the statistic. Thus, a permutation test would need to be applied to compute the p-value. You're better off just considering the last bin to extend from $120$ to $\\infty$.",YES
12083,"Body: <p>The Mann-Whitney or Wilcoxon test compares two groups while the Kruskal-Wallis test compares 3.  Just like in the ordinary ANOVA with three or more groups the procedure generally suggested is to do the overall ANOVA F test first and then look at pairwise comparisons in case there is a significant difference.  I would do the same here with the nonparametric ANOVA.  My interpetation of your result is that there is marginally a significant difference between groups at level 0.05 and if you accept that then the difference based on the Mann-Whitney test indicates that it could be attributed to g$_1$ and g$_2$ being significantly different.</p>

<p>Don't get hung up with the magic of the 0.05 significance level!  Just because the Kruskal-Wallis test gives p-value slightly over 0.05, don't take that to mean that there is no statistically significant difference between the groups.  Also the fact that the Mann-Whitney test gives a p-value for the difference between g$_1$ and g$_2$ a little below 0.03 does not somehow make the difference between the two groups highly significant.  Both p-values are close to 0.05.  A slightly different data set could easily change to Kruskal-Wallis p-value by that much.</p>

<p>Any thought you might have that the results are contradictory would have to come from thinking of a 0.05 cut off as black and white boundary with no gray area in the neighborhood of 0.05.  I think these results are reasonable and quite compatible.</p>
PostId: 34024",PostDate: 2012-08-09 21:36:50.0,"Text: The more classic view is that you failed to find statistical significance with your first test, so you should not report (in a professional publication) any further tests as statistically significant indications of between group differences.  To do so is to use an alpha other than .05.  This is particularly problematic (from the classical view) because you did not choose the higher alpha before conducting the test, so your alpha is unknown.  Of course, when you try to understand your data, to guide your own future research program, you can take note of the difference between groups 1 and 2.",YES
13403,"Body: <p>This is not an easy thing, even for respected statisticians.  Look at one recent attempt by <a href=""http://fivethirtyeight.blogs.nytimes.com/2010/09/29/the-uncanny-accuracy-of-polling-averages-part-i-why-you-cant-trust-your-gut/"">Nate Silver</a>:</p>

<blockquote>
  <p>... if I asked you to tell me how often your commute takes 10 minutes longer than average — something that requires some version of a confidence interval — you’d have to think about that a little bit, ...</p>
</blockquote>

<p>(from the <em>FiveThirtyEight</em> blog in the New York Times, 9/29/10.)  This is <em>not</em> a confidence interval.  Depending on how you interpret it, it's either a tolerance interval or a prediction interval.  (Otherwise there's nothing the matter with Mr. Silver's excellent discussion of estimating probabilities; it's a good read.)  Many other web sites (particularly those with an investment focus) similarly confuse confidence intervals with other kinds of intervals.</p>

<p>The New York Times has made efforts to clarify the meaning of the statistical results it produces and reports on.  The fine print beneath many polls includes something like this:</p>

<blockquote>
  <p>In theory, in 19 cases out of 20, results based on such samples of all adults will differ by no more than three percentage points in either direction from what would have been obtained by seeking to interview all American adults. </p>
</blockquote>

<p>(<em>e.g.</em>, <a href=""http://www.nytimes.com/2011/05/03/business/economy/03method.html"">How the Poll Was Conducted</a>, 5/2/2011.)</p>

<p>A little wordy, perhaps, but clear and accurate: this statement characterizes the variability of the <em>sampling distribution</em> of the poll results.  That's getting close to the idea of confidence interval, but it is not quite there.  One might consider using such wording in place of confidence intervals in many cases, however.</p>

<p>When there is so much potential confusion on the internet, it is useful to turn to authoritative sources.  One of my favorites is Freedman, Pisani, &amp; Purves' time-honored text, <em>Statistics.</em>  Now in its fourth edition, it has been used at universities for over 30 years and is notable for its clear, plain explanations and focus on classical ""frequentist"" methods.  Let's see what it says about interpreting confidence intervals:</p>

<blockquote>
  <p>The confidence level of 95% says something about the sampling procedure...</p>
</blockquote>

<p>[at p. 384; all quotations are from the third edition (1998)].  It continues,</p>

<blockquote>
  <p>If the sample had come out differently, the confidence interval would have been different. ... For about 95% of all samples, the interval ... covers the population percentage, and for the other 5% it does not.</p>
</blockquote>

<p>[p. 384].  The text says much more about confidence intervals, but this is enough to help: its approach is to move the focus of discussion onto the <em>sample,</em> at once bringing rigor and clarity to the statements.  We might therefore try the same thing in our own reporting.  For instance, let's apply this approach to describing a confidence interval of [34%, 40%] around a reported percentage difference in a hypothetical experiment:</p>

<blockquote>
  <p>""This experiment used a randomly selected sample of subjects and a random selection of controls.  We report a confidence interval from 34% to 40% for the difference.  This quantifies the reliability of the experiment: if the selections of subjects and controls had been different, this confidence interval would change to reflect the results for the chosen subjects and controls.  In 95% of such cases the confidence interval would include the true difference (between <em>all</em> subjects and <em>all</em> controls) and in the other 5% of cases it would not.  Therefore it is likely--but not certain--that <em>this</em> confidence interval includes the true difference: that is, we believe the true difference is between 34% and 40%.""</p>
</blockquote>

<p>(This is my text, which surely can be improved: I invite editors to work on it.)</p>

<p>A long statement like this is somewhat unwieldy.  In actual reports most of the context--random sampling, subjects and controls, possibility of variability--will already have been established, making half of the preceding statement unnecessary.  When the report establishes that there is sampling variability and exhibits a probability model for the sample results, it is usually not difficult to explain a confidence interval (or other random interval) as clearly and rigorously as the audience needs.</p>
PostId: 11873",PostDate: 2011-06-13 14:14:13.0,"Text: This is rather confusing. Anne, I suggest you clarify your question... That fact that you used $s$, and thus a T-test (not a normal test) doesn't change the interpretation of Confidence Interval. And when we talk about difference of means, we are refereing about the difference of the mean of two groups (say, mean wages of men and women). And remember that tha whole idea of confidence interval is to provide how uncertain we are about our inferencital tasks, i.e., to infer from the sample the populational value of interest.",YES
1546,"Body: <p><strong>Update</strong></p>

<p>In light of your comments, here is an updated answer:</p>

<p><strong>Approach 1: Difficult to implement/analyze</strong></p>

<p>Consider the simple case of $k$ = 3 and $n$ = 2. In other words you toss 3 coins (with probabilities $p_1$, $p_2$ and $p_3$). Then, the required mass function for the above case is:</p>

<p>$p_1 p_2 (1-p_3) + p_1 (1-p_2) p_3  + (1-p_1) p_2 p_3$</p>

<p>The above reduces to the <a href=""http://en.wikipedia.org/wiki/Binomial_distribution"" rel=""nofollow"">binomial</a> if the probabilities $p_i$ are all identical. </p>

<p>In the general case, you will have ${k \\choose n}$ terms where each term is unique with a structure similar to the one above.</p>

<p><strong>Approach 2: Easier to analyze/implement</strong></p>

<p>Instead of the above, you could model each $X_i$ as a <a href=""http://en.wikipedia.org/wiki/Bernoulli_trial"" rel=""nofollow"">bernoulli variable</a> with probability $p_i$. You could then assume that $p_i$ follows a <a href=""http://en.wikipedia.org/wiki/Dirichlet_distribution"" rel=""nofollow"">dirichlet distribution</a>.</p>

<p>You would then estimate the model parameters by constructing the posterior distribution for $p_i$ conditional on observing $n$ successes.</p>

<p><strike>If you can normalize by n and and assuming that treating them as probabilities/proportions makes sense in your context you can use the <a href=""http://en.wikipedia.org/wiki/Dirichlet_distribution"" rel=""nofollow"">dirichlet distribution</a>.</strike></p>
PostId: 1094",PostDate: 2010-08-01 22:38:49.0,"Text: In that case, you have basically the following scenario: You flip a coin $k$ times and your constraint is that you need $n$ successes as that will ensure that $X_i$ sum to $n$. This is a binomial distribution. If this satisfies your requirements I will change my answer. If it does not can you explain why this will not work?",YES
11664,"Body: <p>Suppose I bootstrap the distribution of the sample mean. Normally, one would use the mean of the bootstrapped distribution as point estimate of the parameter and the s.d. as its standard error. The mean of the bootstrapped distribution is asymptotically equal to the sample estimate (i.e. for a large number of iterated draws).</p>

<p>Now suppose the mean, or more generally, parameter, has an asymetrical bootstrap distribution, so that the sample estimate of the parameter and the mean of the bootstrap distribution are likely to be unequal for a moderate amount of iterated draws. Should I still expect both to be asympotically equivalent, so as I increase the number of draws, both will be equal? And if this is so, is it customary practice to increase the number of iterations until they are equal before I report statistics?</p>

<p>In my practical case, both deviate after 1000 iterated draws. So I am unsure whether I should report the sample estimate or the mean of the bootstrap distribution of the parameter.</p>
PostId: 79208",PostDate: 2013-12-10 14:35:17.0,"Text: The number of bootstrap samples to use is not one that gives equal results; that is necessarily elusive and in any case what does ""equal"" mean here? Choosing that number is, admittedly, an art as well as a science. Look at the literature in your field to see what is standard.",YES
8486,"Body: <p>I'm trying to predict the frequency of buying a product based on data I have. I have two things to consider: </p>

<p>1- Predicting the success of buying this product based on frequencies (0: Failed ;  >0: success)</p>

<p>2- Predicting the time of  buying this product </p>

<p>I could build a model to predict the success of buying this product, but I need to calculate also the time (how successful is the Hour/Day/Month/Year) ? </p>

<p>If I can calculate also the time with the same model, I would have two variables (Predicting_the_success,Predicting_the_time) </p>

<p>I am wondering now, I can't apply this formula as I am not sure if the both variables are not independent.</p>

<blockquote>
  <p>Total_Proba_Success = Predicting_the_success * Predicting_the_time</p>
</blockquote>

<p>A professor of mine said that I need to think about using regression here, What do you suggest as a model to solve this problem ?</p>

<p><strong>UPDATE 3</strong></p>

<p>I have predicted the success of buying this product based on frequencies (0: Failed ;  >0: success).</p>

<p>Based on comment of Gavin: 
I should turn this predicted values to discrete prediction say >= 0.5 == Buy, &lt; 0.5 =</p>

<p>Let's say that I have proba </p>

<p>P : calculate discrete prediction of buying product success. </p>

<p>Now I need to calculate based on that P, the probability of buying an item based on hour/day/month. </p>

<p>Gavin suggested to use Binomial GLM: ""binomial GLM, where the response is a vector of 0s and 1s (indicating product not bought and product bought respectively) and then vectors of the hour of day, day of week, and month of year, for each observation you made. The estimated coefficients from such a model will tell you the effect in terms of log odds (if a logit link is used) for each of those three predictors""</p>

<p>I am confused! - I found this challenging, in my dataset, time is mentioned only in items where product bought. So, I ended-up using only time of products that have been bought, while in response, he mentioned that I should use a vector of 0s and 1s (indicating product not bought and product bought respectively). </p>

<ul>
<li>Should I calculate all the hours between two dates ? and include it in my model. </li>
</ul>

<p>let's say for eg: 
01-01-2014 -> 01-02-2014. </p>

<pre><code>-+--------+-----------+--------+-------------------------+
   hour       day       Month     number products bought  
-+--------+-----------+--------+-------------------------+
   12:00     01           01             2
</code></pre>

<p>In this case: should I transform my model to include all the hours that haven't been mentioned with frequencies 0.</p>

<p>I mean, if we only use time of items that have been bought, we should consider another response vector ?</p>

<p>Last thing, why do we use GLM Binomial instead of using Time-series prediction models? is it because we have ""discrete times"" and not ""continuous times"".</p>

<pre><code>-+--------+-----------+--------+-------------------------+
   hour       day       Month     number products bought  
-+--------+-----------+--------+-------------------------+
   00:00     01           01             00
   01:00     01           01             00
   02:00     01           01             00
   03:00     01           01             00
   04:00     01           01             00
   05:00     01           01             00
   06:00     01           01             00
   07:00     01           01             00
   08:00     01           01             00
   09:00     01           01             00
   10:00     01           01             00
   11:00     01           01             00
   12:00     01           01           **02**
   13:00     01           01             00
   14:00     01           01             00
   15:00     01           01             00
   16:00     01           01             00
   17:00     01           01             00
   18:00     01           01             00
   19:00     01           01             00
   20:00     01           01             00
   21:00     01           01             00
   22:00     01           01             00
   23:00     01           01             00
</code></pre>

<p><strong>UPDATE 4</strong></p>

<p>Based on Gavin's comment:  ""model the number of products bought as the response with hour, day, and month as predictors in a Poisson GLM""</p>

<p>Just to confirm: </p>

<ul>
<li>Should we model the problem separately ? </li>
</ul>

<p>For hours:   </p>

<pre><code>   Predictor      response 
-+------------+-------------------------+
   hour          number products bought  
-+------------+-------------------------+
   00:00               01
   01:00               00
</code></pre>

<p>For days:</p>

<pre><code>  Predictor      response 
-+------------+-------------------------+
   day         number products bought  
-+------------+-------------------------+
   00               03
   01               01
</code></pre>

<p>For months: </p>

<pre><code>  Predictor      response 
-+------------+-------------------------+
   month         number products bought  
-+------------+-------------------------+
   00               20
   01               12
</code></pre>

<p>or should I do it this way </p>

<pre><code>    Predictors                 +  response
-+--------+-----------+--------+-------------------------+
   hour       day       Month     number products bought  
-+--------+-----------+--------+-------------------------+
   00:00     01           01             00
   01:00     02           01             02
</code></pre>

<p>I have data of two years; if I do it this way hour-day-month as predictor, I will end-up having 2 values for everyone. which is not enough to predict -based on what I've learnt from other comments, the more data we have, the most accurate-.</p>

<p>Another comment : 
Should I consider holidays. if I model it by Hour-day --> response 
Thursday 14:00 ==> 12 : 'cause it's Thursday 1st may (most workers are in holidays)
Thursday 14:00 ==> 01 : 'cause it's Thursday 8th may (most of ppl working at this time )</p>

<p>After thinking about it; I am seeing that modeling the problem can make sense this way :</p>

<p>Option1 - GLM Poisson : </p>

<pre><code>    Predictors                 +  response
-+--------+-----------+--------+------------+-------------------------+
   hour       day       month    isHolidays     number products bought  
-+--------+-----------+--------+------------+-------------------------+
   00:00     01           00       00          00
   01:00     02           01       01          02
</code></pre>

<p>isHolidays: based on national holidays.</p>

<ul>
<li>Strength : It gives us all the  details</li>
<li>Weaknesses : we have a lot of cases, and for  ( hour-day-month) we have 3 cases for everyone as we have data of three years.</li>
</ul>

<p>Option2 - GLM Poisson : </p>

<p>Assuming that months doesn't make difference</p>

<pre><code>    Predictors                 +  response
-+--------+----------+-------------------------+
   hour       day        number products bought  
-+--------+----------+-------------------------+
   00:00     01           01      
   20:00     06           21      
</code></pre>

<ul>
<li>Strength : we have many samples of data for every case. We don't need info about months as product is not seasonly (people need it during the whole year).</li>
<li>Weaknesses : less details, Assuming that months doesn't make difference is little bit wrong, as we have people who are traveling during summer. </li>
</ul>

<p>Option3 - GLM Binomial : </p>

<pre><code>-+--------+-----------+--------+-------------------------+
   hour       day       Month     number products bought  
-+--------+-----------+--------+-------------------------+
   00:00     01           01             00
   01:00     01           01             00
   02:00     01           01             00
   03:00     01           01             00
   04:00     01           01             00
</code></pre>

<p>I should add all the -possible- times whether product have been bough or no. then select time where product has been bough by using 1, 0 for no.</p>

<p>Now, what is the most logical model here. They all have weaknesses, what do you recommend ?</p>
PostId: 95854",PostDate: 2014-04-30 19:17:40.0,"Text: @user3378649 It's not clear; from the comments I thought you wanted to know about the probability that they buy a product on a Tuesday, or a Thursday, or at 10-11pm. But the question seems to ask something different; is the number of sales related to the time of day, day of week etc. So which is it? Rather than add the info in the comments, can you edit your question to better explain what you want to do, what data you have etc.?",YES
14728,"Body: <p>Your first sentence is not necessarily correct. First off, an increase in numbers of parameters does indeed increase the degrees of freedom and the standard errors of point estimates (hence their degree of generalization). An example of this is the nested classes of Exponential and Weibull models. It's not universally agreed upon that ""model complexity"" necessarily means the parameter space, though, but it is a good place to start for discussion.</p>

<p>Semiparametric and nonparametric inference make overfitting a nonissue by generalizing the likelihood function into a new type of function where such extraneous parameters are ancillary. The only caveat is that the statistician has to correctly identify such models. Examples of such extended likelihoods are conditional likelihood (in mixed modeling), partial likelihood (in Cox models), pseudo likelihood (forgetting some applications for that...), profile likelihood, quasilikelihood, (and the list goes on). The parameter spaces for such likelihoods are seen as <em>projections</em> of high (possibly infinte) dimensional (compact) parameter spaces. </p>

<p>It's only in fully parametric inference where every causal relationship needs to be specified, such as the correlation structure for teeth within a mouth, or the correlation between failure times in a prospective cohort among denominators of individuals counted more than once. Many of these likelihoods are overly complex or intractable hence inference about them is non-existent or otherwise not popular.</p>

<p>Modeling processes is a fully parametric endeavor. You must be able to simulate data from an estimated data generating mechanism. SP/NP often cannot achieve this. Neither can the produce fitted effects nor can they claim to simulate realizations from any data generating process. SP/NP focuses on the point estimation of a specific parameter and efficiently calculates estimates and standard errors for that parameter cancelling out all other parameters in the data generating process through either conditioning, estimating them as nuisance parameters, or some other process.</p>

<p>SP/NP inference examples are the log-rank test (NP), the plain vanilla asymptotic t-test without normality assumptions (NP), conditional logistic regression (SP), generalized estimating equations (GEE), and Cox proportional hazards models (SP).</p>

<p>Examples where semi-parametric inference breaks down is in the case of missing at random data (as opposed to missing completely at random data), where the value of some observed outcome or covariate depends on the things which we deemed to be ancillary (such as informative censoring in Cox models). A fully likelihood based survival analysis would require separate models (and their correlation) for survival and censoring outcomes.</p>
PostId: 74173",PostDate: 2013-10-30 20:30:32.0,"Text: The ML models can be formalized as nonparametric using sieves (which to an extent is similar to how they are used) I think, combined with universal approximation I'd say nonparametric is a fine label.",YES
13446,"Body: <p>This is the solution I came up with for this problem, based on some research and the answers so far. Sorry in advance for the length. I wanted to provide an in-depth explanation of my approach so I can be sure I didn't miss anything important.</p>

<p>I'll start with a little more detail about my specific situation. I've got a database that contains records of jobs, clients, and workers. My goal is to see if a given worker has a lower than average score when it comes to converting first-time jobs into recurring jobs.</p>

<p>I do this by taking all first-time jobs that a given worker has been assigned to, and I then check to see if any subsequent jobs exist for the same client. If there were subsequent jobs the worker gets a 1, otherwise they get a 0. This gives me something that looks like this: </p>

<pre><code>Worker 1 [48 total records (first-time jobs)]
0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
--
Worker 2 [56 total records (first-time jobs)]
1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
</code></pre>

<p>I then take this data and calculate the mean. I do this by counting the total number of first-time jobs in my system (2,925), as well as the sum of the 1's and 0's. This gives me a mean of 0.38 (so 38% of all first-time jobs typically become recurring jobs). I then calculate the standard deviation, which in this case is 0.13.</p>

<p>I then look at each worker who has completed a minimum of 30 first-time jobs. This is so that I only analyze workers with a sufficiently large sample size, in order to increase confidence in the results.</p>

<p>I then come up with a mean score for each of these workers (the sum of the 1's and 0's, divided by the total number of first-time jobs). Finally, I convert this into a z score, which I do by subtracting the mean (for all the data) from the worker's mean score. I then divide the result by the standard deviation to obtain the individual z score. Finally, I put the results in a bar chart, which looks like this: </p>

<p><img src=""http://i.stack.imgur.com/YE9E5.gif"" alt=""Bar chart demo""> </p>

<p>The dark bars are for any result with a z-score above 1 or below -1.</p>

<p>That's pretty much it. I guess my questions are as follows:</p>

<ol>
<li>Is this the correct approach, given my goals?</li>
<li>Does it make sense to limit this analysis to workers with a minimum of 30 records? I know that generally the larger sample size the better, but would it work to apply this analysis to workers with only 20 records, for example? </li>
<li>Finally, what z-scores should I consider significant? Right now I am focusing on anything greater than 1, but is that too low?</li>
</ol>

<p>I greatly appreciate any input. Thank you.</p>
PostId: 34542",PostDate: 2012-08-17 19:21:32.0,"Text: Q1. Is this the correct approach, given my goals?
A1. This seems like a plausible approach, but the percentages may have different standard deviations depending on the number of first time jobs done by a worker.  So you might get better behaved data if you analyzed the same number of jobs per worker.  Also, if you think that your workers are poaching jobs it might be that you would get more long runs of zeros and fewer long runs of ones.  You might be able to test that using a runs test.",YES
12486,"Body: <p>The problem starts with your sentence :</p>

<blockquote>
  <p>Examples based on incorrect prior
  assumptions are not acceptable as they
  say nothing about the internal
  consistency of the different
  approaches.</p>
</blockquote>

<p>Yeah well, how do you know your prior is correct? </p>

<p>Take the case of Bayesian inference in phylogeny. The probability of at least one change is related to evolutionary time (branch length t) by the formula </p>

<p>$$P=1-e^{-\\frac{4}{3}ut}$$</p>

<p>with u being the rate of substitution. </p>

<p>Now you want to make a model of the evolution, based on comparison of DNA sequences. In essence, you try to estimate a tree in which you try to model the amount of change between the DNA sequences as close as possible. The P above is the chance of at least one change on a given branch. Evolutionary models describe the chances of change between any two nucleotides, and from these evolutionary models the estimation function is derived, either with p as a parameter or with t as a parameter. </p>

<p>You have no sensible knowledge and you chose a flat prior for p. This inherently implies an exponentially decreasing prior for t.  (It becomes even more problematic if you want to set a flat prior on t. The implied prior on p is strongly dependent on where you cut off the range of t.)</p>

<p>In theory, t can be infinite, but when you allow an infinite range, the area under its density function equals infinity as well, so you have to define a truncation point for the prior.  Now when you chose the truncation point sufficiently large, it is not difficult to prove that both ends of the credible interval rise, and at a certain point the true value is not contained in the credible interval any more. Unless you have a very good idea about the prior, Bayesian methods are not guaranteed to be equal to or superior to other methods.</p>

<p>ref: Joseph Felsenstein : Inferring Phylogenies, chapter 18</p>

<p>On a side note, I'm getting sick of that Bayesian/Frequentist quarrel. They're both different frameworks, and neither is the Absolute Truth. The classical examples pro Bayesian methods invariantly come from probability calculation, and not one frequentist will contradict them. The classical argument against Bayesian methods invariantly involve the arbitrary choice of a prior. And sensible priors are definitely possible.</p>

<p>It all boils down to the correct use of either method at the right time. I've seen very few arguments/comparisons where both methods were applied correctly. Assumptions of any method are very much underrated and far too often ignored.</p>

<p>EDIT : to clarify, the problem lies in the fact that the estimate based on p differs from the estimate based on t in the Bayesian framework when working with uninformative priors (which is in a number of cases the only possible solution). This is not true in the ML framework for phylogenetic inference. It is not a matter of a wrong prior, it is inherent to the method.</p>
PostId: 2365",PostDate: 2010-09-03 20:24:52.0,"Text: @Joris, as I said a flat prior is NOT NECCESSARILY UNINFORMATIVE. Consider this, if two priors give different results, then the must logically represent a different state of prior knowledge (see early chapters of Jaynes book that set out desiderata for Baysian inference).  Therefore the ""flat p"" prior and ""flat t"" prior cannot both be uninformative.  Felsenstein may be an expert on phylogenetic inference, but it is possible that he is not an expert on Bayesian inference. If he states that two priors giving different results are both uninformative, he is at odds with Jaynes (who certailny was).",NO
4270,"Body: <p>I am planning a study to collect data of patients dying after brain infection versus number of cases for the next 5 years in 2 cities where they manage the disease with different drugs.</p>

<ol>
<li>Will a <strong><em>year wise t-test be better or year wise Relative Risk of death and meta analysis</em></strong> be better to compare the results of the study?</li>
<li>I am a beginner in statistics. A word of explanation would benefit me.</li>
</ol>

<p>The data would look like</p>

<pre><code>Year     City 1 Deaths    City 1 cases  City 2 Deaths   City 2 cases
2011     237                1124             10            1226
2012     228                1030             26           1181
2013     1500              6061             10           1122
2014     528                2320             32           1173
2015     645                3024             11           1232
</code></pre>
PostId: 8278",PostDate: 2011-03-14 17:53:57.0,Text: @whuber: Thank you for making the question better by editing it,YES
6382,"Body: <p>I have hesitated to wade into this discussion, but because it seems to have gotten sidetracked over a trivial issue concerning how to express numbers, maybe it's worthwhile refocusing it.  A point of departure for your consideration is this:</p>

<blockquote>
  <p>A probability is a hypothetical property.  Proportions summarize observations.</p>
</blockquote>

<p>A <a href=""http://en.wikipedia.org/wiki/Probability_interpretations#Frequentism"" rel=""nofollow"">frequentist</a> might rely on laws of large numbers to justify statements like ""the long-run proportion of an event [is] its probability.""  This supplies meaning to statements like ""a probability is an expected proportion,"" which otherwise might appear merely tautological.  Other interpretations of probability also lead to connections between probabilities and proportions but they are less direct than this one.</p>

<p>In our models we usually take probabilities to be <em>definite</em> but <em>unknown.</em>  Due to the sharp contrasts among the meanings of ""probable,"" ""definite,"" and ""unknown"" I am reluctant to apply the term ""uncertain"" to describe that situation. However, before we conduct a sequence of observations, the [eventual] proportion, like any future event, is indeed ""uncertain"".  <em>After</em> we make those observations, the proportion is both <em>definite</em> and <em>known.</em>  (Perhaps this is what is meant by ""guaranteed"" in the OP.)  Much of our knowledge about the [hypothetical] probability is mediated through these uncertain observations and informed by the idea that <em>they might have turned out otherwise.</em>  In <em>this</em> sense--that uncertainty about the observations is transmitted back to uncertain knowledge of the underlying probability--it seems justifiable to refer to the probability as ""uncertain.""</p>

<p>In any event it is apparent that probabilities and proportions function differently in statistics, despite their similarities and intimate relationships.  It would be a mistake to take them to be the same thing.</p>

<h3>Reference</h3>

<p>Huber, WA <a href=""http://onlinelibrary.wiley.com/doi/10.1111/j.1539-6924.2010.01361.x/abstract"" rel=""nofollow""><em>Ignorance is Not Probability</em></a>.  <strong>Risk Analysis</strong> Volume 30, Issue 3, pages 371–376, March 2010.</p>
PostId: 4850",PostDate: 2010-11-23 22:38:38.0,"Text: I would say that a long run proportion is a hypothetical property while a probability is not,  it is a statement about information. Say someone gives me a coin, tells me that it is *not* fair and flips it. What is the probability of the coin coming up heads? The hypothetical long run proportion is *not* 50% heads. Given the information I have I must still state that the probability of heads is 50% for this one flip as I haven't been told which way the coin is biased.",YES
2993,"Body: <p>Erm, for the (obvious?) reason that the fractional representation of a number is not unique, the medians can't be the same. </p>

<p>EDIT: as per Glen's request</p>

<p>The location of the median relies on the ordering of numbers in your set. Suppose you order your numbers from smallest to largest, so you have something like this {1,2,3}. You can maintain the same median if you perform a transformation only if that transformation preserves the ordering. For example, if you add 1 to every single number in your set, location of the median doesn't change: {2, 3, 4}, i.e. it is still located in the second position. </p>

<p>Any <em>linear</em> transformation maintains the order. The maintenance of order is key. That's the ""property of mathematics"" that's really being referred to below. (And how you define order is also key. Note that order is essentially a notion of distance. 2 is closer to 3 than to 4 because the distance between 2 and 3 is smaller than the distance between 2 and 4). That's why, for positive data, in some instances, it is allowed to apply a log transform of your data - you haven't fundamentally altered the ordering of the numbers, and thus you haven't changed the underlying relationship between your variables. You can do a log transform for say, income data, but you can't do it for inflation data.</p>

<p>If a transformation is not linear, the order is not necessarily maintained. Transforming every number into a fraction is not a linear transformation because the fractional representation of a number is not unique. 1/2 is the same as 2/4. That's why for ""large"" sets the location of the median changes if you transform your numbers into fractions in the way that you described. For large enough sets, you're eventually going to run into a situation where you have the same fraction in multiple places.If that happens, your set ""shrinks"" and so the median must change.</p>
PostId: 94734",PostDate: 2014-04-22 17:46:00.0,Text: @Glen_b Done. Hope it helps.,YES
8540,"Body: <p>I'm using R to develop regression models, and I need to compare two different models' performance. The question that arises is, ""Is Model 1 statistically better than model 2?"" and I don't seem to have a way to answer that question.</p>

<p>Background: Model 1 consists of Variable A regressed on the endpoint. Model 2 consists of Variables B, C, and D regressed on the endpoint. Both models are developed using lm - ordinary least squares, nothing too fancy here.</p>

<p>Given that these are not nested models, I cannot compare them using ANOVA.</p>

<p>I can look at the R2 of actual vs predicted for each model, and I see that Model 2 is better, but how do I determine if it is statistically significantly better?</p>

<p>I've also used the Concordance Correlation Coefficient, but again, I can't find a way to prove significance. The best I've come up with is that the rho for Model 2 is better than Model 1, but that rho value is within the 95% confidence limits of the rho for Model 1.</p>

<p>I should throw in there that my assessment of predicted vs actual has been on a 60 observation hold-out set (240 observations in the training set).</p>
PostId: 79830",PostDate: 2013-12-16 16:45:11.0,"Text: @TaylerJones:  OK...how can I determine that the two models are different in their ability to predict, and can I assign a p-value to the assertion that the models are different with accuracy rates being higher or lower in one versus the other?",YES
11929,"Body: <p>The ""at least 30"" rule is addressed in <a href=""http://stats.stackexchange.com/questions/2541/is-there-a-reference-that-suggest-using-30-as-a-large-enough-sample-size"">another posting</a> on Cross Validated. It's a rule of thumb, at best.</p>

<p>When you think of a sample that's supposed to represent millions of people, you're going to have to have a much larger sample than just 30. Intuitively, 30 people can't even include one person from each state! Then think that you want to represent Republicans, Democrats, and Independents (at least), and for each of those you'll want to represent a couple of different age categories, and for each of those a couple of different income categories.</p>

<p>With only 30 people called, you're going to miss huge swaths of the demographics you need to sample.</p>

<p>EDIT2: [I've removed the paragraph that abaumann and StasK objected to. I'm still not 100% persuaded, but especially StasK's argument I can't disagree with.] If the 30 people are truly selected completely at random from among all eligible voters, the sample would be valid in some sense, but too small to let you distinguish whether the answer to your question was actually true or false (among all eligible voters). StasK explains how bad it would be in his third comment, below.</p>

<p>EDIT: In reply to samplesize999's comment, there is a formal method for determining how large is large enough, called ""<a href=""http://en.wikipedia.org/wiki/Statistical_power"" rel=""nofollow"">power analysis</a>"", which is also <a href=""https://www.statsoft.com/Textbook/Power-Analysis"" rel=""nofollow"">described here</a>. abaumann's comment illustrates how there is a tradeoff between your ability to distinguish differences and the amount of data you need to make a certain amount of improvement. As he illustrates, there's a square root in the calculation, which means the benefit (in terms of increased power) grows more and more slowly, or the cost (in terms of how many more samples you need) grows increasingly rapidly, so you want enough samples, but not more.</p>
PostId: 87731",PostDate: 2014-02-24 22:29:19.0,"Text: @samplesize999, consider a situation where you're randomly sampling a Bernoulli-distributed variable with Pr(X=1)=p. The standard error is the $\\sqrt{\\frac{p(1-p)}{n}}$. With p = 0.25, going from n = 2,000 to n = 100,000 would reduce the s.e. from 1.12% to 0.16% while multiplying the cost by 50!",YES
8348,"Body: <p>I'm simulating a random collision process. At each time interval I calculate the probability of a collision occurring between each object and all other objects in proximity. Currently if I wish to find the total probability of a specific object being in a collision with any other object it is the sum of all these probabilities. However if I were to theoretically increase the number of objects to orders of magnitude higher then this form of calculation could result in a probability of greater than 1.</p>

<p>The following is my current thinking on this:</p>

<p>I calculate the probability of object A colliding with object B and get a value x1. The probability of object A colliding with object C must be less than 1 since object A could collide with object B, therefore the calculated probability between objects A and C should be multiplied by (1-x1), and this should be my x2 value. Is this a good approach?</p>

<p>If I do apply the above approach does it not more heavily weigh the likelihoods of objects earlier in the list (eg. x1>x2>x3>x4 etc.). Clearly x1 is not necessarily greater and x2, but given identical parameters x1 would be greater. How would I negate this effect since the order shouldn't change the results? Thanks!</p>

<p>Some notes from discussion with <a href=""http://stats.stackexchange.com/users/919/whuber"">Whuber</a>:</p>

<ul>
<li>I calculate the values based on the spatial density of each object, the relative velocities of the objects, the cross sectional area and the volume in which they are within.</li>
<li>It seemed to me to be logical to assume this since a collision is not one sided (A colliding with B is treated exactly the same as B colliding with A).</li>
<li>Collision are independent in one step but if A is in a collision with one object it can not be in a second collision.</li>
<li>Three objects can not collide together.</li>
<li>Once an object has been in a collision it is removed from the population.</li>
<li>This is also computed in a finite volume model - eg. each object is within a given volume for a specific step but it may or may not be in the same volume in the following step.</li>
<li>Probabilities between A and B and A and C are typically within the same order of magnitude.</li>
<li>The simulation predicts multiple collisions, but since an object is to be removed once a collision occurs this should not be possible.</li>
</ul>
PostId: 108761",PostDate: 2014-07-21 20:32:49.0,Text: @whuber. There's a second point I neglected to mention in the previous comment. Once an object has been in a collision it is removed from the population. To be frank the time span is finite but I'm actually investigating different values for the time. This is also computed in a finite volume model - eg. each object is within a given volume for a specific step but it may or may not be in the same volume in the following step.,YES
1197,"Body: <p><strong>Background:</strong>
I've modeled a project effort prediction as a Google Spreadsheet template. Details of the Model: <a href=""http://sites.google.com/site/effortprediction/methodology"" rel=""nofollow"">http://sites.google.com/site/effortprediction/methodology</a>
.Google Spreadsheet does not implement beta distribution functions.
In PERT a beta distribution is used to estimate the effort of a task in Ideal Person Days (i.e.: one worker 8h a day without distraction).</p>

<p><strong>Problem:</strong>
What consequences has use of normal distributions instead of beta distributions when the distribution are summed up to a single distribution.</p>

<p><strong>Details</strong>
A <em>project</em> consists of a list of <em>tasks</em>. Each task is <em>beta distributed</em> and <em>independed</em>, the sum of the tasks therefore should obey to the <em>central limit theorem</em>. The <em>project</em> therefore is normal distributed. How does Expected Value and SD change for the project when I use normal instead of beta distributions for each task?
I assume that the tasks distribution is skewed more often to the right than to the left.</p>

<p><strong>Questions</strong></p>

<ul>
<li>Are the tails fater?</li>
<li>Is the Expected Value higher or lower
with normal distributions? </li>
<li>Is the dispersion higher or lower?</li>
</ul>
PostId: 9607",PostDate: 2011-04-16 09:03:50.0,"Text: mean sounds correct, but not sure with beta, i forgot to say how i calculate sd. I start form 90% confidence interval and mean. now i calculate the sd from a norm dist via z-score for 90%. here an error should already be introduced. but what can I say about the error of sd in comparision with beta-dist?",YES
1319,"Body: <p><strong>Edited Question:</strong></p>

<p><em>As I promised I've edited this question.  The previous version was written with the intention of simplifying the real question, but it ended in losing the real significance. Now I'm posting the ""whole story"". ;)</em></p>

<p>My purpose is to calculate $n$ players' equity in a poker tournament (their probability of ending the tournament) in every $j$ place (1st, 2nd, and so on).</p>

<blockquote>
  <p>I've previously solved this problem in 2 different ways you can find
  here:</p>
  
  <p>For the Maths:</p>
  
  <p><a href=""http://math.stackexchange.com/questions/92942/applying-a-math-formula-in-a-more-elegant-way-maybe-a-recursive-call-would-do-t"">http://math.stackexchange.com/questions/92942/applying-a-math-formula-in-a-more-elegant-way-maybe-a-recursive-call-would-do-t</a></p>
  
  <p>And for the code:</p>
  
  <p><a href=""http://stackoverflow.com/questions/8605183/how-to-translate-this-math-formula-in-php"">http://stackoverflow.com/questions/8605183/how-to-translate-this-math-formula-in-php</a></p>
</blockquote>

<p>So, when I know every players' number of chips, I can easily apply those formulas and get their equity.</p>

<hr>

<p>There are 2 problems involved that hopefully can be solved with a statistical method. (I'm not a mathematician so I'm not sure it will be feasible).</p>

<ol>
<li>First problem, even if I know everyone's stack, when the number of players is high, the code is too slow to be implemented;</li>
<li>Second problem, this code should work by knowing only a limited number of stacks, belonging to the players of the analyzed table.</li>
</ol>

<p>Optimistically these 2 problems can both be solved with some kind of approximations.</p>

<p>In particular the formulas mentioned above should be applicable to scenarios with 27,45,90 players who are distributed in tables of 9.</p>

<p>For example in the case of 27 players there would be 3 tables: when there are 18 players left they will be redistributed in 2 tables and when there are only 9 left the final table will be opened.
It's not important to take into account the players' skill since it's a high variance game where its influence is reduced to the minimum, and mostly there are coin flips that eliminate players.</p>

<p>So I'm in a situation where I know:</p>

<ul>
<li>My number of chips.</li>
<li>The number of chips of the other 8 players of my table.</li>
<li>The total number of chips.</li>
<li>The average number of chips.</li>
<li>The maximum and minimum number of chips.</li>
</ul>

<p>As I suggested in the previous question, this seems to me (from my humbles math skills) to be a gaussian curve, with a maximum, a minimum and an average number of chips.</p>

<p><em>I think that's all. If you need additional details please leave a comment, and I will add them as soon as possible.
I wanna thank you for your interest, and for all the previous comments and answers. I hope your statistics can help me solve this. :D</em> </p>

<p>Best Regards,</p>

<p>Giorgio.</p>

<hr>

<hr>

<p><strong>Old Question:</strong> </p>

<p>I'd like to calculate or approximate the probability that I have to win a tournament where every player has a determined amount of chips.</p>

<p>Let's consider a scenario where there are 9 players and I know everyone's number of chips, to calculate my probability of winning I would do: my chips/(tot chips - my chips).</p>

<p>Now imagine those 9 players are put on 3 different tables of 3 players each, and I know the chips only of the 2 players of my table and mine obviously. I also know the total number of chips, the max and min amount chips the players have and the average stack.</p>

<p>Is it possible to make an approximation of my probability of winning?</p>

<p>I have only basic math skills but I think players' stack could be approximated to a gaussian curve,  then use some ""statistical trick"" to calculate my probability.</p>

<p>Thanks in advance for any hint!</p>

<p>Best regards,
Giorgio</p>
PostId: 22084",PostDate: 2012-02-01 16:21:00.0,Text: Oh sorry! I've noticed that what I wrote make no sense as it's written! This it's because the real problem to solve include to calculate the probability of each player n of ending in every place j.,YES
2478,"Body: <p>Because @zaynah posted in the comments that the data are thought to follow a Weibull distribution, I'm gonna provide a short tutorial on how to estimate the parameters of such a distribution using MLE (Maximum likelihood estimation). There is a <a href=""http://stats.stackexchange.com/questions/21745/how-can-i-perform-weibull-analysis-on-monthly-recorded-data-of-wind-speeds"">similar post</a> about wind speeds and Weibull distribution on the site.</p>

<ol>
<li><a href=""http://cran.r-project.org/"" rel=""nofollow"">Download and install <code>R</code></a>, it's free</li>
<li>Optional: <a href=""http://www.rstudio.com/"" rel=""nofollow"">Download and install RStudio</a>, which is a great IDE for R providing a ton of useful functions such as syntax highlighting and more.</li>
<li>Install the packages <code>MASS</code> and <code>car</code> by typing: <code>install.packages(c(""MASS"", ""car""))</code>. Load them by typing: <code>library(MASS)</code> and <code>library(car)</code>.</li>
<li><a href=""http://cran.r-project.org/doc/manuals/R-data.pdf"" rel=""nofollow"">Import your data into <code>R</code></a>. If you have your data in Excel, for example, save them as delimited text file (.txt) and import them in <code>R</code> with <code>read.table</code>.</li>
<li>Use the function <code>fitdistr</code> to calculate the maximum likelihood estimates of your weibull distribution: <code>fitdistr(my.data, densfun=""weibull"")</code>. To see a fully worked out example, see the link at the bottom of the answer.</li>
<li>Make a QQ-Plot to compare your data with a Weibull distribution with the scale and shape parameters estimated at point 5: <code>qqPlot(my.data, distribution=""weibull"", shape=, scale=)</code></li>
</ol>

<p>The <a href=""http://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf"" rel=""nofollow"">tutorial of Vito Ricci</a> on fitting distribution with <code>R</code> is a good starting point on the matter. And there are <a href=""http://stats.stackexchange.com/search?q=fit+distribution"">numerous posts</a> on this site on the subject (see <a href=""http://stats.stackexchange.com/questions/58220/what-distribution-does-my-data-follow"">this post</a> too).</p>

<p>To see a fully worked out example of how to use <code>fitdistr</code>, have a look at <a href=""http://stackoverflow.com/questions/15303310/how-to-create-a-weibull-cumulative-distribution-function-starting-from-fitdistr"">this post</a>.</p>

<p>Let's look at an example in <code>R</code>:</p>

<pre><code># Load packages

library(MASS)
library(car)

# First, we generate 1000 random numbers from a Weibull distribution with
# scale = 1 and shape = 1.5

rw &lt;- rweibull(1000, scale=1, shape=1.5)

# We can calculate a kernel density estimation to inspect the distribution
# Because the Weibull distribution has support [0,+Infinity), we are truncate
# the density at 0

par(bg=""white"", las=1, cex=1.1)
plot(density(rw, bw=0.5, cut=0), las=1, lwd=2,
xlim=c(0,5),col=""steelblue"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/iso1j.png"" alt=""Weibull KDE""></p>

<pre><code># Now, we can use fitdistr to calculate the parameters by MLE

fitdistr(rw, densfun=""weibull"")

     shape        scale   
  1.56788999   1.01431852 
 (0.03891863) (0.02153039)
</code></pre>

<p>The maximum likelihood estimates are close to those we arbitrarily set in the generation of the random numbers. Let's compare our data using a QQ-Plot with a hypothetical Weibull distribution with the parameters that we've estimated with <code>fitdistr</code>:</p>

<pre><code>qqPlot(rw, distribution=""weibull"", scale=1.014, shape=1.568, las=1, pch=19)
</code></pre>

<p><img src=""http://i.stack.imgur.com/PdfJk.png"" alt=""QQPlot""></p>

<p>The points are nicely aligned on the line and mostly within the 95%-confidence envelope. We would conclude that our data are compatible with a Weibull distribution. This was expected, of course, as we've sampled our values from a Weibull distribution. </p>

<hr>

<h2>Estimating the $k$ (shape) and $c$ (scale) of a Weibull distribution without MLE</h2>

<p><a href=""http://journals.ametsoc.org/doi/abs/10.1175/1520-0450%281978%29017%3C0350%3AMFEWSF%3E2.0.CO%3B2"" rel=""nofollow"">This paper</a> lists five methods to estimate the parameters of a Weibull distribution for wind speeds. I'm gonna explain three of them here.</p>

<h2>From means and standard deviation</h2>

<p>The shape parameter $k$ is estimated as:
$$
k=\\left(\\frac{\\hat{\\sigma}}{\\hat{v}}\\right)^{-1.086}
$$
and the scale parameter $c$ is estimated as:
$$
c=\\frac{\\hat{v}}{\\Gamma(1+1/k)}
$$
with $\\hat{v}$ is the mean wind speed and $\\hat{\\sigma}$ the standard deviation and $\\Gamma$ is the <a href=""http://en.wikipedia.org/wiki/Gamma_function0450%281978%29017%3C0350%3AMFEWSF%3E2.0.CO%3B2"" rel=""nofollow"">Gamma function</a>.</p>

<h2>Least-squares fit to observed distribution</h2>

<p>If the observed wind speeds are divided into $n$ speed interval $0-V_{1},V_{1}-V_{2},\\ldots, V_{n-1}-V_{n}$, having frequencies of occurrence $f_{1}, f_{2},\\ldots,f_{n}$ and cumulative frequencies $p_{1}=f_{1}, p_{2}=f_{1}+f_{2}, \\ldots, p_{n}=p_{n-1}+f_{n}$, then you can fit a linear regression of the form $y=a+bx$ to the values
$$
x_{i} = \\ln(V_{i})
$$
$$
y_{i} = \\ln[-\\ln(1-p_{i})]
$$
The Weibull parameters are related to the linear coefficients $a$ and $b$ by
$$
c=\\exp\\left(-\\frac{a}{b}\\right)
$$
$$
k=b
$$</p>

<h2>Median and quartile wind speeds</h2>

<p>If you don't have the complete observed wind speeds but the median $V_{m}$ and quartiles $V_{0.25}$ and $V_{0.75}$ $\\left[p(V\\leq V_{0.25})=0.25, p(V\\leq V_{0.75})=0.75\\right]$, then $c$ and $k$ can be computed by the relations
$$
k = \\ln\\left[\\ln(0.25)/\\ln(0.75)\\right]/\\ln(V_{0.75}/V_{0.25})\\approx 1.573/\\ln(V_{0.75}/V_{0.25})
$$
$$
c=V_{m}/\\ln(2)^{1/k}
$$</p>

<h2>Comparison of the four methods</h2>

<p>Here is an example in <code>R</code> comparing the four methods:</p>

<pre><code>library(MASS)  # for ""fitdistr""

set.seed(123)
#-----------------------------------------------------------------------------
# Generate 10000 random numbers from a Weibull distribution
# with shape = 1.5 and scale = 1
#-----------------------------------------------------------------------------

rw &lt;- rweibull(10000, shape=1.5, scale=1)

#-----------------------------------------------------------------------------
# 1. Estimate k and c by MLE
#-----------------------------------------------------------------------------

fitdistr(rw, densfun=""weibull"")
shape         scale   
1.515380298   1.005562356 

#-----------------------------------------------------------------------------
# 2. Estimate k and c using the leas square fit
#-----------------------------------------------------------------------------

n &lt;- 100 # number of bins
breaks &lt;- seq(0, max(rw), length.out=n)

freqs &lt;- as.vector(prop.table(table(cut(rw, breaks = breaks))))
cum.freqs &lt;- c(0, cumsum(freqs)) 

xi &lt;- log(breaks)
yi &lt;- log(-log(1-cum.freqs))

# Fit the linear regression
least.squares &lt;- lm(yi[is.finite(yi) &amp; is.finite(xi)]~xi[is.finite(yi) &amp; is.finite(xi)])
lin.mod.coef &lt;- coefficients(least.squares)

k &lt;- lin.mod.coef[2]
k
1.515115
c &lt;- exp(-lin.mod.coef[1]/lin.mod.coef[2])
c
1.006004

#-----------------------------------------------------------------------------
# 3. Estimate k and c using the median and quartiles
#-----------------------------------------------------------------------------

med &lt;- median(rw)
quarts &lt;- quantile(rw, c(0.25, 0.75))

k &lt;- log(log(0.25)/log(0.75))/log(quarts[2]/quarts[1])
k
1.537766
c &lt;- med/log(2)^(1/k)
c
1.004434

#-----------------------------------------------------------------------------
# 4. Estimate k and c using mean and standard deviation.
#-----------------------------------------------------------------------------

k &lt;- (sd(rw)/mean(rw))^(-1.086)
c &lt;- mean(rw)/(gamma(1+1/k))
k
1.535481
c
1.006938
</code></pre>

<p>All methods yield very similar results. The maximum likelihood approach has the advantage that the standard errors of the Weibull parameters are directly given.</p>

<hr>

<h2>Using bootstrap to add pointwise confidence intervals to the PDF or CDF</h2>

<p>We can use a the non-parametric bootstrap to construct pointwise confidence intervals around the PDF and CDF of the estimated Weibull distribution. Here's an <code>R</code> script:</p>

<pre><code>#-----------------------------------------------------------------------------
# 5. Bootstrapping the pointwise confidence intervals
#-----------------------------------------------------------------------------

set.seed(123)

rw.small &lt;- rweibull(100,shape=1.5, scale=1)

xs &lt;- seq(0, 5, len=500)


boot.pdf &lt;- sapply(1:1000, function(i) {
  xi &lt;- sample(rw.small, size=length(rw.small), replace=TRUE)
  MLE.est &lt;- suppressWarnings(fitdistr(xi, densfun=""weibull""))  
  dweibull(xs, shape=as.numeric(MLE.est[[1]][13]), scale=as.numeric(MLE.est[[1]][14]))
}
)

boot.cdf &lt;- sapply(1:1000, function(i) {
  xi &lt;- sample(rw.small, size=length(rw.small), replace=TRUE)
  MLE.est &lt;- suppressWarnings(fitdistr(xi, densfun=""weibull""))  
  pweibull(xs, shape=as.numeric(MLE.est[[1]][15]), scale=as.numeric(MLE.est[[1]][16]))
}
)   

#-----------------------------------------------------------------------------
# Plot PDF
#-----------------------------------------------------------------------------

par(bg=""white"", las=1, cex=1.2)
plot(xs, boot.pdf[, 1], type=""l"", col=rgb(.6, .6, .6, .1), ylim=range(boot.pdf),
     xlab=""x"", ylab=""Probability density"")
for(i in 2:ncol(boot.pdf)) lines(xs, boot.pdf[, i], col=rgb(.6, .6, .6, .1))

# Add pointwise confidence bands

quants &lt;- apply(boot.pdf, 1, quantile, c(0.025, 0.5, 0.975))
min.point &lt;- apply(boot.pdf, 1, min, na.rm=TRUE)
max.point &lt;- apply(boot.pdf, 1, max, na.rm=TRUE)
lines(xs, quants[1, ], col=""red"", lwd=1.5, lty=2)
lines(xs, quants[3, ], col=""red"", lwd=1.5, lty=2)
lines(xs, quants[2, ], col=""darkred"", lwd=2)
#lines(xs, min.point, col=""purple"")
#lines(xs, max.point, col=""purple"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/h1HU5.png"" alt=""Weibull PDF CIs""></p>

<pre><code>#-----------------------------------------------------------------------------
# Plot CDF
#-----------------------------------------------------------------------------

par(bg=""white"", las=1, cex=1.2)
plot(xs, boot.cdf[, 1], type=""l"", col=rgb(.6, .6, .6, .1), ylim=range(boot.cdf),
     xlab=""x"", ylab=""F(x)"")
for(i in 2:ncol(boot.cdf)) lines(xs, boot.cdf[, i], col=rgb(.6, .6, .6, .1))

# Add pointwise confidence bands

quants &lt;- apply(boot.cdf, 1, quantile, c(0.025, 0.5, 0.975))
min.point &lt;- apply(boot.cdf, 1, min, na.rm=TRUE)
max.point &lt;- apply(boot.cdf, 1, max, na.rm=TRUE)
lines(xs, quants[1, ], col=""red"", lwd=1.5, lty=2)
lines(xs, quants[3, ], col=""red"", lwd=1.5, lty=2)
lines(xs, quants[2, ], col=""darkred"", lwd=2)
lines(xs, min.point, col=""purple"")
lines(xs, max.point, col=""purple"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/cQDlv.png"" alt=""Weibull CDF CIs""></p>
PostId: 60530",PostDate: 2013-05-31 14:04:46.0,"Text: my mistake. Evidently, it only automatically estimates parameters from some distributions, and Weibull isn't one of those.",YES
6045,"Body: <p>I have a set of predictors in a linear regression, as well as three control variables. The issue here is that one of my variables of interest is only statistically significant if the control variables are included in the final model. However, the control variables themselves are not statistically significant.</p>

<p>Here is how the multicollinearity of all my variables look like (including control variables):</p>

<pre><code> &gt; vif(lm(return ~ EQ + EFF + SIZE + MOM + MSCR + UMP, data = as.data.frame(port.df)))
       EQ      EFF     SIZE      MOM     MSCR      UMP 
 3.687171 3.481672 2.781901 1.064312 1.438596 1.003408

 &gt; vif(lm(return ~ EQ + MOM + MSCR, data = as.data.frame(port.df)))
       EQ      MOM     MSCR 
 1.359992 1.048142 1.412658 
</code></pre>

<p>My variables of interest are <strong>EQ, MOM and MSCR</strong>, and the control variables are <em>EFF, SIZE and UMP</em>. EQ is only significant if the three control var are included, and becomes insignificant when they are not:</p>

<ul>
<li><p>Here are the coefficients (1rst row) and t-stats (2nd row) when control variables are included (notice that EQ is statistically significant)</p>

<pre><code>       intercept           EQ          EFF        SIZE         MOM       MSCR          UMP
[1,] 0.005206246 -0.006310531 0.0001229055 0.004125551 0.007738259 0.00473377 5.838596e-06
[2,] 1.866628909 -1.746583234 0.0388823612 1.178460997 2.145062820 2.08131100 1.994863e-01
</code></pre></li>
<li><p>Now, here is the result of the regression when the control variables are excluded (notice that EQ is NOT statistically significant anymore)</p>

<pre><code>       intercept           EQ         MOM       MSCR
[1,] 0.007313402 -0.002111833 0.007128606 0.00668364
[2,] 2.652662996 -0.595391117 2.036985378 2.80177366
</code></pre></li>
</ul>

<p>The problem is that when I include my control variables, all my variables of interest are significant, but my control variables are not.</p>

<p>Which variables should I include in my final model? How should I structure my final model then, given the fact that the model will be used for forecasting?</p>

<p>Thank you,</p>
PostId: 69371",PostDate: 2013-09-06 13:51:22.0,"Text: Just read up on LASSO again & forget about ""significance"". This is like a doctor asking how many leeches to apply after a course of antibiotics.",YES
9365,"Body: <p>In my problem I have 2 classifiers, C1 and C2. Both C1 and C2 are Naive Bayes classifiers but the difference between them is that they use different feature selection methods. Both classifiers are trained over a dataset of 10,000 instances (noisy-labeled), and tested over a different dataset of 1,000 instances (manually labeled), both datasets being balanced.</p>

<p>Now, I have plotted accuracy of both classifiers on increasing number of instances, and I found by visual inspection that C2 has generally better accuracy and recall than C1 . I would like to know whether such difference is statistically significant or not to assess that C2 is better than C1.</p>

<p>Previously, I used the same dataset for k-cross validation, got the mean and variation of the accuracies of both classifiers and computed student t-test on a specific amount of features. However, now I have 2 different datasets for training and testing. How could I perform a test in such situation? Should I get the mean of accuracies for all different feature amounts?</p>

<p>Thanks in advance...</p>

<p><strong>EDIT</strong> </p>

<p>Regarding the domain, I am dealing with sentiment analysis (SA), classifying text data in 3 classes : positive, negative and neutral. 
Regarding error cost, at this stage I suppose that all error costs are the same (although I understand that the cost of classifying negative as positive would higher than negative as neutral). 
Regarding the ""practical significant difference"" when dealing with SA I am assuming that an improvement of 1% is significant, since previous papers usually present such kind of improvements. 
I want to test the accuracy of C1 and C2 when trained over automatic-labeled data, and tested over manually-labeled data.</p>
PostId: 30354",PostDate: 2012-06-13 05:57:03.0,"Text: Thanks again. 2 final questions: 1) 632 bootstrap samples with replacement to build a training set, leaving the rest of the data as test data. However, in my case, I have 2 different datasets for training and testing, and I can't use training instances for testing because training instances are noisy-labeled. How could I apply 632B to my problem then ? Sampling from each dataset ? 2) when using feature selection only the top k scores are selected. Should I fix such k when doing bootstrapping? Or average for different k's ?",YES
8256,"Body: <p>I'm not sure if ARIMA would be able to decompose the data into interprettable/decomposed time series. </p>

<p>I used <a href=""http://support.sas.com/documentation/cdl/en/etsug/60372/HTML/default/viewer.htm#etsug_ucm_sect006.htm"" rel=""nofollow"">Proc UCM</a> in SAS which is a state space model. Following is the code of a basic structural model. The data is decomposed into Trend (level+slope) + Seasonal and White noise (Irregular). There is not much white noise left after you model. I'm assuming this is what you were looking for. The model fits the data like a glove. If you have access to SAS, you can try running the following code, you can get the decomposed series.</p>

<p>Level + Slope+ Seasonal + Irregular (not Shown).</p>

<p><img src=""http://i.stack.imgur.com/1dyFK.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/j23yi.png"" alt=""Forecast Output which is comination of level+slope+season:""></p>

<p><img src=""http://i.stack.imgur.com/UglD2.png"" alt=""enter image description here""></p>

<p>You could also try <code>STL decomposition</code> in R.</p>

<p>ods graphics on;
       proc ucm data = data;</p>

<pre><code>      id date interval = second; 
      model value;
      irregular plot = smooth;
      level ;
      slope variance = 0 noest;
      season length = 80 ;
       dep lags = 2;
      forecast plot=(forecasts decomp) lead = 80 outfor=forecast; 
   run ; 
ods graphics off ;
</code></pre>

<p>Hope this is helpful.</p>
PostId: 81077",PostDate: 2014-01-02 21:23:35.0,Text: I do not belive! No significant pattern on residuals ACF? Have you checked for 300 lags? I have not got SAS licence unfortunately :(,YES
5394,"Body: <p>I have a classification task where $n\\gg p$ (like 440000 vs. 23). I want to use Lasso (<code>glmnet</code> in R) to select the variables first, then use techniques like random forest or boosting for the actual classification. I read this from <a href=""http://link.springer.com/book/10.1007/978-3-642-20192-9"" rel=""nofollow""><em>Statistics for High-Dimensional Data: Methods, Theory and Applications</em></a>:</p>

<p><img src=""http://i.stack.imgur.com/09CU7.png"" alt=""enter image description here""></p>

<p>I am not sure whether Lasso will still do variable selection when $n\\gg p$.
Any suggestions? Thank you.</p>
PostId: 103671",PostDate: 2014-06-17 10:54:14.0,"Text: When n >> p then you may not need variable selection at all, but if you do decide to use a method, lasso should be fine.",YES
13256,"Body: <p>This is a great question! There is a mathematical concept called likelihood that will help you understand the issues. Fisher invented likelihood but considered it to be somewhat less desirable than probability, but likelihood turns out to be more 'primitive' than probability and Ian Hacking (1965) considered it to be axiomatic in that it is not provable. Likelihood underpins probability rather then the reverse.</p>

<p>Hacking, 1965. <a href=""http://books.google.com.au/books?id=2V44AAAAIAAJ&amp;printsec=frontcover&amp;dq=Ian+Hacking&amp;source=bl&amp;ots=jEGtdBCV1X&amp;sig=Z03BxrZkYFbZ4QbUNZOvLgZQGvY&amp;hl=en&amp;sa=X&amp;ei=UvSBULzxEsiJmwXL7YDgDA&amp;ved=0CF8Q6AEwCA""><em>Logic of Statistical Inference</em></a>.</p>

<p>Likelihood is not given the attention that it should have in standard textbooks of statistics, for no good reason. It differs from probability in having almost exactly the properties that one would expect, and likelihood functions and intervals are very useful for inference. Perhaps likelihood is not liked by some statisticians because there is sometimes no 'proper' way to derive the relevant likelihood functions. However, in many cases the likelihood functions are obvious and well defined. A study of likelihoods for inference should probably start with Richard Royall's small and easy to understand book called <a href=""http://books.google.com.au/books/about/Statistical_Evidence.html?id=oysWLTFaI_gC&amp;redir_esc=y""><em>Statistical Evidence: a Likelihood Paradigm</em></a>.</p>

<p>The answer to your question is that no, the points within any interval do not all have the same likelihood. Those at the edges of a confidence interval usually have lower likelihoods than others towards the centre of the interval. Of course, the conventional confidence interval tells you nothing directly about the parameter relevant to the particular experiment. Neyman's confidence intervals are 'global' in that they are designed to have long-run properties rather than 'local' properties relevant to the experiment in hand. (Happily good long-run performance can be interpreted in the local, but that is an intellectual shortcut rather than a mathematical reality.) Likelihood intervals—in the cases where they can be constructed—directly reflect the likelihood relating the experiment in hand. There is less about likelihood intervals that is confusing than is the case for confidence intervals, in my opinion, and they have more utility than might be expected from their almost complete disuse!</p>
PostId: 40791",PostDate: 2012-10-20 00:50:13.0,Text: @Erik I don't understand your criticism. The interval is about the parameter.,YES
8044,"Body: <p>I'm a biologist, and I have a large dataset that I'm trying to analyze. Here are the variables I'm working with:</p>

<ul>
<li>levels of 211 different metabolites in 16 different blood samples (predictor variables)</li>
<li>how well each of the 16 blood samples performed in a specific test (response variable)</li>
</ul>

<p>I am trying to figure out which variable(s) are the most important in predicting the performance of a blood sample in the test.</p>

<p>I would like to make these data more manageable by doing a PCA, but I'm new to this sort of analysis. I understand that a PCA will create groups of principal components (it will group metabolites that covary with each other and label each of these groups a principal component), but I'm not sure how to take into account the response variable in this analysis.</p>

<p>Any help would be much appreciated!!! Thank you.</p>

<pre><code>&gt; summary(metabolites_princomp)
Importance of components:
                          Comp.1     Comp.2      Comp.3      Comp.4
Standard deviation     3.9608225 0.40128486 0.259868774 0.215004349
Proportion of Variance 0.9805072 0.01006435 0.004220736 0.002889179
Cumulative Proportion  0.9805072 0.99057153 0.994792267 0.997681446
                           Comp.5       Comp.6       Comp.7
Standard deviation     0.12944496 0.0835343589 0.0745499634
Proportion of Variance 0.00104725 0.0004361243 0.0003473561
Cumulative Proportion  0.99872870 0.9991648202 0.9995121763
                             Comp.8       Comp.9      Comp.10
Standard deviation     0.0606023885 0.0467885157 2.566109e-02
Proportion of Variance 0.0002295406 0.0001368228 4.115572e-05
Cumulative Proportion  0.9997417169 0.9998785397 9.999197e-01
                            Comp.11      Comp.12      Comp.13
Standard deviation     2.203313e-02 1.872209e-02 1.450158e-02
Proportion of Variance 3.034116e-05 2.190729e-05 1.314349e-05
Cumulative Proportion  9.999500e-01 9.999719e-01 9.999851e-01
                            Comp.14      Comp.15      Comp.16
Standard deviation     1.141596e-02 7.517519e-03 7.194785e-03
Proportion of Variance 8.145258e-06 3.532068e-06 3.235308e-06
Cumulative Proportion  9.999932e-01 9.999968e-01 1.000000e+00
</code></pre>
PostId: 33053",PostDate: 2012-07-26 01:01:33.0,"Text: I usually create a function `center.rows = function(m) apply(1, function(r) r - mean(r))`",NO
11518,"Body: <p>So let's suppose there are <em>n</em> candidates running for office. A bunch of random people begin to selectively vote the candidates, each vote being a number from <em>1</em> to <em>n</em>. What is a good way to order the candidates based on their vote number and also the number of votes/frequency of votes?</p>

<p>So for example:
If candidate A was voted by a bunch of voters, but was the votes were fairly spaced out, he would probably be considered ""not as great"" as a candidate who was voted less/same amount/more but the votes were shortly after one another.</p>

<p>I was thinking of something like Reddit's Karma system, but that only takes into accounts +1's and -1's (and I'm not even sure if Reddit every gave information on how their Karma system works!)</p>

<hr>

<p>So for example:</p>

<p>Candidates = [ Bob, Sally, Joe, Sam, Rob]</p>

<p>Person A ranks Sally as #1 (best choice) and [Bob, Rob] as #5 (worst choice) and leaves (he doesn't vote for others).</p>

<p>Person B ranks Sam as #2, Joe as #4, and leaves.</p>

<p>Person C ranks Sally as #1, Joe as #3, and Bob as #5.</p>

<p>So in this case it may be compelling to say Sally is close to being #1, while Bob is probably closer to being #4 or #5. But Sam was only voted once as #2 and as such there's not a whole lot of grounds to say ""yes he's definitely #2"". Similarly, Rob was only voted once as being #5 but there isn't sufficient votes to definitively say he's worse than Bob.</p>

<p>As such, the raking may become something like:</p>

<p>Sally > Sam > Joe > Rob > Bob</p>

<p>You can probably twist this example to have even more voters and create a few outliar, but I hope his gives you a general idea of what I'm looking for.</p>
PostId: 34828",PostDate: 2012-08-22 06:59:33.0,"Text: The end goal of mine is to have a tool for a specific subset of gamers. In certain games (RTS ones to be specific), you are allowed to play as one of multiple characters. Some people think that one character should be used more than others. They each have 'some  idea' of where they think a character belongs if they were to be transposed into some sort of list of best to worse. I want to create something that will, to some degree, help them consolidate that idea.",NO
4562,"Body: <p>I am trying to solve this problem on and off for the past couple of months but to no success. This was supposed to be a very small part of my PhD thesis in navigation but I guess I underestimated the problem. It sounded trivial at the beginning, but now I am not so sure.</p>

<p>Lets say we have two ships, each with its own nominal position in 2D coordinates (mean). Due to errors in positioning systems we can only be certain that the ships are within 1 mile of the mean with 95% probability (normal distribution). Given these 2 positions and this probability distribution, what is the probability that the ships are within 5 miles from each other? Also, same question if the ship's probable position is an ellipse, not a circle.</p>

<p>I asked some people and they told me that there are no analytic solutions. If that is really the case, please explain how to solve it numerically.</p>

<p>As you can already tell, I come from engineering background, therefore my math is more than a bit rusty.</p>

<p>I apologize in advance if the question is too vague or too trivial for this forum. I will be more than happy to explain in more detail if needed.</p>

<p>I found <a href=""http://stats.stackexchange.com/questions/12209/percentage-of-overlapping-regions-of-two-normal-distributions"">this</a>, but it is only for univariate case, and besides I don't know how to implement it in my case where I need to find the probability that the distance between two ships is less than 5 miles. </p>

<p>I imagine this problem as a plane with two hills that intersect and the solution is the volume under the circle with diameter of 5 miles that is located somewhere between the two peaks of hills (means).</p>

<p>Am I on the right track?</p>

<p>Thanks</p>
PostId: 41900",PostDate: 2012-11-05 10:20:49.0,"Text: Okay, I'll try to give more information. Given information above, if there is probability >1% that the ships are less than 5 miles apart, a maneuver must be made to assure adequate separation. An outside observer can predict trajectories of each ship with confidence of 95% that the ships will be in their expected positions (as mentioned, that probability is based on bivariate normal distribution). The observer must decide whether to order the separation maneuver or not. This model is a decision support system which alerts the observer to order the maneuver before the loss of separation happens",YES
12231,"Body: <p>The density of my data set was plotted in R as follows.</p>

<p><img src=""http://i.stack.imgur.com/vUUFp.png"" alt=""enter image description here""></p>

<p>What kind of distribution would fit this data?</p>

<p>As I am not experienced to tell by visualization, I can only guess it is not normally distributed.  I shall test it with R: some guidance as a starting point is highly desirable. </p>
PostId: 13497",PostDate: 2011-07-26 10:03:15.0,"Text: @evdstat: This looks to me like it's coming from a distribution with heavy tails, such as a Frechet distribution or an inverse gamma distribution or mayby a Pareto distribution. I'd be interested in knowing more about the source of these data. Marketing?",YES
2496,"Body: <p>Besides the issue of the misspelling of 'anova' that Stephan mentioned, I have a few points.</p>

<p>1) I'd also suggest inserting ""|"" (for ""or"") in your search, since otherwise it will be treated more like ""and""</p>

<p>2) further, the word 'distribution' goes as much with Weibull as it does with gamma. So I suggest a search like so: <em>anova weibull|gamma distribution</em>.</p>

<p>3) further still, in the case of the gamma, an ANOVA-type model would normally be fitted using GLMs, so you may prefer to search on <em>gamma GLM</em></p>

<p>4) Parametric Weibull models are often available under options relating to survival analysis in many statistics packages; while survival models often have censored data, they don't <em>have</em> to, so a Weibull model with grouping-factors as IVs (""ANOVA-like"") models can often be fitted that way.</p>

<hr>

<p>The usual way to compare gamma means would be via a GLM.</p>

<p>This has the underlying assumption of equal shape parameters (much as ordinary ANOVA carries the assumption of equal variances).</p>

<p>This assumption can be assessed, for example, either visually (by looking at whether they seem to have similar shapes), or by finding MLEs of the shape parameters of the groups being compared. If the values are not too dissimilar then this approach should work fine.</p>

<p>[Similarly, a comparison of Weibull means might be achieved by treating the values as (uncensored) survival times in a survival model.]</p>

<p>On the other hand if it's not expected that the gammas are at least reasonably similar in shape, it's more complicated; one might try to form a confidence interval for the difference in means in any of several ways. In large samples, one might try bootstrapping, for example, or the distribution of a ratio of ML estimates of the mean (difference in logs) might be approximated or even assessed via simulation.</p>

<hr>

<p>For comparison purposes, the following can be done in R (the data set is built in):</p>

<pre><code>summary(lm(weight~feed,chickwts)) # Linear regression model for one way model 

summary(glm(weight~feed,family=Gamma(link=""identity""),chickwts)) # Gamma equivalent

summary(glm(weight~feed,family=Gamma(link=""log""),chickwts)) # gamma with log-link

summary(survreg(Surv(weight)~feed,data=chickwts))  # Weibull model
</code></pre>

<p><code>anova(...)</code> can be used in place of <code>summary(...)</code> in those calls to obtain other information.</p>

<p>The first model is an ordinary one-way anova type model. </p>

<p>The second is the equivalent using a gamma model (in this case with essentially identical parameter estimates to the first model but different standard errors). </p>

<p>The third model is also a gamma anova-type model but where the parameters describe effects on the log scale (a test of the model is still a test for differences in means on the original scale, though).</p>

<p>The fourth model is a Weibull model, which has log-scale parameter estimates (which can be compared with the third model).</p>
PostId: 91786",PostDate: 2014-03-29 01:46:29.0,"Text: ... (ctd) If you're reasonably satisfied that an exponential, an exponential may be a reasonable choice. What is best would depend on as yet unstated criteria.",YES
13312,"Body: <p>This is a soft question: How can the order of a sample univariate data be reversed while preserving the variance?</p>
PostId: 28538",PostDate: 2012-05-15 14:59:52.0,"Text: That assume only a linear transformation @MichaelChernick. I didn't post mine as an answer because the question seemed so broad I figured there must be something more to it, so I thought I'd wait for clarification.",YES
10220,"Body: <p>Let's assume that $X$ is a continuous random variable. Then, around variables like $Y$ there are two possible confusions related to terminology:</p>

<p>$Y$ is a <em>censored</em> version of $X$ (and sometimes people confuse ""<a href=""http://en.wikipedia.org/wiki/Censoring_%28statistics%29"" rel=""nofollow"">censoring</a>"" with ""<a href=""http://en.wikipedia.org/wiki/Truncation_%28statistics%29"" rel=""nofollow"">truncating</a>""). Also, $Y$ has a <em>mixed</em> distribution (and sometimes people confuse ""<a href=""http://www.math.uah.edu/stat/dist/Mixed.html"" rel=""nofollow"">mixed</a>"" distributions with ""<a href=""http://en.wikipedia.org/wiki/Mixture_distribution"" rel=""nofollow"">mixture</a>"" distributions).  </p>

<p>$Y$ is a censored version of $X$ because a) we observe only non-negative values and b) when we observe $0$ we only know that the corresponding realization of $X$ was $x\\leq 0$, i.e. we learn only that $X$ fell in an interval, not its specific value. This also creates the mixed distribution, i.e. a distribution that has a discrete part and a continuous part:</p>

<p>At value $0$, non-zero probability mass concentrates, equal to $P(X\\leq 0)$. For $X&gt;0$ we have $Y=X$. So the CDF of Y is</p>

<p>$$F_Y(y)  = \\left\\{
  \\begin{array}{lr}
    F_X(y)&amp; : y &gt; 0\\\\
   F_X(0) &amp; : y = 0\\\\
   0 &amp; \\text{elsewhere}
  \\end{array}
\\right\\}$$</p>

<p>and $\\lim_{y\\rightarrow \\infty}F_Y(y)=1$.  </p>

<p>As for its probability -mass function? - density function? -well, see the illuminating and educative comments below on the matter, as well as ways to express it in one row. Let's say we have a point where non-zero probability mass concentrates, $P(Y=0) = P(X \\leq 0)&lt;1$, while for $y&gt;0$ we have a continuous function $f_X(y)$ that does satisfy $\\int_{0}^{\\infty}f_X(y)dy = 1- P(X \\leq 0)$ , while this entity is $0$ everywhere else. So in all, it sums up to unity.</p>
PostId: 104602",PostDate: 2014-06-24 19:41:16.0,"Text: The CDF is discontinuous at $0$ and so not even giving a hint about this (as in earlier versions of this answer) was certainly not a good idea, as whuber pointed out. While this has been corrected, the pdf is still incorrect. The value of $f_Y$ at $0$ is _not_ $P\\{X \\leq 0\\}$.",YES
7830,"Body: <p>I would like to inquire about a simple test I might be able to perform to determine how 'nicely Gaussian' my empirical data are. If they are 'nicely Gaussian', then I can perform some other analysis, that assumes my data are Gaussian to begin with.</p>

<p>I am looking for a concrete test. I have a simple 1-dimensional data vector, with $N\\approx 10,000$, so I have plenty of data. I want to determine if these data are Gaussian. </p>

<p><strong>What I have tried:</strong></p>

<ol>
<li><p>I understand that the Gaussian PDF has no skewness, and no kurtosis, so I have implemented those metrics and taken a measure. This works OK--I think--so as it stands, this is my plan B. Perhaps there is a better way?</p></li>
<li><p>I have heard the term ""chi-squared"" being thrown around. I understand that it is a PDF in its own right, but I am not sure how this might apply to this problem. </p></li>
<li><p>Although half in jest, my current way is to simply eyeball the data. Needless to say, this is OK for some cases, but it will not work when data is being run and I am sleeping...</p></li>
</ol>

<p><strong>EDIT:</strong></p>

<p>It has been suggested that I talk some more about the 'other analysis' I had in mind, so here goes: If my data is Gaussian, I can readily apply a threshold developed (ex, <a href=""http://www.fceia.unr.edu.ar/~jcgomez/wavelets/Donoho_1995.pdf"" rel=""nofollow"">here</a>), but they only apply for data that is Gaussian. Now, if my test comes back ""not Gaussian"", then what I would like to do is determine what is the closest PDF that matches it, so that I can attempt to derive thresholds myself. </p>

<p>Now, thanks to everyone, I understand that there is an infinite number of PDFs, and I realize my question might have been somewhat open ended. </p>

<p>So to put a lot more clarity into the picture, I can say that my data is either a 'nice Gaussian' looking PDF, or, it tends to be a ""Gaussian PDF with symmetric long tails"". So, if my test comes back and says ""Yes, your data is Gaussian"", I can use one of the canned threshold tests I linked to earlier. If on the other hand my test says ""No, the tails are way too long for a typical Gaussian..."", then I would want to: 1) Know what type of PDF is this, 2) Estimate new thresholds on my own based on this. </p>

<p>I hope this clarifies some more, thanks for everyone.</p>
PostId: 62291",PostDate: 2013-06-21 17:27:10.0,"Text: @user11852 Thank you. Yes, I would be content with just having a rough&tumble-then-go-home measure of goodness measure. Its not a HUGE deal if it is not completely fitting a gaussian, I just have to verify that it can be reasonably considered a gaussian...",YES
1327,"Body: <p><strong>Edited:</strong></p>

<p>I have a categorical variable comprising of values from 1 to 7 with these probability:</p>

<pre><code>score   1        2       3      4       5       6   7
p       0.01    0.01    0.03    0.05    0.2   0.3   0.4
</code></pre>

<p>In two different samples from the two different populations, I found the same value for 10th percentile (i.e. 4) in observed data. I want to find the likelihood of having exactly the same value for 10th percentile.</p>

<p>FYI, the scores are answers to a scale and probabilities are proportions for each category in a normative sample.I want to be able to do this in R.</p>

<p>P.S.: any suggestion on calculating cumulative distribution of a variable with above-mentioned properties is appreciated.</p>

<p>Thanks.</p>
PostId: 79646",PostDate: 2013-12-14 06:23:20.0,Text: @Glen_b The reason is that in a survey from two different samples from two different populations I found the same value for 10th percentile. I want to know how likely is to find the same value for 10th percentile in a variable.This is why I thought that a simulation would be proper.,YES
3073,"Body: <p>First of all, let me explain what I'm after. I have a bunch of archaeological sites (point data) in GIS and a DEM-derived slope map. I classified the slope map into 1-degree interval classes and got the values for the sites. Now I wanted to test whether the values of the sites are significantly different than would be their random placement on the slope map, meaning that the slope of the terrain was an important factor when choosing a place for settlement. This seemed to be a good case for 1-sample KS test as the data is continuous and the reference distribution is actually known (the slope map itself). Also, the distributions are clearly non-normal.</p>

<p>So, I used <code>R</code>'s <code>ks.test</code> function so that the numeric vector x contained the classified values for the sites and y was set the cumulative distribution of the background distribution values via <code>ecdf</code> function: the classification into 1-degree bands was, of course, the same. However, the result is this:</p>

<pre><code>One-sample Kolmogorov-Smirnov test

data:  slope_sites
D = 0.4046, p-value &lt; 2.2e-16
alternative hypothesis: two-sided
</code></pre>

<p>Not only is the p-value ridiculously small, the D statistic is also peculiar. When plotting the two empirical cumulative distribution functions, the difference is much smaller. Not wanting to believe such result I computed the D statistic myself:</p>

<pre><code>max(abs(ecdf(slope_background)(0:55) - ecdf(slope_sites)(0:55)))
</code></pre>

<p>The 55 is set as limit as 55 degrees is the maximum recorded value
Anyway, the resulting D value was 0.1650732, which seems more realistic and is in accordance to the plotted ECDFs. </p>

<p>Any thoughts on that? I suspect I'm doing something fundamentally wrong though.
Also, is there any other, perhaps better way to test this kind of hypothesis? I tried <code>ad.test</code> with the same parameters and I suspect that p-value of 4.38e-06 is also not exactly what would be expected.</p>

<p>The histogram of the site distribution:
<img src=""http://i.stack.imgur.com/vPAWw.png"" alt=""Hist site""></p>

<p>Cumulative distribution functions of the sites (red) and background data (black dashed line):
<img src=""http://i.stack.imgur.com/j5fxk.png"" alt=""ECDF""></p>
PostId: 62489",PostDate: 2013-06-24 18:52:20.0,"Text: @Glen_b That's interesting as I've seen many works (true, not by statisticians) using binned data for KS test. However I take note on that and try to do it without binning (going to need lots of RAM I guess) or try permutation approach. Thanks!",YES
14795,"Body: <p>for some reason, people have difficulty grasping what a p-value really is.</p>
PostId: 6605",PostDate: 2011-01-27 06:06:18.0,"Text: @probabilityislogic: Again, I know. If the null hypothesis is wrong, then the test statistic is meaningless.",YES
4074,"Body: <p>I am encountering a difficulty with the following task.  Have I made a mistake, or is this an inherent flaw in the notion of confidence intervals?  (Other such flaws exist.)</p>

<p>Consider a random sample $X_1,\\ldots , X_n$ from a Uniform($\\theta$, $\\theta + a$) distribution, where $\\theta$ is unknown and $a$ is known.  We wish to determine a confidence interval for $\\theta$.</p>

<p>The reader may verify the following details: The statistics $Y=\\text{min}_i X_i$ and $Z=\\text{max}_i X_i$ are jointly sufficient for $\\theta$.  For  $\\theta \\le c_1 \\le c_2 \\le \\theta + a$,
$P\\{c_1 \\le Y \\le Z \\le c_2\\} = [(c_2 - c_1)/a]^n$.  For $0 &lt; \\gamma &lt; 1$, set $d_1 =(1-\\sqrt[n]\\gamma)/2$ and $d_2 =(1+\\sqrt[n]\\gamma)/2$.  Then
$\\gamma = P\\{\\theta + ad_1 \\le Y \\le Z \\le \\theta + ad_2\\} = 
P\\{Z -ad_2 \\le \\theta \\le Y-ad_1\\}$.  Thus, $[Z -ad_2, Y -ad_1]$ is a $\\gamma$ confidence interval for $\\theta$.</p>

<p>Now here's the difficulty:  If we observe $Z - Y &gt; a\\sqrt[n]\\gamma$, then $Z -ad_2 &gt; Y -ad_1$, so our formula yields a nonsensical answer.  Have I made an error in my calculations?  Or is this one of those problems with confidence intervals?</p>

<p>(Homework?  I guess so - but a homework problem that I <em>wrote</em> for my students.  Inspired by another problem in DeGroot &amp; Schervish.)</p>

<p>[Also posted at math.stackexchange.  I didn't know about this group, and I received no satisfactory answer there.]</p>
PostId: 66407",PostDate: 2013-08-03 03:53:12.0,"Text: I do not see how a nonsensical answer is consistent with the statement that ""a CI... cannot fail.""",YES
14682,"Body: <p>You've got one big problem in your data as posted: there's no ""spending"" column, so you don't have a response. Once you get that, just run a linear model with sex as your only predictor.</p>

<pre><code> yourModel &lt;- lm(spending ~ sex, data = spending)
 summary(yourModel)
</code></pre>

<p>The base level (intercept only) corresponds to your factor's 0-level, in this case male. The difference between the levels is sex coefficient in the model output.</p>

<p>When you have more than 2 levels, the output is explicit. For example, using the built-in dataset <code>mtcars</code></p>

<pre><code>head(mtcars)
mtcars$cyl &lt;- factor(mtcars$cyl)
mylm &lt;- lm(mpg ~ cyl, data = mtcars)

&gt; levels(mtcars$cyl)
[1] ""4"" ""6"" ""8""

&gt; summary(mylm)

Call:
lm(formula = mpg ~ cyl, data = mtcars)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.2636 -1.8357  0.0286  1.3893  7.2364 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  26.6636     0.9718  27.437  &lt; 2e-16 ***
cyl6         -6.9208     1.5583  -4.441 0.000119 ***
cyl8        -11.5636     1.2986  -8.905 8.57e-10 ***
---
</code></pre>

<p>Here the factor levels are called out, with the 0-level (4-cylinder) corresponding the the intercept. As in your example, when there are no other predictors, this is as simple as taking the means for each factor level. The 4-cyl mean is 26.66, same as the intercept, and then adding -6.9 we get the 6-cyl mean, and adding -11.56 (to the original 26.66) we get the 8-cyl mean of 15.1.</p>

<pre><code>&gt; with(mtcars, tapply(mpg, cyl, mean)) ## just calculates mean mpg at each level of cyl
       4        6        8 
26.66364 19.74286 15.10000
</code></pre>
PostId: 36104",PostDate: 2012-09-11 15:41:34.0,"Text: But, how is it females spend less and not vice-versa?",YES
7799,"Body: <p>I would like to generate a random correlation matrix such that the distribution of the entries (except 1) look approximately like normal.</p>

<p>The motivation is this. For a set of n time series data, the correlation distribution often look quite close to normal. I would like to generate many ""normal"" correlation matrices to represent the general siutation and use them to calculate risk number. Do you think I answer your question?</p>
PostId: 10121",PostDate: 2011-04-28 22:18:23.0,"Text: @cardinal, that's a good point.  Another source of dependency comes from matrix multiplication.",YES
6326,"Body: <p>I have encountered a joint cumulative distribution function with marginal properties. Although I have the answer, mine doesn't really match it and I don't seem to understand why.</p>

<p>IMG: <a href=""http://i.stack.imgur.com/0fwuN.jpg"" rel=""nofollow"">http://i.stack.imgur.com/0fwuN.jpg</a></p>

<p>Following is my understanding, which seems to be in conflict of the resolutely answer.</p>

<p>IMG: <a href=""http://i.stack.imgur.com/QG7Fn.jpg"" rel=""nofollow"">http://i.stack.imgur.com/QG7Fn.jpg</a></p>

<p>Appreciate any advice please.</p>

<p>PS: can't post image as I don't have enough rep points yet. :(( still learning the proper equations and syntax.</p>
PostId: 55801",PostDate: 2013-04-11 06:43:18.0,Text: What? What rule is this?,YES
13265,"Body: <p>This is a great question! There is a mathematical concept called likelihood that will help you understand the issues. Fisher invented likelihood but considered it to be somewhat less desirable than probability, but likelihood turns out to be more 'primitive' than probability and Ian Hacking (1965) considered it to be axiomatic in that it is not provable. Likelihood underpins probability rather then the reverse.</p>

<p>Hacking, 1965. <a href=""http://books.google.com.au/books?id=2V44AAAAIAAJ&amp;printsec=frontcover&amp;dq=Ian+Hacking&amp;source=bl&amp;ots=jEGtdBCV1X&amp;sig=Z03BxrZkYFbZ4QbUNZOvLgZQGvY&amp;hl=en&amp;sa=X&amp;ei=UvSBULzxEsiJmwXL7YDgDA&amp;ved=0CF8Q6AEwCA""><em>Logic of Statistical Inference</em></a>.</p>

<p>Likelihood is not given the attention that it should have in standard textbooks of statistics, for no good reason. It differs from probability in having almost exactly the properties that one would expect, and likelihood functions and intervals are very useful for inference. Perhaps likelihood is not liked by some statisticians because there is sometimes no 'proper' way to derive the relevant likelihood functions. However, in many cases the likelihood functions are obvious and well defined. A study of likelihoods for inference should probably start with Richard Royall's small and easy to understand book called <a href=""http://books.google.com.au/books/about/Statistical_Evidence.html?id=oysWLTFaI_gC&amp;redir_esc=y""><em>Statistical Evidence: a Likelihood Paradigm</em></a>.</p>

<p>The answer to your question is that no, the points within any interval do not all have the same likelihood. Those at the edges of a confidence interval usually have lower likelihoods than others towards the centre of the interval. Of course, the conventional confidence interval tells you nothing directly about the parameter relevant to the particular experiment. Neyman's confidence intervals are 'global' in that they are designed to have long-run properties rather than 'local' properties relevant to the experiment in hand. (Happily good long-run performance can be interpreted in the local, but that is an intellectual shortcut rather than a mathematical reality.) Likelihood intervals—in the cases where they can be constructed—directly reflect the likelihood relating the experiment in hand. There is less about likelihood intervals that is confusing than is the case for confidence intervals, in my opinion, and they have more utility than might be expected from their almost complete disuse!</p>
PostId: 40791",PostDate: 2012-10-20 00:50:13.0,"Text: Michael Lew I disagree with your statement ""the points within any interval do not all have the same likelihood"" in the context of confidence intervals.  The OP's question is for CI NOT LIs.  Although researchers agree about the likelihood principle and its fundamental importance,  Prof. Wasserman and others have a reason not worrying about it too much http://normaldeviate.wordpress.com/2012/07/28/statistical-principles/.",YES
4920,"Body: <p>I carried out a test on a group of 100 people. The test contained 10 questions with 5-point Likert scale answers (1 = Strongly Agree, 5 = Strongly Disagree). After the test was conducted, the group was educated over a period of time on how to answer the questions. After the education completed, the same test was carried out again on the same group, but with 10 absent. Knowing that the pre and post tests were anonymously conducted, I would like to test whether there is a significant difference between the two test results.
If I use the Wilcoxon Signed Rank Test for a paired sample to compare results for individual question of each test, the results will be different depending of the entry of the test results. In other words, I can't tell if one student improved because I can't link his first test to his second test.</p>

<p>I don't know if using the Wilcoxon Signed Rank Test for a paired sample would be possible given the tests were done anonymously.</p>

<p>Any help is appreciated.
Thanks</p>
PostId: 74402",PostDate: 2013-11-02 18:11:51.0,"Text: Sorry @Subzero-273K but can not treat them as 2 independent samples when you KNOW they are dependent - it is just not scientifically acceptable. You should deal with statistical analyses before data collection. Lack of good experimental design is common mistake, (I did it too) but I rather admit my mistake and do it the right way later instead of ""rape"" the data to get any result.",YES
2769,"Body: <p>Consider the following example long data.frame with two dependent measures, ""Score"" and ""NewVariable"", 1 between subjects variable ""Prep"" (3 levels), 2 within subjects variables ""Day"" (3 levels) and ""Experiment"" (2 levels), and a subject identifier ""SID"".</p>

<pre><code>example &lt;- structure(list(SID = structure(c(1L, 8L, 12L, 13L, 5L, 6L, 1L, 
8L, 12L, 13L, 5L, 6L, 1L, 8L, 12L, 13L, 5L, 6L, 1L, 8L, 12L, 
13L, 5L, 6L, 1L, 8L, 12L, 13L, 5L, 6L, 1L, 8L, 12L, 13L, 5L, 
6L), .Label = c(""S1"", ""S10"", ""S11"", ""S12"", ""S13"", ""S14"", ""S15"", 
""S2"", ""S3"", ""S4"", ""S5"", ""S6"", ""S7"", ""S8"", ""S9""), class = ""factor""), 
    Prep = structure(c(2L, 2L, 1L, 1L, 3L, 3L, 2L, 2L, 1L, 1L, 
    3L, 3L, 2L, 2L, 1L, 1L, 3L, 3L, 2L, 2L, 1L, 1L, 3L, 3L, 2L, 
    2L, 1L, 1L, 3L, 3L, 2L, 2L, 1L, 1L, 3L, 3L), .Label = c(""Group work only"", 
    ""Lecture only"", ""No instruction""), class = ""factor""), Day = structure(c(1L, 
    1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 
    3L, 3L, 3L, 3L, 3L), .Label = c(""Day1"", ""Day2"", ""Day3""), class = ""factor""), 
    Score = c(14, 14, 16, 18, 11, 12, 13, 15, 17, 15, 12, 11, 
    18, 17, 18, 17, 10, 12, 15, 15, 17, 19, 12, 13, 14, 16, 18, 
    16, 13, 12, 19, 18, 19, 18, 11, 13), NewVariable = c(-0.887056411864653, 
    -0.480360621343027, -0.490415963314823, 1.3654758915317, 
    -1.90913204292831, 0.0300532242614742, -1.84822348735206, 
    -0.3813757992351, -1.70572162896999, 1.00321046322335, -0.758813794873949, 
    -0.966033445643038, 0.11876111343571, -1.2312333727132, -0.836123526615442, 
    0.137868615951057, -1.05143917652043, -0.556162009526374, 
    0.112943588135347, 0.519639378656973, 0.509584036685177, 
    2.3654758915317, -0.909132042928306, 1.03005322426147, -0.848223487352064, 
    0.6186242007649, -0.705721628969986, 2.00321046322335, 0.241186205126052, 
    0.0339665543569619, 1.11876111343571, -0.231233372713198, 
    0.163876473384558, 1.13786861595106, -0.0514391765204339, 
    0.443837990473626), Experiment = c(1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
    2, 2, 2, 2, 2, 2, 2, 2, 2)), .Names = c(""SID"", ""Prep"", ""Day"", 
""Score"", ""NewVariable"", ""Experiment""), row.names = c(""1"", ""2"", 
""6"", ""7"", ""13"", ""14"", ""16"", ""17"", ""21"", ""22"", ""28"", ""29"", ""31"", 
""32"", ""36"", ""37"", ""43"", ""44"", ""11"", ""23"", ""61"", ""71"", ""131"", 
""141"", ""161"", ""171"", ""211"", ""221"", ""281"", ""291"", ""311"", ""321"", 
""361"", ""371"", ""431"", ""441""), class = ""data.frame"")
</code></pre>

<p>My best attempt so far would involve creating the two corresponding wide datasets, e.g.</p>

<pre><code>library(reshape2)
dcast(example,SID+Prep~Day+Experiment,value.var=""Score"")
dcast(example,SID+Prep~Day+Experiment,value.var=""NewVariable"")
</code></pre>

<p>... then manually gluing them back together.</p>

<p>Is there a better way?</p>
PostId: 47736",PostDate: 2013-01-15 05:19:51.0,"Text: I see. The calls to `dcast()` should then probably be `dcast(example, SID+Prep ~ Day+Experiment, value.var=""Score"")` and `dcast(example, SID+Prep ~ Day+Experiment, value.var=""NewVariable"")`.",NO
8660,"Body: <p>I've generated a user test to compare two methods: M1 and M2. I generate 40 test cases and show the result of each method on test case to 20 individuals, side by side, the individuals don't know what result came from which method. For each test case each person has to say if the result computed by M1 is better or M2 is better or they are equally good.</p>

<p>I want to know if M1 is better than M2. I add up all the results and generate 3-D histogram, votes for M1, votes for tie, and votes for M2. </p>

<p>If I only looked at M1 and M2 as 2-D histogram. I know that if M1 and M2 were equally good this histogram would be uniform. Then I'll just perform $\\chi^2$ test.</p>

<p>What I don't know how to model are the votes for tie. Here are two options I've thought of:</p>

<ul>
<li>The basis of chi-squared test is that histograms are mutually
exclusive and add up to one. It seems like the votes for tie can be
split in two and added to each M1 and M2 (and ties removed), but this
does not seem very principled.</li>
<li>Another option is that I could just ignore the ties, that seems
flawed because it breaks the ""add up to one"" property. For example if
I had (M1:2, ties:98 M2:0) the difference between both methods would
be not statistically significant.</li>
</ul>

<p>What else can I do? Am I looking at this incorrectly? This seems like a common problem people would face when modeling user votes. What is correct way to model the ties? </p>
PostId: 33746",PostDate: 2012-08-06 05:26:50.0,"Text: @gung: From wikipedia: http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test. I quote: ""It tests a null hypothesis stating that the frequency distribution of certain events observed in a sample is consistent with a particular theoretical distribution. The events considered must be mutually exclusive and have total probability 1. A common case for this is where the events each cover an outcome of a categorical variable. A simple example is the hypothesis that an ordinary six-sided die is “fair”, i. e., all six outcomes are equally likely to occur.""",YES
5807,"Body: <p>I have a problem like the following:</p>

<p>1) There are six measurements for each individual with large within-subject variance </p>

<p>2) There are two groups (Treatment and Control)</p>

<p>3) Each group consists of 5 individuals</p>

<p>4) I want to perform a significance test comparing the two groups to know if the group means are different from one another.</p>

<p>The data looks like this:
<img src=""http://i.stack.imgur.com/55V9J.png"" alt=""http://s10.postimg.org/p9krg6f3t/examp.png""></p>

<p>And I have run some simulations using this code which does t tests to compare the group means. The group means were calculated by taking the means of the individual means. <strong>This ignores within-subject variability</strong>:</p>

<pre><code> n.simulations&lt;-10000
    pvals=matrix(nrow=n.simulations,ncol=1)
    for(k in 1:n.simulations){
      subject=NULL
      for(i in 1:10){
        subject&lt;-rbind(subject,as.matrix(rep(i,6)))
      }
      #set.seed(42)

      #Sample Subject Means
      subject.means&lt;-rnorm(10,100,2)

      #Sample Individual Measurements
      values=NULL
      for(sm in subject.means){
        values&lt;-rbind(values,as.matrix(rnorm(6,sm,20)))
      }

      out&lt;-cbind(subject,values)

      #Split into GroupA and GroupB
      GroupA&lt;-out[1:30,]
      GroupB&lt;-out[31:60,]

      #Add effect size to GroupA
      GroupA[,2]&lt;-GroupA[,2]+0

      colnames(GroupA)&lt;-c(""Subject"", ""Value"")
      colnames(GroupB)&lt;-c(""Subject"", ""Value"")

      #Calculate Individual Means and SDS
      GroupA.summary=matrix(nrow=length(unique(GroupA[,1])), ncol=2)
      for(i in 1:length(unique(GroupA[,1]))){
        GroupA.summary[i,1]&lt;-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
        GroupA.summary[i,2]&lt;-sd(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
      }
      colnames(GroupA.summary)&lt;-c(""Mean"",""SD"")


      GroupB.summary=matrix(nrow=length(unique(GroupB[,1])), ncol=2)
      for(i in 1:length(unique(GroupB[,1]))){
        GroupB.summary[i,1]&lt;-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
        GroupB.summary[i,2]&lt;-sd(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
      }
      colnames(GroupB.summary)&lt;-c(""Mean"",""SD"")

      Summary&lt;-rbind(cbind(1,GroupA.summary),cbind(2,GroupB.summary))
      colnames(Summary)[1]&lt;-""Group""

      pvals[k]&lt;-t.test(GroupA.summary[,1],GroupB.summary[,1], var.equal=T)$p.value
    }
</code></pre>

<p>And here is code for plots:</p>

<pre><code>#Plots
par(mfrow=c(2,2))
boxplot(GroupA[,2]~GroupA[,1], col=""Red"", main=""Group A"", 
        ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
        xlab=""Subject"", ylab=""Value"")
stripchart(GroupA[,2]~GroupA[,1], vert=T, pch=16, add=T)
#abline(h=mean(GroupA[,2]), lty=2, lwd=3)

for(i in 1:length(unique(GroupA[,1]))){
  m&lt;-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
  ci&lt;-t.test(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])$conf.int[1:2]

  points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
  segments(i-.2,
           ci[1],i-.2,
           ci[2], lwd=4, col=""Grey""
  )
}
legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")


boxplot(GroupB[,2]~GroupB[,1], col=""Light Blue"", main=""Group B"", 
        ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
        xlab=""Subject"", ylab=""Value"")
stripchart(GroupB[,2]~GroupB[,1], vert=T, pch=16, add=T)
#abline(h=mean(GroupB[,2]), lty=2, lwd=3)

for(i in 1:length(unique(GroupB[,1]))){
  m&lt;-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
  ci&lt;-t.test(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])$conf.int[1:2]

  points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
  segments(i-.2,
           ci[1],i-.2,
           ci[2], lwd=4, col=""Grey""
  )
}
legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")


boxplot(Summary[,2]~Summary[,1], col=c(""Red"",""Light Blue""), xlab=""Group"", ylab=""Average Value"",
        ylim=c(.9*min(Summary[,2]),1.1*max(Summary[,2])),
        main=""Individual Averages"")
stripchart(Summary[,2]~Summary[,1], vert=T, pch=16, add=T)

points(.9, mean(GroupA.summary[,1]), pch=15,cex=1.5, col=""Grey"")
segments(.9,
         t.test(GroupA.summary[,1])$conf.int[1],.9,
             t.test(GroupA.summary[,1])$conf.int[2], lwd=4, col=""Grey""
)

points(1.9, mean(GroupB.summary[,1]), pch=15,cex=1.5, col=""Grey"")
segments(1.9,
         t.test(GroupB.summary[,1])$conf.int[1],1.9,
             t.test(GroupB.summary[,1])$conf.int[2], lwd=4, col=""Grey""
)
legend(""topleft"", legend=c(""Group Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")


hist(pvals, breaks=seq(0,1,by=.05), col=""Grey"",
     main=c(paste(""# sims="", n.simulations),
            paste(""% Sig p-values="",100*length(which(pvals&lt;0.05))/length(pvals)))
)
</code></pre>

<p>Now, it seems to me that because each individual mean is an estimate itself, that we should be less certain about the group means than shown by the 95% confidence intervals indicated by the bottom-left panel in the figure above. Thus the p-values calculated are underestimating the true variability and should lead to increased false-positives if we wish to extrapolate to future data.</p>

<p>So what is the correct way to analyze this data?</p>

<p><strong>Bonus:</strong></p>

<p>The example above is a simplification. For the actual data: </p>

<p>1) The within-subject variance is positively correlated with the mean. </p>

<p>2) Values can only be multiples of two. </p>

<p>3) The individual results are not roughly normally distributed. They suffer from zero floor effect, and have long tails at the positive end. </p>

<p>4) Number of Subjects in each group are not necessarily equal. </p>

<p>Previous literature has used the t-test ignoring within-subject variability and other nuances as was done for the simulations above. Are these results reliable? If I can extract some means and standard errors from the figures how would I calculate the ""correct"" p-values.</p>

<p><strong>EDIT:</strong></p>

<p>Ok, here is what <em>actual</em> data looks like. There is also three groups rather than two:</p>

<p><img src=""http://i.stack.imgur.com/k1xWd.png"" alt=""enter image description here""></p>

<p>dput() of data:</p>

<pre><code>structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 
3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 
6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 10, 
10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 
12, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 
15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 
18, 18, 18, 18, 18, 2, 0, 16, 2, 16, 2, 8, 10, 8, 6, 4, 4, 8, 
22, 12, 24, 16, 8, 24, 22, 6, 10, 10, 14, 8, 18, 8, 14, 8, 20, 
6, 16, 6, 6, 16, 4, 2, 14, 12, 10, 4, 10, 10, 8, 4, 10, 16, 16, 
2, 8, 4, 0, 0, 2, 16, 10, 16, 12, 14, 12, 8, 10, 12, 8, 14, 8, 
12, 20, 8, 14, 2, 4, 8, 16, 10, 14, 8, 14, 12, 8, 14, 4, 8, 8, 
10, 4, 8, 20, 8, 12, 12, 22, 14, 12, 26, 32, 22, 10, 16, 26, 
20, 12, 16, 20, 18, 8, 10, 26), .Dim = c(108L, 3L), .Dimnames = list(
    NULL, c(""Group"", ""Subject"", ""Value"")))
</code></pre>

<p><strong>EDIT 2:</strong></p>

<p>In response to Henrik's answer:
So if I instead perform anova followed by TukeyHSD procedure on the individual averages as shown below, I could interpret this as underestimating my p-value by about 3-4x? </p>

<p>My goal with this part of the question is to understand how I, as a reader of a journal article, can better interpret previous results given their choice of analysis method. For example they have those ""stars of authority"" showing me 0.01>p>.001. So if i accept 0.05 as a reasonable cutoff I should accept their interpretation? The only additional information is mean and SEM.</p>

<pre><code>#Get Invidual Means
summary=NULL
for(i in unique(dat[,2])){
sub&lt;-which(dat[,2]==i)
summary&lt;-rbind(summary,cbind(
dat[sub,1][3],
dat[sub,2][4],
mean(dat[sub,3]),
sd(dat[sub,3])
)
)
}
colnames(summary)&lt;-c(""Group"",""Subject"",""Mean"",""SD"")

TukeyHSD(aov(summary[,3]~as.factor(summary[,1])+ (1|summary[,2])))

#      Tukey multiple comparisons of means
#        95% family-wise confidence level
#    
#    Fit: aov(formula = summary[, 3] ~ as.factor(summary[, 1]) + (1 | summary[, 2]))
#    
#    $`as.factor(summary[, 1])`
#             diff       lwr       upr     p adj
#    2-1 -0.672619 -4.943205  3.597967 0.9124024
#    3-1  7.507937  1.813822 13.202051 0.0098935
#    3-2  8.180556  2.594226 13.766885 0.0046312
</code></pre>

<p><strong>EDIT 3:</strong>
I think we are getting close to my understanding. Here is the simulation described in the comments to @Stephane:</p>

<pre><code>#Get Subject Means
means&lt;-aggregate(Value~Group+Subject, data=dat, FUN=mean)

#Initialize ""dat2"" dataframe
dat2&lt;-dat

#Initialize within-Subject sd
s&lt;-.001
pvals=matrix(nrow=10000,ncol=2)

for(j in 1:10000){
#Sample individual measurements for each subject
temp=NULL
for(i in 1:nrow(means)){
temp&lt;-c(temp,rnorm(6,means[i,3], s))
}

#Set new values
dat2[,3]&lt;-temp

#Take means of sampled values and fit to model
dd2 &lt;- aggregate(Value~Group+Subject, data=dat2, FUN=mean)
fit2 &lt;- lm(Value~Group, data=dd2)

#Save sd and pvalue
pvals[j,]&lt;-cbind(s,anova(fit2)[[5]][5])

#Update sd
s&lt;-s+.001
}

plot(pvals[,1],pvals[,2], xlab=""Within-Subject SD"", ylab=""P-value"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/gMMDY.png"" alt=""enter image description here""></p>
PostId: 72453",PostDate: 2013-10-10 13:47:37.0,"Text: @StéphaneLaurent If you want variance weights and nested structure (i.e., replicates per participants) you can simply use `lme`instead of `gls`. Or what sepaks against `lme`?",YES
14251,"Body: <p>Wikipedia says that almost all Measurment error models can be formulated as follows:</p>

<blockquote>
  <p>Usually measurement error models are described using the latent variable model latent variables approach.  If ''y'' is the response variable and ''x'' are observed values of the regressors, then we assume there exist some latent variables ''y*'' and ''x*'' which follow the model's ""true"" functional relationship ''g'', and such that the observed quantities are their noisy observations:</p>
  
  <p>$\\begin{cases}
  x = x^* + \\eta, \\\\
  y = y^* + \\varepsilon, \\\\
  y^* = g(x^*\\!,w\\,|\\,\\theta),
\\end{cases}$</p>
  
  <p>where $\\theta$ is the model's parameter and ''w'' are those regressors which are assumed to be error-free (for example when linear regression contains an intercept, the regressor which corresponds to the constant certainly has no ""measurement errors""). </p>
  
  <p>The variables ''y'', ''x'', ''w'' are all ''observed'', meaning that the statistician possesses a data set of ''n'' statistical units $y_i$, $x_i$, $w_i$ which follow the data collection|data generating process described above; the latent variables ''$x^*$'', ''$y^*$'', ''$\\epsilon$'', and ''$\\eta$'' are not observed however.</p>
</blockquote>

<p>I am dealing with a particular problem that arises from measuring an electronic device for which I can make a similar model. The difference is that I don't observe $y$, but I know $h$ and $g$ and I can observe different $x \\in \\mathbb{R^p}$ for which hold:</p>

<p>$$\\begin{cases}
  x = x^* + \\eta, \\\\
  h(x^*) = g(x^*\\!,w\\,|\\,\\theta)\\\\
\\end{cases}$$</p>

<p>I want to <strong>name</strong> this problem so I can properly study it. Is it still some kind of Variable-with-error-model or is it something different?</p>

<p>The actual formulation of my problem is:
$$\\begin{cases}
  x = x^* + \\eta, \\\\
  h(x^*) = f(x^*)\\theta_1+\\theta_2\\\\
\\end{cases}$$</p>

<p>I observe $x_i$ i know that the equation holds and i want to make inference about $\\theta_1$ and $\\theta_2$</p>
PostId: 66046",PostDate: 2013-07-30 18:14:10.0,Text: may be @user61038 knows something about these?,YES
12543,"Body: <p>The question asks for a random lower triangular binary matrix which represents the edges.  Here is one way, with the number of vertices (as <code>v</code>) and number of edges stipulated and edges chosen independently and uniformly among the available ones:</p>

<pre><code>DAG.random &lt;- function(v, nedges=1) {
    edges.max &lt;- v*(v-1)/2
    # Assert length(v)==1 &amp;&amp; 1 &lt;= v
    # Assert 0 &lt;= nedges &lt;= edges.max
    index.edges &lt;- lapply(list(1:(v-1)), function(k) rep(k*(k+1)/2, v-k)) 
    index.edges &lt;- index.edges[[1]] + 1:edges.max
    graph.adjacency &lt;- matrix(0, ncol=v, nrow=v)
    graph.adjacency[sample(index.edges, nedges)] &lt;- 1
    graph.adjacency
}
</code></pre>

<p>This solution uses R's simultaneous use of matrix and array indexing.  <code>index.edges</code> computes a list of the <em>array</em> indexes corresponding to the lower triangular elements of <code>graph.adjacency</code>.  (This is done by finding the gaps in these indexes left by the diagonal and upper triangular entries and shifting the sequence <code>c(1,2,3,...)</code> by those gaps.)  Sampling the indexes (without replacement) does the trick.</p>

<h3>Edit</h3>

<p>One <em>ad-hoc</em> way to create connected DAGs is by adjoining a connected ""skeleton"" to the random DAG in a way that keeps it acyclic.  For instance, we can create a linear skeleton <em>post hoc</em>:</p>

<pre><code>set.seed(17)
n &lt;- 6; e &lt;- 4
a &lt;- DAG.random(n, e)
a[seq(from=2, by=n+1, length.out=n-1)] &lt;- 1
</code></pre>

<p>Here's the adjacency matrix:</p>

<pre><code>&gt; a
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    0    0    0    0    0    0
[2,]    1    0    0    0    0    0
[3,]    0    1    0    0    0    0
[4,]    1    1    1    0    0    0
[5,]    0    0    0    1    0    0
[6,]    0    0    0    1    1    0
</code></pre>
PostId: 28046",PostDate: 2012-05-08 18:43:28.0,"Text: @Cardinal My solution is not any more efficient: it has to compute a list of v(v-1)/2 indexes.  Yours has to generate a permutation of a list of that length, which only takes time directly proportional to the list length.",NO
4242,"Body: <p>I am not sure the beta will be likely to be particularly suitable as a way of dealing with this problem -- ""The number of plays until ..."" is clearly a count. It's an integer, and there is no upper limit on values where you get positive probability. </p>

<p>By contrast the beta distribution is continuous, and on a bounded interval, so it would seem to be an unusual choice. If you moment match a scaled beta, the cumulative distribution functions might perhaps approximate not so badly in the central body of the distribution. However, some other choice is likely to substantially better further into either tail. </p>

<p>If you have either an expression for the probabilities or simulations from the distribution (which you presumably need in order to find an approximating beta), why would you not use those directly?</p>

<hr>

<p>If your interest is in finding expressions for probabilities or the probability distribution of the number of tosses required, probably the simplest idea is to work with probability generating functions. These are useful for deriving functions from recursive relationships among probabilities, which functions (the pgf) in turn allow us to extract whatever probabilities we require.</p>

<p>Here's a post with a good answer taking the algebraic approach, which explains both the difficulties and makes good use of pgfs and recurrence relations. It has specific expressions for mean and variance in the ""two successes in a row"" case:</p>

<p><a href=""http://math.stackexchange.com/questions/73758/probability-of-n-successes-in-a-row-at-the-k-th-bernoulli-trial-geometric"">http://math.stackexchange.com/questions/73758/probability-of-n-successes-in-a-row-at-the-k-th-bernoulli-trial-geometric</a></p>

<p>The four successes case will be substantially more difficult of course. On the other hand, $p = \\frac{1}{2}$ does simplify things somewhat.</p>

<p>-- </p>

<p>If you just want numerical answers, simulation is relatively straightforward. The probability estimates could be used directly, or alternatively it would be reasonable to smooth the simulated probabilities.</p>

<p>If you must use an approximating distribution, you can probably choose something that does pretty well. </p>

<p>It's possible a mixture of negative binomials (the 'number of trials' version rather than 'the number of successes' version) might be reasonable. Two or three components should be expected to give a good approximation in all but the extreme tail. </p>

<p>If you want a single continuous distribution for an approximation, there may be better choices than the beta distribution; it would be something to investigate.</p>

<hr>

<p>Okay, I've since done a little algebra, some playing with recurrence relations, some simulation and even a little thinking.</p>

<p>To a very good approximation, I think you can get away with simply specifying the first four nonzero probabilities (which is easy), computing the next few handfuls of values via recurrence (also easy) and then using a geometric tail once the recurrence relation has smoothed out the initially less smooth progression of probabilities.</p>

<p>It looks like you can use the geometric tail to very high accuracy past k=20, though if you're only worried about say 4 figure accuracy you could bring it in earlier.</p>

<p>This should let you compute the pdf and cdf to good accuracy. </p>

<p>I'm a little concerned - my calculations give that the mean number of tosses is 30.0, and the standard deviation is 27.1; if I understood what you mean by ""x"" and ""u"", you got 40 and 28 in your tossing. The 28 looks okay but the 40 seems quite a way off what I got... which makes me worry I did something wrong.</p>

<p>====</p>

<p>NOTE: Given the complexities between first time and subsequent times that we encountered, I just want to be absolutely certain now that we are counting the same thing.</p>

<p>Here's a short sequence, with the ends of the '4 or more H' sequences marked (pointing to the gap between flips immediately after the last H)</p>

<pre><code>       \\/                     \\/
TTHHHHHHTTHTTTTTHHTTHTTHHTHHHHHT...
       /\\                     /\\
</code></pre>

<p>Between those two marks I count 23 flips; that is as soon as the previous sequence of (6 in this case) H's ends, we start counting at the immediately following T and then we count right to the end of the sequence of 5 H's (in this case) that ends the next sequence, giving a count of 23 in this case.</p>

<p>Is that how you count them?</p>

<hr>

<p>Given the above is correct, this is what the probability function of the number of tosses after one run of at least 4 heads is complete until the next run of at least 4 heads is complete looks like:</p>

<p><img src=""http://i.stack.imgur.com/y1KMB.png"" alt=""Coin probs""></p>

<p>At first glance it looks like it's flat for the first few values, then has a <a href=""http://en.wikipedia.org/wiki/Geometric_distribution"">geometric</a> tail, but that impression is not quite accurate - it takes a while to settle down to an effectively geometric tail.</p>

<p>I am working on coming up with a suitable approximation you can use to answer whatever questions you'd have about probabilities associated with this process to good accuracy that is at the same time as simple as possible. I have a pretty good approximation that should work (that I have already checked against a simulation of a billion coin tosses) but there's some (small but consistent) bias in the probabilities the approximation gives in part of the range and I'd like to see if I can get an extra digit of accuracy out of it. </p>

<p>It may be that the best way to do it is simply to give you a table of the probability function and cdf out to a point beyond which a geometric distribution can be used.</p>

<p>However, it would help if you can give some idea of the range of things you need to use the approximation for.</p>

<hr>

<p>I hope to follow through on the pgf approach, but it's possible someone else will be more proficient with them than me and can do not just the 4-case but other cases.</p>
PostId: 56200",PostDate: 2013-04-16 00:16:07.0,"Text: To perhaps clarify things further. A distribution that adjusts or appromixate simulation that takes into account the fluxuation of 4 more successful heads would be ideal. For example, if the populatoin mean is 150 flips for 4 consecutive heads. If 4 or more heads came at the 8th flip. It's unlikely that another 4 or more heads wouldn't come in another 20 or so flips (I'm just guessing) and perhaps be closer to the mean. Something that would get me the probability for when its probable 4 consecutive heads will occur within a certain range of tosses would be AMAZING.",YES
8956,"Body: <p>If you have multiple dimensions to your data, where it is not possible to visualize 
them together, how to decide if your model should be linear or polynomial?</p>
PostId: 64672",PostDate: 2013-07-18 05:37:05.0,Text: @TooTone surely the model is not the same thing as the cost function. That would be bizarre. OP should clarify.,NO
2390,"Body: <p>Asymptotically the McNemar test statistic follows a chi-squared distribution with 1 degree of freedom. So, if $x_{obs}$ is your observed McNemar test statistic,  the $p$ value is</p>

<p>$p = \\text{Pr}\\left\\{ \\chi^2_1 &gt; x_{obs}\\right\\}$</p>

<p>but perhaps this is all the jargon and what not that you were saying you were confused about. What the statement above is saying is that the $p$ value is a probability calcuated under the chi-squared $(\\chi^2_1)$ distribution. You can think of a probability as being an area under a particular curve (think back to integrals in calculus). </p>

<p>The curve in question here is the chi-squared density
<img src=""http://i.stack.imgur.com/Fm7p2.jpg"" alt=""enter image description here""></p>

<p>The $p$ value is the blue shaded area under the curve I have plotted. </p>

<p>The curve is defined by: $f(x)=\\dfrac{e^{-x/2}}{\\sqrt{2x}\\Gamma(1/2)}$</p>

<p>Now sure you can try to calculate this by hand to get p-value but most programming languages have built in functions to calculate are under density curves. In R you can do this:</p>

<pre><code>&gt; pchisq(1.75, df=1,lower.tail=FALSE)
[1] 0.1858767
</code></pre>
PostId: 99103",PostDate: 2014-05-17 20:28:57.0,"Text: Sorry, are you sure that it is a _two-sided_ p-value?",YES
10668,"Body: <p>My situation:</p>

<ul>
<li>small sample size: 116 </li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 44</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
<li>most cases in the sample and most variables have missing values.</li>
</ul>

<p>Approach to feature selection chosen: LASSO</p>

<p>R's glmnet package won't let me run the glmnet routine, apparently due to the existence of missing values in my data set. There seems to be various methods for handling missing data, so I would like to know:</p>

<ul>
<li>Does LASSO impose any restriction in terms of the method of imputation that I can use?</li>
<li>What would be the best bet for imputation method? Ideally, I need a method that I could run on SPSS (preferably) or R.</li>
</ul>

<p>UPDATE1: It became clear from some of the answers below that I have do deal with more basic issues before considering imputation methods. I would like to add here new questions regarding that. On the the answer suggesting the coding as constant value and the creation of a new variable in order to deal with 'not applicable' values and the usage of group lasso:</p>

<ul>
<li>Would you say that if I use group LASSO, I would be able to use the approach suggested to continuous predictors also to categorical predictors? If so, I assume it would be equivalent to creating a new category - I am wary that this may introduce bias.</li>
<li>Does anyone know if R's glmnet package supports group LASSO? If not, would anyone suggest another one that does that in combination with logistic regression? Several options mentioning group LASSO can be found in CRAN repository, any suggestions of the most appropriate for my case? Maybe SGL?</li>
</ul>

<p>This is a follow-up on a previous question of mine (<a href=""http://stats.stackexchange.com/questions/102892/how-to-select-a-subset-of-variables-from-my-original-long-list-in-order-to-perfo"">How to select a subset of variables from my original long list in order to perform logistic regression analysis?</a>).</p>

<p>OBS: I am not a statistician.</p>
PostId: 104194",PostDate: 2014-06-20 22:17:02.0,"Text: @Scortchi: About the female age variable, I see what you mean. But the issue than becomes: how would I code the absent female case if not as NA? About zeroes: yes, that was exactly my question, sorry if it was not clear. I thought that the program might have some problem in handling zero values and that it might not 'understand' what I meant with it.",NO
5505,"Body: <p>I have a data set of code patches and the bugs they produce. I'm using ordinary least squares to find a line which predicts bugs based on some attributes about the patch, such as the department which produced the code, how many issues were found in design, etc.</p>

<p>One thing is that the patches vary in size, and larger patches will (obviously) create more bugs. I'm trying to think of how I should handle this, and I came up with several options:</p>

<ol>
<li>Use patch size (lines of code changed) as an independent variable. The problem with this is that I think I really want to predict defect ""density"". E.g. the Sales department might cause one bug every 15 lines of code, but the shipping people cause one every 30 lines of code. So I would need a variable like ""department * lines of code changed"", which I worry is more sensitive to overfitting. </li>
<li>Break it into samples of equal size. E.g. if a change to 10 lines of code caused 2 bugs, I pretend that this was actually 10 changes, each of which caused .2 bugs.</li>
<li>Divide through by the size and then weight by the size. i.e. minimize $\\sum w_i(\\frac{y_i}{w_i} - f(\\vec{x_i}))^2$. </li>
</ol>

<p>Are any of these preferable, or is there another option I should be using? I assume this must be a pretty standard problem to have, but I haven't been able to find anything online.</p>
PostId: 45974",PostDate: 2012-12-15 15:46:23.0,"Text: @Peter: I found [this link](http://www.psycho.uni-duesseldorf.de/abteilungen/aap/gpower3/user-guide-by-distribution/z/poisson_regression) which talks about Poisson regression based on an ""exposure time"". It seems like if I could have my ""exposure time"" be the number of lines of code changed, this would fit nicely. Do you think so? Do you know what it's called when you do regression with a non-fixed time window? I can't find any sources on it.",YES
3849,"Body: <p>How do I approximate the following integral using MC simulation?</p>

<p>$$\\n\\int_{-1}^{1} \\int_{-1}^{1} |x-y| \\,\\mathrm{d}x \\,\\mathrm{d}y\\n$$</p>

<p>Thanks!</p>

<p><strong>Edit</strong> (Some context): I am trying to learn how to use simulation to approximate integrals, and am getting some practice done when I ran into some difficulties.</p>

<p><strong>Edit 2+3</strong>: Somehow I got confused and thought I needed to split the integral into separate parts. So, I actually figured it out:</p>

<pre><code>n &lt;- 15000
x &lt;- runif(n, min=-1, max=1)
y &lt;- runif(n, min=-1, max=1)
mean(4*abs(x-y))
</code></pre>
PostId: 17455",PostDate: 2011-10-23 21:59:38.0,"Text: Or in my admittedly weird case, because your brain *works* via simulation, which means even elaborate simulations are easier than theoretically straightforward analytical solutions. *mutter*",NO
5168,"Body: <p>I got it! Well, with help. I found the part of the book that gives steps to work through when proving the $Var \\left( \\hat{\\beta}_0 \\right)$ formula (thankfully it doesn't actually work them out, otherwise I'd be tempted to not actually do the proof). I proved each separate step, and I think it worked. </p>

<p>I'm using the book's notation, which is:
$$
SST_x = \\displaystyle\\sum\\limits_{i=1}^n (x_i - \\bar{x})^2,
$$
and $u_i$ is the error term.</p>

<p>1) Show that $\\hat{\\beta}_1$ can be written as $\\hat{\\beta}_1 = \\beta_1 + \\displaystyle\\sum\\limits_{i=1}^n w_i u_i$ where $w_i = \\frac{d_i}{SST_x}$ and $d_i = x_i - \\bar{x}$.</p>

<p>This was easy because we know that</p>

<p>\\begin{align}
\\hat{\\beta}_1 &amp;= \\beta_1 + \\frac{\\displaystyle\\sum\\limits_{i=1}^n (x_i - \\bar{x}) u_i}{SST_x} \\\\
&amp;= \\beta_1 + \\displaystyle\\sum\\limits_{i=1}^n \\frac{d_i}{SST_x} u_i \\\\
&amp;= \\beta_1 + \\displaystyle\\sum\\limits_{i=1}^n w_i u_i
\\end{align}</p>

<p>2) Use part 1, along with $\\displaystyle\\sum\\limits_{i=1}^n w_i = 0$ to show that $\\hat{\\beta_1}$ and $\\bar{u}$ are uncorrelated, i.e. show that $E[(\\hat{\\beta_1}-\\beta_1) \\bar{u}] = 0$. </p>

<p>\\begin{align}
E[(\\hat{\\beta_1}-\\beta_1) \\bar{u}] &amp;= E[\\bar{u}\\displaystyle\\sum\\limits_{i=1}^n w_i u_i] \\\\
&amp;=\\displaystyle\\sum\\limits_{i=1}^n  E[w_i \\bar{u} u_i] \\\\
&amp;=\\displaystyle\\sum\\limits_{i=1}^n w_i E[\\bar{u} u_i] \\\\
&amp;= \\frac{1}{n}\\displaystyle\\sum\\limits_{i=1}^n w_i E\\left(u_i\\displaystyle\\sum\\limits_{j=1}^n u_j\\right) \\\\
&amp;= \\frac{1}{n}\\displaystyle\\sum\\limits_{i=1}^n w_i \\left[E\\left(u_i u_1\\right) +\\cdots + E(u_i u_j) + \\cdots+ E\\left(u_i u_n \\right)\\right] \\\\
\\end{align}</p>

<p>and because the $u$ are i.i.d., $E(u_i u_j) = E(u_i) E(u_j)$ when $ j \\neq i$.</p>

<p>When $j = i$, $E(u_i u_j) = E(u_i^2)$, so we have:</p>

<p>\\begin{align}
&amp;= \\frac{1}{n}\\displaystyle\\sum\\limits_{i=1}^n w_i \\left[E(u_i) E(u_1) +\\cdots + E(u_i^2) + \\cdots + E(u_i) E(u_n)\\right] \\\\
&amp;= \\frac{1}{n}\\displaystyle\\sum\\limits_{i=1}^n w_i E(u_i^2) \\\\
&amp;= \\frac{1}{n}\\displaystyle\\sum\\limits_{i=1}^n w_i \\left[Var(u_i) + E(u_i) E(u_i)\\right] \\\\
&amp;= \\frac{1}{n}\\displaystyle\\sum\\limits_{i=1}^n w_i \\sigma^2 \\\\
&amp;= \\frac{\\sigma^2}{n}\\displaystyle\\sum\\limits_{i=1}^n w_i \\\\
&amp;= \\frac{\\sigma^2}{n \\cdot SST_x}\\displaystyle\\sum\\limits_{i=1}^n (x_i - \\bar{x}) \\\\
&amp;= \\frac{\\sigma^2}{n \\cdot SST_x} \\left(0\\right)
&amp;= 0
\\end{align}</p>

<p>3) Show that $\\hat{\\beta_0}$ can be written as $\\hat{\\beta_0} = \\beta_0 + \\bar{u} - \\bar{x}(\\hat{\\beta_1} - \\beta_1)$. This seemed pretty easy too:</p>

<p>\\begin{align}
\\hat{\\beta_0} &amp;= \\bar{y} - \\hat{\\beta_1} \\bar{x} \\\\
&amp;= (\\beta_0 + \\beta_1 \\bar{x} + \\bar{u}) - \\hat{\\beta_1} \\bar{x} \\\\
&amp;= \\beta_0 + \\bar{u} - \\bar{x}(\\hat{\\beta_1} - \\beta_1).
\\end{align}</p>

<p>4) Use parts 2 and 3 to show that $Var(\\hat{\\beta_0}) = \\frac{\\sigma^2}{n} + \\frac{\\sigma^2 (\\bar{x}) ^2} {SST_x}$:
\\begin{align}
Var(\\hat{\\beta_0}) &amp;= Var(\\beta_0 + \\bar{u} - \\bar{x}(\\hat{\\beta_1} - \\beta_1)) \\\\
&amp;= Var(\\bar{u}) + (-\\bar{x})^2 Var(\\hat{\\beta_1} - \\beta_1) \\\\
&amp;= \\frac{\\sigma^2}{n} + (\\bar{x})^2 Var(\\hat{\\beta_1}) \\\\
&amp;= \\frac{\\sigma^2}{n} + \\frac{\\sigma^2 (\\bar{x}) ^2} {SST_x}.
\\end{align}</p>

<p>I believe this all works because since we provided that $\\bar{u}$ and $\\hat{\\beta_1} - \\beta_1$ are uncorrelated, the covariance between them is zero, so the variance of the sum is the sum of the variance. $\\beta_0$ is just a constant, so it drops out, as does $\\beta_1$ later in the calculations.</p>

<p>5) Use algebra and the fact that $\\frac{SST_x}{n} = \\frac{1}{n} \\displaystyle\\sum\\limits_{i=1}^n x_i^2 - (\\bar{x})^2$:</p>

<p>\\begin{align}
Var(\\hat{\\beta_0}) &amp;= \\frac{\\sigma^2}{n} + \\frac{\\sigma^2 (\\bar{x}) ^2} {SST_x} \\\\
&amp;= \\frac{\\sigma^2 SST_x}{SST_x n} + \\frac{\\sigma^2 (\\bar{x})^2}{SST_x} \\\\
&amp;= \\frac{\\sigma^2}{SST_x} \\left( \\frac{1}{n} \\displaystyle\\sum\\limits_{i=1}^n x_i^2 - (\\bar{x})^2 \\right) + \\frac{\\sigma^2 (\\bar{x})^2}{SST_x} \\\\
&amp;=  \\frac{\\sigma^2 n^{-1} \\displaystyle\\sum\\limits_{i=1}^n x_i^2}{SST_x}
\\end{align}</p>
PostId: 64266",PostDate: 2013-07-14 00:23:22.0,"Text: @QuantIbex I didn't see your previous comment. Why isn't step 1 obvious? Since the $x_i$ are known, can't we treat $SST_x$ as a constant? Sorry I'm so stupid at this.",NO
9962,"Body: <p>I´m modeling the effect of pregnancy on the outcome of a disease (dead-alive). Approx 40% of the patients did become pregnant after the time of diagnosis-but at different points in time. So far I´ve done KM plots showing a clear protective effect of pregnancy on survival and also a regular Cox model-however these have been modeled using only a dichotomised pregnancy variable and assuming the effect is present from the time of diagnosis which is clearly unrealistic since the median time to pregnancy is 4 years from diagnosis.</p>

<p>What kind of model would absorb the effect of multiple pregnancies at different time points after diagnosis? Would it be correct to model the pregnancies interacting with time (which would require some serious data reconstruction-any automated software that could help with this?) or is there another preferred modeling strategy for these problems? Also what is the preferred plotting strategy for these problems?</p>
PostId: 43554",PostDate: 2012-11-14 07:16:29.0,Text: interesting question (+1)... this recent paper might be of help: http://www.ncbi.nlm.nih.gov/pubmed/21328605,YES
8019,"Body: <p>I'm a beginner in BUGS. I'm trying to code the Dawid Skene model in BUGS. The model is as follows:
<img src=""http://i.stack.imgur.com/hQRti.png"" alt=""""></p>

<p>Currently, I'm adapting the code from Stan, <a href=""https://groups.google.com/forum/#!topic/stan-dev/w3IkGXLqs6k"" rel=""nofollow"">here</a>. </p>

<p>Here is the code:</p>

<pre><code>model { 
   for (i in 1:I) {
     z[i] ~ dcat(pi[])
   }
   for (n in 1:M) {
     y[n] ~ dcat(theta[jj[n],z[ii[n]]])
   }
} 
</code></pre>

<p>The data are</p>

<pre><code>list(I=4, M=12, ii=c(1,1,1,2,2,2,3,3,3,4,4,4), jj=c(1,2,3,1,2,3,1,2,3,1,2,3),y=c(1,0,1,1,1,0,1,1,1,0,0,1))
</code></pre>

<p>I cannot compile the code since I keep getting the error: ""variable pi is not declared in model or data"". Do you have any idea how I can model the Dawid Skene model in BUGS?</p>

<p>Edit: the context of this model is as follows: there are J annotators who need to label N items into K categories. There are M annotations ($M=N \\times J$). The label of annotator $i$ for item $j$ is denoted as $r_{i,j}$. I need to find the actual correct label of each item $z_i$ and the competency of each annotator $\\Theta$.</p>

<p>Edit 2: $\\Theta$ is a $J \\times K \\times K$ matrix where each element in the matrix is in $[0,1]$. In addition, if $i,j$ are fixed, $\\sum_{k=1:K}\\Theta[i,j,k] = 1$. $\\rho$ is an array of $K$ elements. Each element in the array is in $[0,1]$ and the sum of the elements is 1. </p>

<p>Edit 3: $r[i,j]$ is a categorical distribution taking $\\Theta[i,j,:]$ as its parameter and returns an integer in $[1,K]$. $z[i]$ is also a categorical distribution which takes $\\rho$ as a parameter and returns an integer in $[1,K]$. </p>

<p>The question is that I don't know how to model $\\Theta$ in BUGS. Since it is also a parameter that I want to infer, I suppose I need to model $\\Theta$ as a matrix in the model (which I don't know how).</p>
PostId: 113011",PostDate: 2014-08-23 21:00:37.0,"Text: $\\pi$ is actually $\\rho$ in the diagram. Since I'm a beginner in BUGS, how do I know where $\\rho$ is a parameter or not ? Thanks.",YES
8781,"Body: <p>If I have a large number of relationships I want to check that may or may not be linear (as in the bottom row of image below), is there a method that will reject the hypothesis the scatter plot is showing white noise? I want to check for the presence of any relationship without knowing what the form of that may be. What method/algorithm of measuring deviations from white noise captures the most different types of deviation?</p>

<p><img src=""http://i.stack.imgur.com/AjMDQ.png"" alt=""enter image description here""></p>

<p>image from:
<a href=""https://en.wikipedia.org/wiki/Correlations"" rel=""nofollow"">https://en.wikipedia.org/wiki/Correlations</a></p>
PostId: 77008",PostDate: 2013-11-19 15:00:52.0,"Text: There are some interesting questions of human perception here, to be sure, but as a practical matter *what your eyes think they see* and *what matters for your application* are rarely the same. I'm just seeking clarification concerning whether your question is about psychology or statistics. If the latter, then please describe the kinds of patterns you need to detect.",YES
13815,"Body: <p>We have a bivariate normal process where $X \\sim N(\\mu_x, \\sigma), \\, Y \\sim N(\\mu_y, \\sigma)$, with no covariance.</p>

<p>$(\\mu_x, \\mu_y)$ are unknown.</p>

<p>(For convenience we can assert that $\\sigma = 1$, or that we have a good estimate for its value.)</p>

<p>We are trying to characterize the distance between our sample center and the true center $(\\mu_x, \\mu_y)$ as a function of shots sampled <em>n</em>.</p>

<p>Because we don't care about the location of the true center, only our distance from it, we assert that $\\mu_x = \\mu_y = 0$ and look at the random variable $R(n) =  \\sqrt{\\overline{x_i}^2 + \\overline{y_i}^2}$ -- the distance between sample center and true center.</p>

<p><strong>Question:</strong> How can we characterize the confidence interval of <em>R(n)</em>?</p>

<p>Note that $R(n) \\ge 0$ and $E[R(n)] \\to 0$ as $n \\to \\infty$</p>

<p>I have Monte Carlo estimates of both the mean and standard deviation of <em>R(n)</em> for small <em>n</em>.</p>

<p>I want to calculate confidence levels and intervals for <em>R(n)</em>.  I.e., given <em>n</em> and confidence level 90% what is the confidence interval of a sample <em>R(n)</em> about its population mean?</p>

<p>I don't believe this is amenable to CLT analysis because the values are bounded at 0.</p>

<p>I suppose I could Monte Carlo the edf since I'm only interested in $n \\in [2, 30]$, and the edf must scale with $\\sigma$ or $\\sigma^2$.  But first I want to make sure I'm not missing something obvious or a known closed-form expression.</p>
PostId: 88171",PostDate: 2014-02-27 18:20:00.0,"Text: Just as a wrap up for statisticians: It the following correct? You want to find a confidence interval for the parameter $\\theta := \\sqrt{\\mu_x^2+\\mu_y^2}$, where $(\\mu_x, \\mu_y)$ is the (unknown) center of spherical normal with known $\\sigma$?",YES
11974,"Body: <p>The <a href=""http://en.wikipedia.org/wiki/Sleeping_Beauty_problem"">Sleeping Beauty Problem</a>.</p>

<p>This is a recent invention; it was heavily discussed within a small set of philosophy journals over the last decade.  There are staunch advocates for two very different answers (the ""Halfers"" and ""Thirders"").  It raises questions about the nature of belief, probability, and conditioning, and has caused people to invoke a quantum-mechanical ""many worlds"" interpretation (among other bizarre things).</p>

<p>Here is the statement from Wikipedia:</p>

<blockquote>
  <p>Sleeping Beauty volunteers to undergo the following experiment and is
  told all of the following details. On Sunday she is put to sleep. A
  fair coin is then tossed to determine which experimental procedure is
  undertaken. If the coin comes up heads, Beauty is awakened and
  interviewed on Monday, and then the experiment ends. If the coin comes
  up tails, she is awakened and interviewed on Monday and Tuesday. But
  when she is put to sleep again on Monday, she is given a dose of an
  amnesia-inducing drug that ensures she cannot remember her previous
  awakening. In this case, the experiment ends after she is interviewed
  on Tuesday.</p>
  
  <p>Any time Sleeping beauty is awakened and interviewed, she is asked,
  ""What is your credence now for the proposition that the coin landed
  heads?""</p>
</blockquote>

<p>The Thirder position is that S.B. should respond ""1/3"" (this is a simple Bayes' Theorem calculation) and the Halfer position is that she should say ""1/2"" (because that's the correct probability for a fair coin, obviously!).  IMHO, the entire debate rests on a limited understanding of probability, but isn't that the whole point of exploring apparent paradoxes?</p>

<p><img src=""http://i.stack.imgur.com/izaK5.png"" alt=""Prince Florimond Finds the Sleeping Beauty""></p>

<p><em>(Illustration from <a href=""http://upload.wikimedia.org/wikipedia/commons/thumb/7/78/Prince_Florimund_finds_the_Sleeping_Beauty_-_Project_Gutenberg_etext_19993.jpg/418px-Prince_Florimund_finds_the_Sleeping_Beauty_-_Project_Gutenberg_etext_19993.jpg"">Project Gutenberg</a>.)</em></p>

<hr>

<p>Although this is not the place to try to resolve paradoxes--only to state them--I don't want to leave people hanging and I'm sure most readers of this page don't want to wade through the philosophical explanations.  We can take a tip from <a href=""http://bayes.wustl.edu/etj/science.pdf.html"">E. T. Jaynes</a>, who replaces the question “how can we build a mathematical model of human common sense”—which is something we need in order to think through the Sleeping Beauty problem—by “How could we build a machine which would carry out useful plausible reasoning, following clearly defined principles expressing an idealized common sense?”  Thus, if you like, replace S. B. by Jaynes' thinking robot.  You can <em>clone</em> this robot (instead of administering a fanciful amnesiac drug) for the Tuesday portion of the experiment, thereby creating a clear model of the S. B. setup that can be unambiguously analyzed.  Modeling this in a standard way using statistical decision theory then reveals there are really <em>two</em> questions being asked here (<em>what is the chance a fair coin lands heads?</em> and <em>what is the chance the coin has landed heads, conditional on the fact that you were the clone who was awakened?</em>).  The answer is either 1/2 (in the first case) or 1/3 (in the second, using Bayes' Theorem).  No quantum mechanical principles were involved in this solution :-).</p>

<hr>

<h3>References</h3>

<p><strong>Arntzenius, Frank</strong> (2002).  <em>Reflections on Sleeping Beauty</em>.  Analysis 62.1 pp 53-62.
Elga, Adam (2000).  Self-locating belief and the Sleeping Beauty Problem.  Analysis 60 pp 143-7.</p>

<p><strong>Franceschi, Paul</strong> (2005).  <em>Sleeping Beauty and the Problem of World Reduction</em>.  Preprint.</p>

<p><strong>Groisman, Berry</strong> (2007).  <em>The end of Sleeping Beauty’s nightmare</em>.</p>

<p><strong>Lewis, D</strong> (2001).  <em>Sleeping Beauty: reply to Elga</em>.  Analysis 61.3 pp 171-6.</p>

<p><strong>Papineau, David</strong> and <strong>Victor Dura-Vila</strong> (2008).  <em>A thirder and an Everettian: a reply to Lewis’s ‘Quantum Sleeping Beauty’</em>.</p>

<p><strong>Pust, Joel</strong> (2008).  <em>Horgan on Sleeping Beauty</em>.  Synthese 160 pp 97-101.</p>

<p><strong>Vineberg, Susan</strong> (undated, perhaps 2003).  <em>Beauty’s Cautionary Tale</em>.</p>

<p>All can be found (or at least were found several years ago) on the Web.</p>
PostId: 23812",PostDate: 2012-02-28 17:02:58.0,"Text: Do you think it's equally effective to formulate the solution in terms of ""base units""? By that I mean, you have to consider whether the base unit is the person, or the interview. 1/2 of persons will have had a head, but 1/3 of interviews will. Then to choose our base unit, we can revisit the question and phrase as ""What is the chance that this interview is associated with a 'heads' result?""",NO
11825,"Body: <p>Suppose we have $N$ independent random variables $X_1$, $\\ldots$, $X_n$ with finite means $\\mu_1 \\leq \\ldots \\leq \\mu_N$ and variances $\\sigma_1^2$, $\\ldots$, $\\sigma_N^2$. I am looking for distribution-free bounds on the probability that any $X_i \\neq X_N$ is larger than all other $X_j$, $j \\neq i$. In other words, if for simplicity we assume the distributions of $X_i$ are continuous (such that $P(X_i = X_j) = 0$), I am looking for bounds on:
$$
P( X_i = \\max_j X_j ) \\enspace.
$$
If $N=2$, we can use Chebyshev's inequality to get:
$$
P(X_1 = \\max_j X_j) = P(X_1 &gt; X_2) \\leq \\frac{\\sigma_1^2 + \\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2 + (\\mu_1 - \\mu_2)^2} \\enspace.
$$
I would like to find some simple (not necessarily tight) bounds for general $N$, but I have not been able to find (esthetically) pleasing results for general $N$.</p>

<p>Please note that the variables are not assumed to be i.i.d.. Any suggestions or references to related work are welcome.</p>

<hr>

<p>Update: recall that by assumption, $\\mu_j \\geq \\mu_i$. We can then use the above bound to arrive at:
$$
P(X_i = \\max_j X_j) \\leq \\min_{j &gt; i} \\frac{\\sigma_i^2 + \\sigma_j^2}{\\sigma_i^2 + \\sigma_j^2 + (\\mu_j - \\mu_i)^2} \\leq \\frac{\\sigma_i^2 + \\sigma_N^2}{\\sigma_i^2 + \\sigma_N^2 + (\\mu_N - \\mu_i)^2} \\enspace.
$$
This implies
$$
( \\mu_N - \\mu_i ) P( X_i = \\max_j X_j ) \\leq (\\mu_N - \\mu_i) \\frac{\\sigma_i^2 + \\sigma_N^2}{\\sigma_i^2 + \\sigma_N^2 + (\\mu_N - \\mu_i)^2} \\leq \\frac{1}{2} \\sqrt{ \\sigma_i^2 + \\sigma_N^2 } \\enspace.
$$
This, in turn, implies
$$
\\sum_{i=1}^N \\mu_i P( X_i = \\max_j X_j ) \\geq \\mu_N - \\frac{N}{2} \\sqrt{ \\sum_{i=1}^{N-1} (\\sigma_i^2 + \\sigma_N^2) } \\enspace.
$$
I am now wondering whether this bound can be improved to something that does not depend linearly on $N$. For instance, does the following hold:
$$
\\sum_{i=1}^N \\mu_i P( X_i = \\max_j X_j ) \\geq \\mu_N - \\sqrt{ \\sum_{i=1}^N \\sigma_i^2 } \\enspace?
$$
And if not, what could be a counterexample?</p>
PostId: 30245",PostDate: 2012-06-11 14:10:14.0,"Text: @cardinal : Lets denote $p_i = P( X_i = \\max_j X_j )$. I'm mainly interested in **lower** bounds for $\\sum p_i \\mu_i$. However, I would like to express this lower bound as $\\sum p_i \\mu_i \\geq \\mu_N - A$ for some $A$ (preferrably, the smallest possible $A$). This translates into a question on **upper** bounds on $p_i$, since we can rewrite this as $\\sum p_i (\\mu_N - \\mu_i) \\leq A$. Note that $\\mu_N - \\mu_i$ is non-negative by definition. For know, I'm mainly focussing on finding the lowest expression for $A$ that depends only on means and variances, but other ideas are welcome.",YES
13164,"Body: <p>These data have a short tail compared to a lognormal distribution, not unlike a Gamma distribution:</p>

<pre><code>set.seed(17)
par(mfcol=c(1,1))
x &lt;- rgamma(500, 1.9)
qqnorm(log(x), pch=20, cex=.8, asp=1)
abline(mean(log(x)) + .1,1.2*sd(log(x)), col=""Gray"", lwd=2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/HrtDQ.png"" alt=""QQPlot""></p>

<p>Nevertheless, because the data <em>are</em> strongly right-skewed, we can expect the largest values to play an important role in estimating the mean and its confidence interval.  Therefore <strong>we should anticipate that a lognormal (LN) estimator will tend to <em>overestimate</em> the mean and the two confidence limits</strong>.</p>

<p>Let's check and, for comparison, use the usual estimators: that is, the sample mean and its normal-theory confidence interval.  Note that the usual estimators rely only on the approximate normality of the <em>sample mean</em>, not of the data, and--with such a large dataset--can be expected to work well.  To do this, we need a slight modification of the <code>ci</code> function:</p>

<pre><code>ci &lt;- function (x, alpha=.05) {
  z &lt;- -qnorm(alpha / 2)
  y &lt;- log(x); n &lt;- length(y); s2 &lt;- var(y)
  m &lt;- mean(y) + s2 / 2
  d &lt;- z * sqrt(s2 / n + s2 * s2 / (2 * (n - 1)))
  exp(c(mean=m, lcl=m-d, ucl=m+d))
}
</code></pre>

<p>Here is a parallel function for the normal-theory estimates:</p>

<pre><code>ci.u &lt;- function(x, alpha=.05) {
 mean(x) + sd(x) * c(mean=0, lcl=1, ucl=-1) / sqrt(length(x)) * qnorm(alpha/2)
}
</code></pre>

<p>Applied to this simulated dataset, the outputs are</p>

<pre><code>&gt; ci(x)
   mean     lcl     ucl 
2.03965 1.87712 2.21626 
&gt; ci.u(x)
   mean     lcl     ucl 
1.94301 1.81382 2.07219 
</code></pre>

<p>The normal-theory estimates produced by <code>ci.u</code> look a little closer to the true mean of $1.9$, but it's hard to tell from one dataset which procedure tends to work better. To find out, let's simulate a lot of datasets:</p>

<pre><code>trial &lt;- function(n=500, k=1.9) {
  x &lt;- rgamma(n, k)
  cbind(ci(x), ci.u(x))
}
set.seed(17)
sim &lt;- replicate(5000, trial())
</code></pre>

<p>We are interested in comparing the outputs to the true mean of $1.9$.  A panel of histograms is revealing in that regard:</p>

<pre><code>xmin &lt;- min(sim)
xmax &lt;- max(sim)
h &lt;- function(i, ...) {
  b &lt;- seq(from=floor(xmin*10)/10, to=ceiling(xmax*10)/10, by=0.1)
  hist(sim[i,], freq=TRUE, breaks=b, col=""#a0a0FF"", xlab=""x"", xlim=c(xmin, xmax), ...)
  hist(sim[i,sim[i,] &gt;= 1.9], add=TRUE,freq=TRUE, breaks=b, col=""#FFa0a0"",
                              xlab=""x"", xlim=c(xmin, xmax), ...)
}
par(mfcol=c(2,3))
h(1, main=""LN Estimate of Mean"")
h(4, main=""Sample Mean"")
h(2, main=""LN LCL"")
h(5, main=""LCL"")
h(3, main=""LN UCL"")
h(6, main=""UCL"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/CtvrN.png"" alt=""Histograms""></p>

<p>It is now clear that the lognormal procedures tend to overestimate the mean and the confidence limits, whereas the usual procedures do a good job.  We can estimate the coverages of the confidence interval procedures:</p>

<pre><code>&gt; sapply(c(LNLCL=2, LCL=5, LNUCL=3, UCL=6), function(i) sum(sim[i,] &gt; 1.9)/dim(sim)[2])
 LNLCL    LCL  LNUCL    UCL 
0.2230 0.0234 1.0000 0.9648 
</code></pre>

<p>This calculation says:</p>

<ul>
<li><p>The LN lower limit will fail to cover the true mean about 22.3% of the time (instead of the intended 2.5%).</p></li>
<li><p>The usual lower limit will fail to cover the true mean about 2.3% of the time, close to the intended 2.5%.</p></li>
<li><p>The LN upper limit will <em>always</em> exceed the true mean (instead of falling below it 2.5% of the time as intended).  This makes it a two-sided 100% - (22.3% + 0%) = 77.7% confidence interval instead of a 95% confidence interval.</p></li>
<li><p>The usual upper limit will fail to cover the true mean about 100 - 96.5 = 3.5% of the time.  This is a little greater than the intended value of 2.5%.  The usual limits therefore comprise a two-sided 100% - (2.3% + 3.5%) = 94.2% confidence interval instead of a 95% confidence interval.</p></li>
</ul>

<p>The reduction of the nominal coverage from 95% to 77.7% for the lognormal interval is terrible.  The reduction to 94.2% for the usual interval is not bad at all and can be attributed to the effect of the skewness (of the raw data, not of their logarithms).</p>

<p>We have to conclude that <strong>further analyses <em>of the mean</em> should <em>not</em> assume lognormality.</strong></p>

<p>Be careful! Some procedures (such as prediction limits) will be more sensitive to skewness than these confidence limits for the mean, so their skewed distribution may need to be accounted for. However, it looks unlikely that lognormal procedures will perform well with these data for practically any intended analysis.</p>
PostId: 35671",PostDate: 2012-09-04 14:19:58.0,"Text: You are bootstrapping, which is a good idea, but you're not getting relevant information: you need to draw samples (with replacement) of *the same size* as your original.  I am surprised that you are getting such good coverages with the lognormal procedure: unless your data are quite different from those portrayed in the question, I think there may be an error in your code. Are you perhaps not using the resamples to estimate *all* the parameters of the distribution?",YES
10403,"Body: <p>Luckily someone has posted some <a href=""http://www.panix.com/~murphy/bdata.txt"">genuine birthday data</a> with a <a href=""http://www.panix.com/~murphy/bday.html"">bit of discussion</a> of a related question (is the distribution uniform).  We can use this and resampling to show that the answer to your question is apparently 23 - the same as the <a href=""http://en.wikipedia.org/wiki/Birthday_problem"">theoretical answer</a>.</p>

<pre><code>&gt; x &lt;- read.table(""bdata.txt"", header=T)
&gt; birthday &lt;- data.frame(date=as.factor(x$date), count=x$count)
&gt; summary(birthday) 
      date         count     
 101    :  1   Min.   : 325  
 102    :  1   1st Qu.:1266  
 103    :  1   Median :1310  
 104    :  1   Mean   :1314  
 105    :  1   3rd Qu.:1362  
 106    :  1   Max.   :1559  
 (Other):360                 
&gt; results &lt;- rep(0,50)
&gt; reps &lt;-2000 # big number needed as there is some instability otherwise
&gt; for (i in 1:50)
+ {
+ count &lt;- 0
+ for (j in 1:reps)
+ {
+ samp &lt;- sample(birthday$date, i, replace=T, prob=birthday$count)
+ count &lt;- count + 1*(max(table(samp))&gt;1)
+ }
+ results[i] &lt;- count/reps
+ }
&gt; results
 [1] 0.0000 0.0045 0.0095 0.0220 0.0210 0.0395 0.0570 0.0835 0.0890 0.1165
[11] 0.1480 0.1770 0.1955 0.2265 0.2490 0.2735 0.3105 0.3350 0.3910 0.4165
[21] 0.4690 0.4560 0.5210 0.5310 0.5745 0.5975 0.6240 0.6430 0.6950 0.7015
[31] 0.7285 0.7510 0.7690 0.8025 0.8225 0.8280 0.8525 0.8645 0.8685 0.8830
[41] 0.8965 0.9020 0.9240 0.9435 0.9350 0.9465 0.9545 0.9655 0.9600 0.9665
</code></pre>
PostId: 22012",PostDate: 2012-01-31 10:15:50.0,"Text: It's probably not clear, except to R cognoscenti, that the code in previous comments by @Xi'an and myself simulates the OP's situation.   Running it establishes that the chance of 9 or more people sharing a birthday, out of 360 *randomly* chosen from a uniformly distributed population, is only around 40 out of 100,000.  The most likely value for the maximum number of shared birthdays is 5.",YES
1631,"Body: <p>A Rank Histogram (or Talagrand Diagram) is a neat way of measuring whether your numerical model is giving appropriate variance. It's used for weather and climate forcasting, where you only have one observational series, and many model series (an ensemble). It's <a href=""http://www.eumetcal.org/resources/ukmeteocal/verification/www/english/msg/ver_prob_forec/uos4b/uos4b_ko1.htm"" rel=""nofollow"">described pretty clearly here</a>. Basically, you take a handful of runs of your model, then for each timestep/gridpoint, whatever, you calculate how the observations rank relative to the model ensemble. Then you plot a histogram of those ranks. If the histogram is u-shaped, then variance is too low (obs are rank high or low too often), if the histogram looks kind of gaussian, then the variance is too high (obs rarely ranks high or low), and if the histogram is flat, then your variance is spot-on (obs have a similar variance as the ensemble).</p>

<p>Examples from <a href=""http://www.eumetcal.org/resources/ukmeteocal/temp/msgcal/www/english/msg/ver_prob_forec/uos4b/uos4b_ko1.htm"" rel=""nofollow"">http://www.eumetcal.org/resources/ukmeteocal/temp/msgcal/www/english/msg/ver_prob_forec/uos4b/uos4b_ko1.htm</a>:</p>

<p><img src=""http://i.stack.imgur.com/4nSdP.jpg"" alt=""Example rank histograms""></p>

<p>So the question is, <strong>which distribution</strong> should I fit to this kind of data, and <strong>why</strong>? The latter part of the question is more important, because I <em>think</em> that the correct distribution is <a href=""https://en.wikipedia.org/wiki/Beta-binomial_distribution"" rel=""nofollow"">the Beta-binomial distribution</a>, but I'm unfamiliar with this area.</p>
PostId: 32370",PostDate: 2012-07-16 08:31:53.0,"Text: What leads you to think that the 'correct distribution would be beta-binomial'? I see nothing that really suggests it couldn't in practice be almost anything. The examples shown might be more-or-less described by a flexible discrete distribution like a beta-binomial but that doesn't make them beta-binomial. No doubt you could make an argument that would imply a beta-binomial, though I think I don't know enough about the whole area to judge whether it's tenable.",YES
6232,"Body: <p>I have been given an excel document with many rows full of numbers, some rows are marked.</p>

<p>Each row represents a case in the clinic, each column represents a research test parameter.</p>

<p>I need to find, what's so special about these marks that make them marked, and it has to be in a unified pattern, so I can create an algorithm to find the same exceptions in the Excel file.</p>

<p>I'll be using C# to write the program, but I'm sure the question is about what to look into rather than how to actually write the code.</p>

<p>I'm brand new to this website, I'm a programmer, not a math genius, so please be patient. I'll check for comments and respond as soon as possible.</p>
PostId: 58921",PostDate: 2013-05-13 21:39:04.0,"Text: @user603 I don't know what percentage, but do I have to know? Since I'm new to stats, can you please try to explain? Columns like 8-9 params. I added a couple of lines to my answer.",NO
12852,"Body: <p>There are a range of possibilities described by J.W. Gillard in <a href=""http://www.cardiff.ac.uk/maths/resources/Gillard_Tech_Report.pdf"">An Historical Overview
of Linear Regression with Errors in both Variables</a> </p>

<p>If you are not interested in details or reasons for choosing one method over another, just go with the simplest, which is to draw the line through the centroid $(\\bar{x},\\bar{y})$ with slope $\\hat{\\beta}=s_y/s_x$, i.e. the ratio of the observed standard deviations (making the sign of the slope the same as the sign of the covariance of $x$ and $y$); as you can probably work out, this gives an intercept on the $y$-axis of $\\hat{\\alpha}=\\bar{y}-\\hat{\\beta}\\bar{x}.$  </p>

<p>The merits of this particular approach are </p>

<ol>
<li>it gives the same line comparing $x$ against $y$ as $y$ against $x$, </li>
<li>it is scale-invariant so you do not need to worry about units,</li>
<li>it lies between the two ordinary linear regression lines </li>
<li>it crosses them where they cross each other at the centroid of the observations, and </li>
<li>it is very easy to calculate.</li>
</ol>

<p>The slope is the geometric mean of the slopes of the two ordinary linear regression slopes. It is also what you would get if you standardised the $x$ and $y$ observations, drew a line at 45&deg; (or 135&deg; if there is negative correlation) and then de-standardised the line.  It could also be seen as equivalent to making an implicit assumption that the variances of the two sets of errors are proportional to the variances of the two sets of observations; as far as I can tell, you claim not to know which way this is wrong. </p>

<p>Here is some R code to illustrate: the red line in the chart is OLS regression of $Y$ on $X$, the blue line is OLS regression of $X$ on $Y$, and the green line is this simple method. Note that the slope should be about 5.</p>

<pre><code>X0 &lt;- 1600:3600
Y0 &lt;- 5*X0 + 700
X1 &lt;- X0 + 400*rnorm(2001)
Y1 &lt;- Y0 + 2000*rnorm(2001)
slopeOLSXY  &lt;- lm(Y1 ~ X1)$coefficients[2]     #OLS slope of Y on X
slopeOLSYX  &lt;- 1/lm(X1 ~ Y1)$coefficients[2]   #Inverse of OLS slope of X on Y
slopesimple &lt;- sd(Y1)/sd(X1) *sign(cov(X1,Y1)) #Simple slope
c(slopeOLSXY, slopeOLSYX, slopesimple)         #Show the three slopes
plot(Y1~X1)
abline(mean(Y1) - slopeOLSXY  * mean(X1), slopeOLSXY,  col=""red"")
abline(mean(Y1) - slopeOLSYX  * mean(X1), slopeOLSYX,  col=""blue"")
abline(mean(Y1) - slopesimple * mean(X1), slopesimple, col=""green"")
</code></pre>
PostId: 7666",PostDate: 2011-02-26 21:48:43.0,"Text: @Henry, your definition of $\\hat{\\beta}$ doesn't make any sense to me. Are some ""hats"" missing?",YES
13177,"Body: <p>Think of this as a comparison between the $t$-test/Wilcoxon test and the Mood median test.  The median test uses optimum classification (above or below the median for a continuous variable) so that it only loses $\\frac{1}{\\pi}$ of the information in the sample.  Dichotomization at a point different from the median will lose much more information.  Using an improper scoring rule such as proportion classified ""correctly"" is at most $\\frac{2}{\\pi}$ or about $\\frac{2}{3}$ efficient.  This results in selection of the wrong features and finding a model that is bogus.</p>
PostId: 91200",PostDate: 2014-03-24 18:44:49.0,Text: Yes but you are in my opinion missing a subtle point.  The analyst does not possess the utility function.  Only the decision maker does.  So our output should normally be $Prob(Y = 1 | X=x)$.,YES
3230,"Body: <p>For a cloud of i.i.d. standard normals, try shrinking the points toward the origin logarithmically. That is, multiply the coordinates of each point by $\\log(1+\\alpha\\, d)/(\\alpha\\, d)$, where $d$ is the distance of the point from the origin, and $\\alpha$ controls the amount of shrinkage, with larger values shrinking more.</p>
PostId: 79985",PostDate: 2013-12-18 06:43:30.0,"Text: No special reason. I also considered multiplying by $\\mathrm{arcsinh}(\\alpha\\,d)/(\\alpha\\,d)$, but something based on the square (or some other) root might do just as well.",YES
7436,"Body: <p>I think your idea of repeating cross-validation many times is right on the mark.</p>

<p>Repeat your CV let's say 1000 times, each time splitting your data into 10 parts (for 10-fold CV) in a different way (do <em>not</em> shuffle the labels). You will get 1000 estimations of the classification accuracy. Of course you will be reusing the same data, so these 1000 estimations are not going to be independent. But this is akin to bootstrap procedure: you can take <em>standard deviation</em> over these accuracies as the <em>standard error of the mean</em> of your overall accuracy estimator. Or a 95% percentile interval as the 95% confidence interval.</p>

<p>Alternatively, you can combine cross-validation loop and the bootstrap loop, and simply select random (maybe stratified random) 10% of your data as a test set, and do this 1000 times. The same reasoning as above applies here as well. However, this will result in higher variance over repetitions, so I think the above procedure is better.</p>

<p>If your misclassification rate is 0.00, your classifier makes zero errors and if this happens on each bootstrap iteration, you will get zero wide confidence interval. But this would simply mean that your classifier is pretty much perfect, so good for you.</p>
PostId: 88193",PostDate: 2014-02-27 21:06:18.0,"Text: @Tal: I did *not* suggest to do a shuffling procedure, but I see now that my wording was unfortunate and I will improve it now. What I suggested, is repeated CV with different splits. This should give confidence intervals, as desired by OP. Your worry about tiny sample size I don't understand: random classifier will show chance level performance on repeated CV, even if there are only 4 samples.",YES
12222,"Body: <p>The condition number is the statistic you want to look at. It's an exhaustive measure of colinearity of your design space and much cheaper to compute than the VIF. The formula is:</p>

<p>$$2\\log\\left(\\frac{d_1(X_s)}{d_p(X_s)}\\right) (1)$$</p>

<p>where $d_p$ ($d_1$) is the smallest (largest) singular value of $X_s$ and $X_s$ are the centred, re-scaled variables.</p>

<p>Reading your <a href=""http://stats.stackexchange.com/questions/56645/what-is-the-fastest-method-for-determining-collinearity-degree?noredirect=1#comment109235_56645"">comment</a>: this is what I was suspecting. To compute the condition # you don't need all 100e6 data points. Just samples, randomly, in the order of 100e3 (by experimenting with simulated datasets you can convince yourself that to get reliable results you should target about 5*k, where k is the # of non collinear variables so even 100e3 is very large already). </p>

<p>That should already give you a pretty good idea what variables are causing collinearity. </p>

<p>Also, you have specialized algorithm to compute the first and last few singular values and the last few singular vector only. There are algorithms do get these w/o computing the full SVD of $X$ (SVD-S). However, I don't know if these are implemented in R so to make the example below accessible I'll just use a small example and the classical SVD from R.</p>

<p>A high condition number (typically when the ratio (1) is larger than 10) tells you that $X_s'X_s$ is ill-conditioned, that there are components that can be re-written as (near) linear combination of the other variables. Below I give a brief (small scale) example of how SVD can be used to uncover such relations.</p>

<pre><code>n&lt;-100
p&lt;-20
#non-ill conditioned part of the dataset.
x&lt;-matrix(rnorm(n*p),nc=p)
x&lt;-scale(x)
#introduce a variable that causes x to be 
#ill conditioned.
y&lt;-x%*%c(rnorm(3),rep(0,p))[1:p]
y&lt;-scale(y)
x&lt;-cbind(x,y)
p&lt;-ncol(x)

A&lt;-svd(x,nu=0)
#x is ill-conditioned: this ratio is larger 
#than 10. (step 1)
2*log(A$d[1]/A$d[p])

#check what is causing it: (step 2)
round(A$v[,ncol(A$v)],2)
#you can write the last variable as (.23*x_1+.5*x_2-.45*x_3)/(-.7) [1]
#here the relation is exact because:
min(A$d)
    #is 0. if min(A$d)&gt;0 then this gives you how much there is noise 
#there is arround [1].

#so I remove the last variable. (step 3)
x&lt;-x[,-ncol(x)]
#no more ill-condition.
2*log(A$d[1]/A$d[p-1])
</code></pre>

<p>This is for the linear algebra of the problem when there is a single variable causing the ill-regression. In most case you will have more than one (near) exact relationship and you will have to repeat steps 1 through 3. In practice, the computational specifics will depend on how clever is the approach you use to solve the SVD problem. </p>

<p>You can make yourself an idea of how many exact relationships there are in your dataset by computing </p>

<p>$$2\\log\\left(\\frac{d_1(X_s)}{d_j(X_s)}\\right)$$</p>

<p>for all $j$'s. To do this you only need the singular values which you can get at cost $O(p^2)$ </p>
PostId: 56664",PostDate: 2013-04-20 08:41:42.0,"Text: @user603 Yes, I believe I understand it now. Thanks.",YES
632,"Body: <p>$$P(\\lambda|w) \\propto \\lambda^{s-1} \\exp(-t\\lambda) \\:|\\lambda\\Lambda|^{1/2} \\exp(-\\frac{w'(\\lambda\\Lambda)w}{2})$$</p>

<p>$$ \\propto \\lambda^{s-1}\\lambda^{k/2} \\:|\\Lambda|^{1/2}   \\exp\\left(-t\\lambda-\\frac{w'(\\lambda\\Lambda)w}{2}\\right)$$</p>

<p>where $k$ is the dimension of $w$,</p>

<p>$$ \\propto \\:|\\Lambda|^{1/2} \\lambda^{s+{k/2}-1}  \\exp\\left(-\\lambda\\left[t+\\frac{w'\\Lambda w}{2}\\right]\\right)$$</p>

<p>$$ \\propto \\:|\\Lambda|^{1/2}\\: \\cdot\\: \\lambda^{(s+\\frac{k}{2})-1}  \\exp\\left(-\\lambda\\left[t+\\frac{w'\\Lambda w}{2}\\right]\\right)$$</p>

<p>Which - up to constants - is clearly in the form of a $\\Gamma\\left(s+\\frac{k}{2},t+\\frac{w'\\Lambda w}{2}\\right)$ (if we're working in shape-rate form).</p>
PostId: 89942",PostDate: 2014-03-13 21:43:18.0,"Text: Luca, I just found another error; my star is looking a little rusty. I need a second cup of coffee this morning (well, it's morning in my part of the world). The form of the result was niggling at me (the signs didn't make sense) so I went back through it again and soon found the (basic) mistake. Now they all make sense.",YES
13525,"Body: <p>This question is (I think) simpler than most posed here, but it's beyond my ability to solve.  </p>

<p>I'm trying to calculate the probability of various outcomes for a charter school lottery.  There are two classes of entrants: students otherwise zoned for under-performing schools, and all other students. </p>

<p>Let x=the number of students zoned for under-performing schools</p>

<p>Let y=the number of students not zoned for under-performing schools</p>

<p>Let z=the number of spaces available in the school</p>

<p>It's easy enough to determine the expected number of students admitted from each class, but how can I determine the probability of various other outcomes.  For instance, how likely is it that the number of students admitted from a class is half the expected number?</p>

<p>For what it's worth, I'm a math teacher who has never had a stat, probablity, or discrete math course, although I've taught myself the basics.  If you have any suggestions for further self-guided study in probability I'd appreciate them, but right now I just need to solve this practical problem.</p>
PostId: 37760",PostDate: 2012-09-21 18:11:09.0,"Text: I thought that the other things you called out as lacking were implicit in the description I gave.  each ticket has the same chance of being drawn.  Tickets would not be replaced after drawing, because no one is admitted twice.  Every prize is the same.  Tickets are drawn until every prize is awarded.",NO
4081,"Body: <p>I am facing the following problem: I have a training sample and estimate a model on that training sample. My model is simply OLS: $y_t = a + \\beta x_t + \\varepsilon_t$. The model is estimated on points in set $t\\in T$. The training sample contains well behaved data. When forecasting with this model out of sample, there may be points that are poorly measured and thus take on extreme values. I would like to prevent my model from forecasting extreme output values at times when poorly measured points occur.  Thus for points $t \\notin T$, I would like my coefficients $(\\alpha, \\beta)$ to be less sensitive to extremes. I think the appropriate thing is to transform the data in some way (maybe through $ln$)? Maybe Box-Cox?</p>

<p>Let me illustrate. Imagine you have a sensor which functions normally 99.9% of the time but .1% of the time generates a random extreme value that has nothing to do with the measurement. Unless your training set includes that point, you are unable to tailor a model around it. However, you would like not to generate an extreme prediction out of sample when that .1% occurs.</p>

<p>I would like to know what the standard techniques are for dealing with this problem. Please provide some references as well if possible.</p>
PostId: 70943",PostDate: 2013-09-24 17:40:05.0,"Text: @Alex I read your update. Can't you get enough data to have a representative sample of that .1% of extreme values? On the other hand, based on your description of a sensor, if those values are so extreme, just set a threshold and remove them. Paraphrasing the words of Pauli, those values are not even wrong.",YES
1538,"Body: <p><strong>UPDATE:</strong> <em>Added material to respond to some issues that arose in the comments.</em></p>

<p>As stated by the reference the OP pointed to, $\\hat \\mu$ does not follow Student's z-distribution - $(\\hat \\mu -\\mu)/s$ does. But <em>when</em> these distributional results hold in finite samples? <em>Only when the $X$'s are normally distributed</em>.  </p>

<p>So you have to wonder: Under non-normality the usual t-statistic still converges to a standard normal by the Central Limit Theorem, because under the null it is a sum of random variables centered, and scaled by their standard deviation. Technically, it is the term $\\sqrt n$ that prevents it for converging to zero.</p>

<p>So where does the statistic you propose, $(\\hat \\mu -\\mu)/s$, converges to? Isn't it zero? <em>Irrespective of the true value of $\\mu$?</em><br>
So what good is it for a statistic in non-normal samples if it gradually ""loses"" its finite sample distribution and tends to the same constant?   </p>

<p><strong>ADDENDUM</strong><br>
One can search and verify that if $Z_n$ follows a Student's- $z$ distribution with $n$ degrees of freedom, and $T_{n-1}$ follows a Student's- $t$ distribution with $n-1$ degrees of freedom, then the following relation holds between them:</p>

<p>$$T_{n-1} =Z_{n} \\sqrt {n-1} \\Rightarrow Z_{n}=\\frac 1{\\sqrt {n-1}}T_{n-1}$$</p>

<p>Assume that we have a size-$n$ sample of i.i.d. random variables $\\{X_i\\}$ following $N(\\mu, \\sigma^2)$, both parameters unknown.  Setting</p>

<p>$$\\bar X_n = \\frac 1n\\sum_{i=1}^nX_i,\\;\\;\\; s^2 = \\frac 1{n-1}\\sum_{i=1}^n(X_i-\\bar X_n)^2$$</p>

<p>then we know that the random variable</p>

<p>$$\\sqrt n\\frac  {\\bar X_n -\\mu}{s} \\sim_{\\text{finite-sample}} T_{n-1}$$</p>

<p>and also that</p>

<p>$$T_{n-1} \\sim_{\\text{asympt}} N(0,1)$$</p>

<p>The important thing to note here is that $T_{n-1}$, has a) a finite sample distribution and b) an asymptotic distribution. This means that $T_{n-1}$ remains a non-trivial random variable <em>even asymptotically</em>.    </p>

<p>Let's now turn to the $Z_n$ random variable. By previous results we have </p>

<p>$$Z_n = \\frac 1{\\sqrt {n-1}} \\left({\\sqrt n}\\cdot\\frac  {\\bar X_n -\\mu}{s}\\right)  \\sim_{\\text{finite-sample}}\\frac 1{\\sqrt {n-1}}T_{n-1}$$</p>

<p>and this quantity follows a Student's-$z$ distribution. What is its asymptotic distribution? It is obvious that $Z_n$ converges to zero, since</p>

<p>$$\\lim_{n\\rightarrow \\infty} \\frac 1{\\sqrt {n-1}} = 0,\\;\\; T_{n-1} \\xrightarrow{d}N(0,1)\\;\\; \\Rightarrow Z_n \\rightarrow 0$$  </p>

<p>What does that tell us? That while $Z_n$ has a finite-sample distribution, <em>asymptotically it collapses to the  constant $0$</em>, i.e. it does <em>not</em> remain a non-trivial random variable asymptotically.  </p>

<p>So, to respond to a comment, these two statistics  are <em>not</em> asymptotically equivalent -the one remains a random variable with a distribution, the other collapses to a constant. Therefore the tests based on $T_{n-1}$ are asymptotically valid while the tests based on $Z_n$ are not.</p>

<p><em>Should we care?</em>  After all, our samples are always of finite size, why should we care what happens asymptotically? </p>

<p>Well, from the above it is obvious that while the distribution of $T_{n-1}$ ""stabilizes"" as the sample size increases (due to the existence of an asymptotic distribution), the distribution of $Z_n$ <em>does not</em>, as it gradually collapses to a constant. And we do not know <em>how</em> does this affects the shape of the collapsing distribution, and the related probabilities.</p>

<p>This means that, in order for our finite-sample test using $Z_n$ to be valid, for each $n$ we have to calculate the probabilities-critical values that interests us... Amusingly, the OP asked </p>

<blockquote>
  <p>""why even bother scaling by $\\sqrt n$?""</p>
</blockquote>

<p>Because, the answer is, </p>

<blockquote>
  <p>""if you don't bother scaling by $\\sqrt n$,then you will have to very
  much bother computing the CDF of $Z_n$ for the specific $n$""</p>
</blockquote>

<p>Which approach is more time-efficient?  </p>

<p>Now, when we move from normal to <strong>non-normal samples</strong>, the centered and scaled sample mean does not follow Student's-$t$ in the first place, and we are left only with the asymptotically valid normality result which now comes from the Central Limit Theorem (and not through Student's -$t$). In this case, using $Z_n$ even for small samples sizes introduces an unknown approximation error, and renders the statistical test results questionable.</p>

<p><strong>PS:</strong> Historically, W.S.Gosset derived first Student's -$z$ distribution (since he was interested in <em>very</em> small sample sizes), and laboriously calculated critical values for $n\\leq 10$. It was through his collaboration with R.A. Fisher that they turned from it to the Student's -$t$.</p>
PostId: 111800",PostDate: 2014-08-14 04:00:46.0,"Text: @StéphaneLaurent a) regarding the asymptotic equivalence of the tests: the distribution $N(0,1/\\sqrt{n-1})$ -to where this distribution converges as $n\\rightarrow \\infty$? b) Regarding the finite-sample tests for non-normal samples: here the usual statistic is not distributed according to Student's-t in the first place, and so how can we claim that the transformed statistic follows Student's z?",YES
6112,"Body: <p>I have a treatment group of size 30 (30 schools in California) that used a math supplemental software. In a simple analysis, I'd like to compare students' average Math growth between our treatment group and a comparable control group. There are many schools in CA that didn't use the software. I'd like the control group to include similar performing schools (their baseline scores be similar to treatment schools with a reasonable margin of error). Also, I'd like the control group sample size be 3 times of my treatment (here 90 schools). There are many many choices of 90 schools out of more than 1000 schools in CA. How would you choose your control group?</p>
PostId: 31740",PostDate: 2012-07-06 00:20:02.0,Text: @whuber: So then let's vote it up :) Thank you.,NO
14402,"Body: <p>You are a bit off I think because the formula for epsilon you seem to use relies on the neccesity to decompose your error variance into a constant term ($\\sigma^2$) and a correlation term $\\Omega$, which is the $C$ in your post. You actually have to use this $C$ as a weight for the regression. Let me explain.  </p>

<p>It is assumed you know $\\Omega$, otherwise you have to do FGLS instead. The formula you have is a result of a transformed model. First, let's look at our GLS estimators:<br>
Under the assumption that $V[\\epsilon|X]=\\sigma^2\\Omega$ you can transform the model and estimate the following
$$\\widehat{\\beta}^{GLS} = [X'\\Omega^{-1}X]^{-1}X'\\Omega^{-1}y$$
This, generally, is a BLUE estimator under the assumptions.<br>
Which has, under the above and usual assumptions this variance
$$V[\\widehat{\\beta}^{GLS}] = \\sigma^2[X'\\Omega^{-1}X]^{-1} $$
Since you know $\\Omega$, you can now calculate everything and estimate the $\\sigma^2$. To understand how have a look at how the GLS estimator is reached.<br>
You reached this result by applying a matrix $P$ to the model formula as such
$$PY = PX\\beta + P\\epsilon$$
What is P? If you know omega, you can calculate the $P$ (Cholesky decomposition) with
$$ P'P = \\Omega^{-1}$$
And then you have a consistent estimator in
$$ \\widehat{\\sigma}^2 = \\frac{(P\\hat{\\epsilon}^{OLS})'P\\hat{\\epsilon}^{OLS}}{n-k} \\\\
=  
\\frac{ \\hat{\\epsilon}'^{OLS} \\Omega^{-1} \\hat{\\epsilon}^{OLS}}{n-k} $$
This is the formula you have listed, but it implies a possible decomposition  $V[\\epsilon|X]=\\sigma^2\\Omega$. Note how it uses the residuals of OLS. This is because of said existence of a transformed and non-transformed model.  </p>

<p>You said you used $V[Y]=V$ as weight for GLS. Can you tell us the exact formula? Is it
$$\\widehat{\\beta}^{GLS} = [X'V^{-1}X]^{-1}X'V^{-1}y$$
?<br>
In that case you used $V$ instead of $C$. You mentioned that you think $V[\\epsilon]=V$ and $V=\\sigma^2C$ (I assume this is what you mean).<br>
You could go ahead and plug in $V$ instead of $C$.
But of course this is not actually correct because $V$ is not equal to $C$ as you discovered.<br>
This seems to be your problem.</p>

<p>If you know nothing about the composition of your error terms, you just can't get to that decomposition and the $C$ you need. That is the 'point' of GLS. It takes advantage of a known correlation structure $\\Omega$ or $C$ to regain BLUE estimators.<br>
So the equation you can not validate is the result of said transformed model. For this you also need to have your $\\Omega$ or $C$ as you called it and by that the $P$ to transform the model.<br>
If you do not have this, use FGLS instead.</p>
PostId: 56574",PostDate: 2013-04-19 10:53:03.0,"Text: Do you actually have the distribution of $y$, or do you just have empirical values? Because of the latter is the case, I think you need to use FGLS. Your initial question was how to get to $C$. The answer is pretty much that $C$ is the prerequisite to using GLS - which is a way to utilize knowledge of the systemic component in $\\epsilon$. You use an additional knowledge to transform the model. Knowledge of values of $y$ does not hold that information. Frankly, I think you can not use GLS. If you are sure about a systematic relationship in $\\epsilon$, FGLS can give you good results.",YES
4100,"Body: <p>I am having performance issues using the <code>MCMCglmm</code> package in R to run a mixed effects model. The code looks like this:</p>

<pre><code>MC1&lt;-MCMCglmm(bull~1,random=~school,data=dt,family=""categorical""
, prior=list(R=list(V=1,fix=1), G=list(G1=list(V=1, nu=0)))
, slice=T, nitt=iter, ,burnin=burn, verbose=F)
</code></pre>

<p>There are around 20,000 observations in the data and they are clustered in around 200 schools. I have dropped all unused variables from the dataframe and removed all other objects from memory, prior to running. The problem I have is that it takes a very long time to run, unless I reduce the iterations to an unacceptably small number. With 50,000 iterations, it takes 5 hours and I have many different models to run. So I would like to know if there are ways to speed up the code execution, or other packages I could use. I am using <code>MCMCglmm</code> because I want confidence intervals for the random effects.</p>

<p>On the other hand, I was hoping to get a new PC later this year but with a little luck I may be able to bring that forward, so I have been wondering how to best spend a limited amount of money on new hardware - more RAM, faster CPU etc. From watching the task manager I don't believe RAM is the issue (it never gets above 50% of physical used), but the CPU usage doesn't get much above 50% either, which strikes me as odd. My current setup is a intel core i5 2.66GHz, 4GB RAM, 7200rpm HDD. Is it reasonable to just get the fastest CPU as possible, at the expense of additional RAM ? I also wondered about the effect of level 3 CPU cache size on statistical computing problems like this ?</p>

<p><strong>Update:</strong> Having <a href=""http://meta.stackexchange.com/questions/137404/should-i-request-to-migrate-my-performance-related-question-from-cross-validated"">asked on meta SO</a> I have been advised to rephrase the question and post on Superuser. In order to do so I need to give more details about what is going on ""under the hood"" in MCMCglmm. Am I right in thinking that the bulk of the computations time is spent doing optimisation - I mean finding the maximum of some complicated function ? Is matrix inversion and/or other linear algebra operations also a common operation that could be causing bottlenecks ? Any other information I could give to the Superuser community would be most gratefully received.</p>
PostId: 30942",PostDate: 2012-06-22 14:51:02.0,"Text: @Macro according to [this](http://glmm.wikidot.com/pkg-comparison) _""MCMCglmm uses MCMC instead of ML to fit the model""_",NO
1616,"Body: <p>A <a href=""http://stats.stackexchange.com/questions/28787/decomposing-the-normal-distribution"">question</a> was asked whether or not two independent variables $X$ and $Y$ that take on only positive values can have $X-Y$ be a normal distribution.  I was shown that the answer is no.  But I think that this can be done with two half normals that are dependent.  But I could not quite figure out how to structure the dependence.</p>
PostId: 28815",PostDate: 2012-05-20 14:05:45.0,Text: I would still be interested in finding a way to do this with half normals.  But even though it looked to me like to would work I haven't found a way and since Dilip said his method did not involve half normals maybe it can't be done.,NO
7940,"Body: <p>I wouldn't call the definition of CIs as wrong, but they are easy to mis-interpret, due to there being more than one definition of probability.  CIs are based on the following definition of Probability (Frequentist or ontological)</p>

<p>(1)<em>probability of a proposition=long run proportion of times that proposition is observed to be true, conditional on the data generating process</em> </p>

<p>Thus, in order to be conceptually valid in using a CI, you <em>must</em> accept this definition of probability.  If you don't, then your interval is not a CI, from a theoretical point of view.</p>

<p>This is why the definition used the word <strong>proportion</strong> and NOT the word <strong>probability</strong>, to make it clear that the ""long run frequency"" definition of probability is being used.</p>

<p>The main alternative definition of Probability (Epistemological or probability as an extension of deductive Logic or Bayesian) is </p>

<p>(2)<em>probability of a proposition = rational degree of belief that the proposition is true, conditional on a state of knowledge</em></p>

<p>People often intuitively get both of these definitions mixed up, and use whichever interpretation happens to appeal to their intuition.  This can get you into all kinds of confusing situations (especially when you move from one paradigm to the other).</p>

<p>That the two approaches often lead to the same result, means that in some cases we have:</p>

<p><em>rational degree of belief that the proposition is true, conditional on a state of knowledge = long run proportion of times that proposition is observed to be true, conditional on the data generating process</em></p>

<p>The point is that it does not hold <em>universally</em>, so we cannot expect the two different definitions to always lead to the same results.  So, unless you actually work out the Bayesian solution, and then find it to be the same interval, you cannot give the interval given by the CI the interpretation as a probability of containing the true value.  And if you do, then the interval is not a Confidence Interval, but a Credible Interval.</p>
PostId: 6714",PostDate: 2011-01-30 06:15:56.0,"Text: Is (1) really the commonly accepted definition of classical probability? I find it circular, and use a different one myself. Specifically, the long-run proportion is a random variable, and only approaches the probability in probability. The definition I use is a many-worlds interpretation, in which probability is the proportion of relevant possible futures. My understanding is that quantum physics isn't at war with this interpretation, but that's difficult to ascertain with coevolutionary fields.",YES
11023,"Body: <p>One possible solution is to use a boot strapping approach, given the new set of data points, to construct a boot strap estimate and confidence interval for $\\sum_i\\hat p(x_i)$</p>

<pre><code>set.seed(44)

#Pseudo Data
prob = .2
x = sort(rnorm(100))
y = rbinom(100,1,prob)

#Fit the logistic regression model
model = glm(y~x,family=""binomial"")

#More data arrives
x.new = rnorm(10000)

#The number of bootstrap samples
B = 10000

sum.p = rep(NA,)
for(i in 1:B){
    #Create a bootstrap sample
    x.boot = sample(x.new,length(x.new)*.1,replace=TRUE)        

    #Calculate the sum of p
    sum.p[i] = sum(1/(1+exp(-(model$coef[1]+model$coef[2]*x.boot))))
}

#Get the 2.5% and 97.5% quantile from the bootstrap estimator 
lower = quantile(sum.p,prob=.025)
upper = quantile(sum.p,prob=.975)

#Construct a 95% confidence interval
ci = c(lower,upper)
</code></pre>

<p>An alternative method would be to take a Bayesian approach and recaculate $\\sum_i\\hat p(x_i)$ for every sample of $\\beta$ at every step of an MCMC type algorithm.  Then at the end of our MCMC we would have a sample of $\\sum_i\\hat p(x_i)^{(j)}$ for every $j^{th}$ step of the MCMC that we could take the quantiles of in order to obtain our 95% confidence interval for $\\sum_i\\hat p(x_i)$ .</p>

<h1>Using Scortchi's Suggestion here is the revised code:</h1>

<pre><code>#Scortchi's suggestion
set.seed(44)
prob = .2

#More data arrives
x.new = rnorm(10000)
y.new = rbinom(10000,1,prob)

#The number of bootstrap samples
B = 10000

sum.p = rep(NA,B)
for(i in 1:B){
    #Create a bootstrap sample
    index = sample(1:length(x.new),length(x.new)*.1,replace=TRUE)
    x.boot = x.new[index]       
    y.boot = y.new[index]

    model = glm(y.boot~x.boot,family=""binomial"")

    #Calculate the sum of p
    sum.p[i] = sum(1/(1+exp(-(model$coef[1]+model$coef[2]*x.boot))))
}

#Get the 2.5% and 97.5% quantile from the bootstrap estimator 
lower = quantile(sum.p,prob=.025)
upper = quantile(sum.p,prob=.975)

#Construct a 95% confidence interval
ci = c(lower,upper)
</code></pre>

<p>Now interestingly, the confidence interval from using Scortchi's suggestion results in </p>

<pre><code>&gt; ci
 2.5% 97.5% 
  174   223 
</code></pre>

<p>where as using my original code we obtain the following:</p>

<pre><code>&gt; ci
    2.5%    97.5% 
242.4727 247.0230
</code></pre>

<p>So there is clearly a difference between the two methods.</p>
PostId: 68827",PostDate: 2013-08-31 02:13:19.0,"Text: Now what you're doing is re-fitting the model to a new random predictor matrix in each simulation step. What you want to be doing is this:
(1) Fit a logistic regression model to the original observations & predictor matrix.
(2) Use the fitted probabilities from Step 1 to generate a simulated sample of observations for the original predictor matrix.
(3) Fit a new model to the simulated data from Step 2.
(4) Get predicted probabilities for the new predictor matrix using fits from Step 3 & sum them.
(5) Repeat Steps 2 to 4 many times & see the distribution of the sum of predicted probabilities.",YES
11831,"Body: <p>Suppose we have $N$ independent random variables $X_1$, $\\ldots$, $X_n$ with finite means $\\mu_1 \\leq \\ldots \\leq \\mu_N$ and variances $\\sigma_1^2$, $\\ldots$, $\\sigma_N^2$. I am looking for distribution-free bounds on the probability that any $X_i \\neq X_N$ is larger than all other $X_j$, $j \\neq i$. In other words, if for simplicity we assume the distributions of $X_i$ are continuous (such that $P(X_i = X_j) = 0$), I am looking for bounds on:
$$
P( X_i = \\max_j X_j ) \\enspace.
$$
If $N=2$, we can use Chebyshev's inequality to get:
$$
P(X_1 = \\max_j X_j) = P(X_1 &gt; X_2) \\leq \\frac{\\sigma_1^2 + \\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2 + (\\mu_1 - \\mu_2)^2} \\enspace.
$$
I would like to find some simple (not necessarily tight) bounds for general $N$, but I have not been able to find (esthetically) pleasing results for general $N$.</p>

<p>Please note that the variables are not assumed to be i.i.d.. Any suggestions or references to related work are welcome.</p>

<hr>

<p>Update: recall that by assumption, $\\mu_j \\geq \\mu_i$. We can then use the above bound to arrive at:
$$
P(X_i = \\max_j X_j) \\leq \\min_{j &gt; i} \\frac{\\sigma_i^2 + \\sigma_j^2}{\\sigma_i^2 + \\sigma_j^2 + (\\mu_j - \\mu_i)^2} \\leq \\frac{\\sigma_i^2 + \\sigma_N^2}{\\sigma_i^2 + \\sigma_N^2 + (\\mu_N - \\mu_i)^2} \\enspace.
$$
This implies
$$
( \\mu_N - \\mu_i ) P( X_i = \\max_j X_j ) \\leq (\\mu_N - \\mu_i) \\frac{\\sigma_i^2 + \\sigma_N^2}{\\sigma_i^2 + \\sigma_N^2 + (\\mu_N - \\mu_i)^2} \\leq \\frac{1}{2} \\sqrt{ \\sigma_i^2 + \\sigma_N^2 } \\enspace.
$$
This, in turn, implies
$$
\\sum_{i=1}^N \\mu_i P( X_i = \\max_j X_j ) \\geq \\mu_N - \\frac{N}{2} \\sqrt{ \\sum_{i=1}^{N-1} (\\sigma_i^2 + \\sigma_N^2) } \\enspace.
$$
I am now wondering whether this bound can be improved to something that does not depend linearly on $N$. For instance, does the following hold:
$$
\\sum_{i=1}^N \\mu_i P( X_i = \\max_j X_j ) \\geq \\mu_N - \\sqrt{ \\sum_{i=1}^N \\sigma_i^2 } \\enspace?
$$
And if not, what could be a counterexample?</p>
PostId: 30245",PostDate: 2012-06-11 14:10:14.0,"Text: Incidentally, is it a good idea to post the same question on some other sites (e.g., math.stackexchange.com), or is this perhaps even discouraged?",YES
6835,"Body: <p>I have written a function (in R-language) to perform a Theil-Sen regression with large sample approximation (please see below).  It seems to work well for the one-sided case as illustrated in Hollander &amp; Wolfe (2nd Edition Ch. 9)   </p>

<pre><code>NP.lm &lt;-function(dat, X, Y, alpha) {

  dat &lt;- dat[order(dat[,X]),]
  n &lt;- nrow(dat)    
  combos &lt;- combn(n, 2)
  i.s &lt;- combos[1,]
  j.s &lt;- combos[2,]
  Y.num &lt;- dat[j.s,Y] - dat[i.s,Y]
  X.dom &lt;- dat[j.s,X] - dat[i.s,X]      
  Z.p &lt;-  qnorm(alpha/2, lower.tail=F)
  N &lt;- (n*(n-1))/2
  s &lt;- (Y.num/X.dom)[X.dom != 0]
  C.stat &lt;- sum(sign(s))  

  slope &lt;- median(s, na.rm=TRUE)                 
  intercept &lt;- median((dat[,Y] - slope*dat[,X]), na.rm=TRUE)     

  C.star &lt;- C.stat / sqrt(n*(n-1)*((2*n)+5)/18)
  p.val &lt;- pnorm(-abs(C.star))

  out &lt;- data.frame(colnames(dat[X]), colnames(dat[Y]), slope, intercept, p.val)

  out

} 
</code></pre>

<h1>Example usage</h1>

<pre><code>set.seed(123) # to make the example fully reproducible
sigma &lt;- as.matrix(data.frame(c(1, .7), c(.7, 1)))
N &lt;- 50
dat &lt;- data.frame( matrix(rnorm(N*nrow(sigma)), N, nrow(sigma)) %*% 
    chol(sigma) ) # cor structure via Choleski Decomp.
colnames(dat) &lt;- c(""X"", ""Y"")
summary(lm(Y~X, data=dat))
NP.lm(dat=dat, X=1, Y=2, alpha=0.05)

  colnames.dat.X.. colnames.dat.Y..     slope  intercept        p.val
1                X                Y 0.6719583 0.08337517 1.227402e-07

# This p-value can't be right
</code></pre>

<p>Could someone please tell me what I am doing wrong, and how to make the test 2-sided.  </p>

<p>Thank you so much for your help!  </p>
PostId: 62209",PostDate: 2013-06-20 17:09:02.0,"Text: whuber, your point is well taken re: repeated code.  I will absolutely change this.",YES
11292,"Body: <p>Sampling with replacement $N$ items from $K$, where $N \\geq K$, what is the probability that each item in $K$ is selected at least once?</p>

<p>For example, from $10,000$ individuals, what is the probability that every birthday is represented? (assume each birthday $p = 1/365$)</p>

<p>I've been using simulations to work this out, but would like to know the general solution. Thanks!</p>
PostId: 30483",PostDate: 2012-06-14 21:18:13.0,Text: I took down my answer because I saw the flaw in the argument.  This is what I wrote before I took it down. My mistake here is that I was thinking that if I don't fill the designated k slots with one of each type I can't get all k in the N slots. But that is not true because the missing type(s) could still be in the remaining N-k slots. So this argument does not work at all and I took down my answer.,YES
5316,"Body: <p>I have a 65 samples of 21-dimensional data (pasted <a href=""http://pastebin.com/h8TzsZEK"" rel=""nofollow"">here</a>) and I am constructing the covariance matrix from it. When computed in C++ I get the covariance matrix pasted <a href=""http://pastebin.com/qj35dwAd"" rel=""nofollow"">here</a>. And when computed in matlab from the data (as shown below) I get the covariance matrix pasted <a href=""http://pastebin.com/eZb1nwa0"" rel=""nofollow"">here</a></p>

<p>Matlab code for computing cov from data:</p>

<pre><code>data = csvread('path/to/data');
matlab_cov = cov(data);
</code></pre>

<p>As you can see the difference in covariance matrices are minute (~e-07), which is probably due to numerical problems in the compiler using floating point arithmetic.</p>

<p>However, when I compute the pseudo-inverse covariance matrix from the covariance matrix produced by matlab and the one produced by my C++ code, I get widely different results. I am computing them in the same way i.e.:</p>

<pre><code>data = csvread('path/to/data');
matlab_cov = cov(data);
my_cov = csvread('path/to/cov_file');
matlab_inv = pinv(matlab_cov);
my_inv = pinv(my_cov);
</code></pre>

<p>The difference is so huge that when I am computing the mahalanobis distance from a sample (pasted <a href=""http://pastebin.com/dsyci16L"" rel=""nofollow"">here</a>) to the distribution of the 65 samples by:</p>

<p>$(65/64^2) \\times ((sample-mean)\\times {\\sum}^{-1} \\times (sample-mean)')$</p>

<p>using the different inverse covariance matrices (${\\sum}^{-1}$) I get widely different results i.e.:</p>

<pre><code> (65/(64^2))*((sample-sample_mean)*my_inv*(sample-sample_mean)')
ans =

   1.0167e+05

(65/(64^2))*((sample-sample_mean)*matlab_inv*(sample-sample_mean)')
ans =

  109.9612
</code></pre>

<p>Is it normal for the small (e-7) differences in covariance matrix to have such an effect on the computation of the pseudo-inverse matrix? And if so, what can I do to mitigate this effect? </p>

<p>Failing this, are there any other distance metrics I can use that do not involve the inverse covariance? I use the Mahalanobis distance as we know for n samples it follows a beta distribution, which I use for hypothesis testing</p>

<p>Many thanks in advance</p>

<p><strong>EDIT: Adding C++ code for calculating covariance matrix below:</strong>
The <code>vector&lt;vector&lt;double&gt; &gt;</code> represents the collection of rows from the file pasted.</p>

<pre><code>Mat covariance_matrix = Mat(21, 21, CV_32FC1, cv::Scalar(0));
    for(int j = 0; j &lt; 21; j++){
        for(int k = 0; k &lt; 21; k++){
            for(std::vector&lt;vector&lt;double&gt; &gt;::iterator it = data.begin(); it!= data.end(); it++){
                covariance_matrix.at&lt;float&gt;(j,k) += (it-&gt;at(j) - mean.at(j)) * (it-&gt;at(k) - mean[k]);
            }
            covariance_matrix.at&lt;float&gt;(j,k) /= 64; 
        }
    }
</code></pre>
PostId: 50947",PostDate: 2013-02-27 14:31:30.0,"Text: @Aly: the matrices you are looking to invert are not ""valid"" covariances matrices because they are not positive definite; numerically they even have some eigenvalues that are negative (but close to zero). I would probably just add some very small constant along the diagonal; it is a form of Tikhonov correction really ($Χ + \\lambda I$). Also don't use floats, use doubles to store your covariance matrix. (And besides you already use OpenCV, you might as well use Eigen or Armadillo..)",YES
2470,"Body: <p>Because @zaynah posted in the comments that the data are thought to follow a Weibull distribution, I'm gonna provide a short tutorial on how to estimate the parameters of such a distribution using MLE (Maximum likelihood estimation). There is a <a href=""http://stats.stackexchange.com/questions/21745/how-can-i-perform-weibull-analysis-on-monthly-recorded-data-of-wind-speeds"">similar post</a> about wind speeds and Weibull distribution on the site.</p>

<ol>
<li><a href=""http://cran.r-project.org/"" rel=""nofollow"">Download and install <code>R</code></a>, it's free</li>
<li>Optional: <a href=""http://www.rstudio.com/"" rel=""nofollow"">Download and install RStudio</a>, which is a great IDE for R providing a ton of useful functions such as syntax highlighting and more.</li>
<li>Install the packages <code>MASS</code> and <code>car</code> by typing: <code>install.packages(c(""MASS"", ""car""))</code>. Load them by typing: <code>library(MASS)</code> and <code>library(car)</code>.</li>
<li><a href=""http://cran.r-project.org/doc/manuals/R-data.pdf"" rel=""nofollow"">Import your data into <code>R</code></a>. If you have your data in Excel, for example, save them as delimited text file (.txt) and import them in <code>R</code> with <code>read.table</code>.</li>
<li>Use the function <code>fitdistr</code> to calculate the maximum likelihood estimates of your weibull distribution: <code>fitdistr(my.data, densfun=""weibull"")</code>. To see a fully worked out example, see the link at the bottom of the answer.</li>
<li>Make a QQ-Plot to compare your data with a Weibull distribution with the scale and shape parameters estimated at point 5: <code>qqPlot(my.data, distribution=""weibull"", shape=, scale=)</code></li>
</ol>

<p>The <a href=""http://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf"" rel=""nofollow"">tutorial of Vito Ricci</a> on fitting distribution with <code>R</code> is a good starting point on the matter. And there are <a href=""http://stats.stackexchange.com/search?q=fit+distribution"">numerous posts</a> on this site on the subject (see <a href=""http://stats.stackexchange.com/questions/58220/what-distribution-does-my-data-follow"">this post</a> too).</p>

<p>To see a fully worked out example of how to use <code>fitdistr</code>, have a look at <a href=""http://stackoverflow.com/questions/15303310/how-to-create-a-weibull-cumulative-distribution-function-starting-from-fitdistr"">this post</a>.</p>

<p>Let's look at an example in <code>R</code>:</p>

<pre><code># Load packages

library(MASS)
library(car)

# First, we generate 1000 random numbers from a Weibull distribution with
# scale = 1 and shape = 1.5

rw &lt;- rweibull(1000, scale=1, shape=1.5)

# We can calculate a kernel density estimation to inspect the distribution
# Because the Weibull distribution has support [0,+Infinity), we are truncate
# the density at 0

par(bg=""white"", las=1, cex=1.1)
plot(density(rw, bw=0.5, cut=0), las=1, lwd=2,
xlim=c(0,5),col=""steelblue"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/iso1j.png"" alt=""Weibull KDE""></p>

<pre><code># Now, we can use fitdistr to calculate the parameters by MLE

fitdistr(rw, densfun=""weibull"")

     shape        scale   
  1.56788999   1.01431852 
 (0.03891863) (0.02153039)
</code></pre>

<p>The maximum likelihood estimates are close to those we arbitrarily set in the generation of the random numbers. Let's compare our data using a QQ-Plot with a hypothetical Weibull distribution with the parameters that we've estimated with <code>fitdistr</code>:</p>

<pre><code>qqPlot(rw, distribution=""weibull"", scale=1.014, shape=1.568, las=1, pch=19)
</code></pre>

<p><img src=""http://i.stack.imgur.com/PdfJk.png"" alt=""QQPlot""></p>

<p>The points are nicely aligned on the line and mostly within the 95%-confidence envelope. We would conclude that our data are compatible with a Weibull distribution. This was expected, of course, as we've sampled our values from a Weibull distribution. </p>

<hr>

<h2>Estimating the $k$ (shape) and $c$ (scale) of a Weibull distribution without MLE</h2>

<p><a href=""http://journals.ametsoc.org/doi/abs/10.1175/1520-0450%281978%29017%3C0350%3AMFEWSF%3E2.0.CO%3B2"" rel=""nofollow"">This paper</a> lists five methods to estimate the parameters of a Weibull distribution for wind speeds. I'm gonna explain three of them here.</p>

<h2>From means and standard deviation</h2>

<p>The shape parameter $k$ is estimated as:
$$
k=\\left(\\frac{\\hat{\\sigma}}{\\hat{v}}\\right)^{-1.086}
$$
and the scale parameter $c$ is estimated as:
$$
c=\\frac{\\hat{v}}{\\Gamma(1+1/k)}
$$
with $\\hat{v}$ is the mean wind speed and $\\hat{\\sigma}$ the standard deviation and $\\Gamma$ is the <a href=""http://en.wikipedia.org/wiki/Gamma_function0450%281978%29017%3C0350%3AMFEWSF%3E2.0.CO%3B2"" rel=""nofollow"">Gamma function</a>.</p>

<h2>Least-squares fit to observed distribution</h2>

<p>If the observed wind speeds are divided into $n$ speed interval $0-V_{1},V_{1}-V_{2},\\ldots, V_{n-1}-V_{n}$, having frequencies of occurrence $f_{1}, f_{2},\\ldots,f_{n}$ and cumulative frequencies $p_{1}=f_{1}, p_{2}=f_{1}+f_{2}, \\ldots, p_{n}=p_{n-1}+f_{n}$, then you can fit a linear regression of the form $y=a+bx$ to the values
$$
x_{i} = \\ln(V_{i})
$$
$$
y_{i} = \\ln[-\\ln(1-p_{i})]
$$
The Weibull parameters are related to the linear coefficients $a$ and $b$ by
$$
c=\\exp\\left(-\\frac{a}{b}\\right)
$$
$$
k=b
$$</p>

<h2>Median and quartile wind speeds</h2>

<p>If you don't have the complete observed wind speeds but the median $V_{m}$ and quartiles $V_{0.25}$ and $V_{0.75}$ $\\left[p(V\\leq V_{0.25})=0.25, p(V\\leq V_{0.75})=0.75\\right]$, then $c$ and $k$ can be computed by the relations
$$
k = \\ln\\left[\\ln(0.25)/\\ln(0.75)\\right]/\\ln(V_{0.75}/V_{0.25})\\approx 1.573/\\ln(V_{0.75}/V_{0.25})
$$
$$
c=V_{m}/\\ln(2)^{1/k}
$$</p>

<h2>Comparison of the four methods</h2>

<p>Here is an example in <code>R</code> comparing the four methods:</p>

<pre><code>library(MASS)  # for ""fitdistr""

set.seed(123)
#-----------------------------------------------------------------------------
# Generate 10000 random numbers from a Weibull distribution
# with shape = 1.5 and scale = 1
#-----------------------------------------------------------------------------

rw &lt;- rweibull(10000, shape=1.5, scale=1)

#-----------------------------------------------------------------------------
# 1. Estimate k and c by MLE
#-----------------------------------------------------------------------------

fitdistr(rw, densfun=""weibull"")
shape         scale   
1.515380298   1.005562356 

#-----------------------------------------------------------------------------
# 2. Estimate k and c using the leas square fit
#-----------------------------------------------------------------------------

n &lt;- 100 # number of bins
breaks &lt;- seq(0, max(rw), length.out=n)

freqs &lt;- as.vector(prop.table(table(cut(rw, breaks = breaks))))
cum.freqs &lt;- c(0, cumsum(freqs)) 

xi &lt;- log(breaks)
yi &lt;- log(-log(1-cum.freqs))

# Fit the linear regression
least.squares &lt;- lm(yi[is.finite(yi) &amp; is.finite(xi)]~xi[is.finite(yi) &amp; is.finite(xi)])
lin.mod.coef &lt;- coefficients(least.squares)

k &lt;- lin.mod.coef[2]
k
1.515115
c &lt;- exp(-lin.mod.coef[1]/lin.mod.coef[2])
c
1.006004

#-----------------------------------------------------------------------------
# 3. Estimate k and c using the median and quartiles
#-----------------------------------------------------------------------------

med &lt;- median(rw)
quarts &lt;- quantile(rw, c(0.25, 0.75))

k &lt;- log(log(0.25)/log(0.75))/log(quarts[2]/quarts[1])
k
1.537766
c &lt;- med/log(2)^(1/k)
c
1.004434

#-----------------------------------------------------------------------------
# 4. Estimate k and c using mean and standard deviation.
#-----------------------------------------------------------------------------

k &lt;- (sd(rw)/mean(rw))^(-1.086)
c &lt;- mean(rw)/(gamma(1+1/k))
k
1.535481
c
1.006938
</code></pre>

<p>All methods yield very similar results. The maximum likelihood approach has the advantage that the standard errors of the Weibull parameters are directly given.</p>

<hr>

<h2>Using bootstrap to add pointwise confidence intervals to the PDF or CDF</h2>

<p>We can use a the non-parametric bootstrap to construct pointwise confidence intervals around the PDF and CDF of the estimated Weibull distribution. Here's an <code>R</code> script:</p>

<pre><code>#-----------------------------------------------------------------------------
# 5. Bootstrapping the pointwise confidence intervals
#-----------------------------------------------------------------------------

set.seed(123)

rw.small &lt;- rweibull(100,shape=1.5, scale=1)

xs &lt;- seq(0, 5, len=500)


boot.pdf &lt;- sapply(1:1000, function(i) {
  xi &lt;- sample(rw.small, size=length(rw.small), replace=TRUE)
  MLE.est &lt;- suppressWarnings(fitdistr(xi, densfun=""weibull""))  
  dweibull(xs, shape=as.numeric(MLE.est[[1]][13]), scale=as.numeric(MLE.est[[1]][14]))
}
)

boot.cdf &lt;- sapply(1:1000, function(i) {
  xi &lt;- sample(rw.small, size=length(rw.small), replace=TRUE)
  MLE.est &lt;- suppressWarnings(fitdistr(xi, densfun=""weibull""))  
  pweibull(xs, shape=as.numeric(MLE.est[[1]][15]), scale=as.numeric(MLE.est[[1]][16]))
}
)   

#-----------------------------------------------------------------------------
# Plot PDF
#-----------------------------------------------------------------------------

par(bg=""white"", las=1, cex=1.2)
plot(xs, boot.pdf[, 1], type=""l"", col=rgb(.6, .6, .6, .1), ylim=range(boot.pdf),
     xlab=""x"", ylab=""Probability density"")
for(i in 2:ncol(boot.pdf)) lines(xs, boot.pdf[, i], col=rgb(.6, .6, .6, .1))

# Add pointwise confidence bands

quants &lt;- apply(boot.pdf, 1, quantile, c(0.025, 0.5, 0.975))
min.point &lt;- apply(boot.pdf, 1, min, na.rm=TRUE)
max.point &lt;- apply(boot.pdf, 1, max, na.rm=TRUE)
lines(xs, quants[1, ], col=""red"", lwd=1.5, lty=2)
lines(xs, quants[3, ], col=""red"", lwd=1.5, lty=2)
lines(xs, quants[2, ], col=""darkred"", lwd=2)
#lines(xs, min.point, col=""purple"")
#lines(xs, max.point, col=""purple"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/h1HU5.png"" alt=""Weibull PDF CIs""></p>

<pre><code>#-----------------------------------------------------------------------------
# Plot CDF
#-----------------------------------------------------------------------------

par(bg=""white"", las=1, cex=1.2)
plot(xs, boot.cdf[, 1], type=""l"", col=rgb(.6, .6, .6, .1), ylim=range(boot.cdf),
     xlab=""x"", ylab=""F(x)"")
for(i in 2:ncol(boot.cdf)) lines(xs, boot.cdf[, i], col=rgb(.6, .6, .6, .1))

# Add pointwise confidence bands

quants &lt;- apply(boot.cdf, 1, quantile, c(0.025, 0.5, 0.975))
min.point &lt;- apply(boot.cdf, 1, min, na.rm=TRUE)
max.point &lt;- apply(boot.cdf, 1, max, na.rm=TRUE)
lines(xs, quants[1, ], col=""red"", lwd=1.5, lty=2)
lines(xs, quants[3, ], col=""red"", lwd=1.5, lty=2)
lines(xs, quants[2, ], col=""darkred"", lwd=2)
lines(xs, min.point, col=""purple"")
lines(xs, max.point, col=""purple"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/cQDlv.png"" alt=""Weibull CDF CIs""></p>
PostId: 60530",PostDate: 2013-05-31 14:04:46.0,"Text: @gung Thanks. I'm not aware that qqPlot from `car` calculates the MLE parameters automatically. If I generate a random variable with a weibull distribution (`rweibull`) and use the command `qqPlot(rw, distribution=""weibull"")` I get an error message saying that must provide the parameters `shape` and `scale` to `qqPlot`. Am I missing something?",YES
832,"Body: <p>**<strong>Edit:</strong> (10/26/13) More clear (hopefully) mini-rewrites added at the bottom**</p>

<p><em><strong>I'm asking this from a theoretical/general standpoint - not one that applies to a specific use case.</em></strong></p>

<p>I was thinking about this today:</p>

<p><strong>Assuming the data does not contain any measurement errors, if you're looking at a specific observation in your data and one of the measurements you recorded contains what could be considered an outlier, does this increase the probability (above that of the rest of the observations that do not contained measured outliers) that the same observation will contain another outlier in another measurement?</strong></p>

<p><em><strong>For my answer I'm looking for some sort of theorem, principle, etc. that states what I'm trying to communicate here much more elegantly.  For clearer explanations see Gino and Behacad's answers.</em></strong></p>

<p>Example: </p>

<p>Let's say you're measuring the height and circumference of a certain type of plant.  Each observation corresponds to 1 plant (you're only doing this once).</p>

<p>For height, you measure:</p>

<pre>
Obs 1  |   10 cm
Obs 2  |   9 cm
Obs 3  |   11 cm
Obs 4  |   22 cm
Obs 5  |   10 cm
Obs 6  |   9 cm
Obs 7  |   11 cm
Obs 8  |   10 cm
Obs 9  |   11 cm
Obs 10  |   9 cm
Obs 11  |   11 cm
Obs 12  |   10 cm
Obs 13  |   9 cm
Obs 14  |   10 cm
</pre>

<p>Since observation 4 contains what could be considered an outlier from the rest of the data, would the probability increase that measured circumference also contains an outlier for observation #4?</p>

<p>I understand my example may be too idealistic but I think it gets the point across...just change the measurements to anything.</p>

<h1><em><strong>Edited in attempts to make more clear:</em></strong></h1>

<p>(10/26/13)</p>

<p><strong>Version 1 Attempt of Abbreviated Question:</strong></p>

<blockquote>
  <p>In nature and in general, is there a tendency (even a weak one) that
  the <strong><em>probability is greater</em></strong> that the ""degree of variance from the
  mean in any attribute(s) of an observation"" will be <em>similar</em> to the
  ""degree of variance from the mean in any other* specific attribute of
  that same observation"" <strong><em>in comparison to the probability</em></strong> that it
  will INSTEAD be <em>more similar</em> to the ""degree of variance from the
  mean in that same* specific attribute of <em>any other observation</em>.""</p>
</blockquote>

<p><em>* next to a word means I was pairing what they reference.<br>
""Quotes"" used above mean nothing and are used simply to help section parts together/off for clarity.</em></p>

<p><strong>Version 2 Attempt of Abbreviated Question:</strong></p>

<blockquote>
  <p>In nature and in general, is variance from the mean across
  observations for one attribute¹ correlated (even with <em>extremely</em>
  loose correlation) to the variance from the mean across observations for all
  attributes¹?</p>
</blockquote>

<p><em>¹Attribute meaning measurement, quality, presence-of-either of these, and/or nearly anything else that the word ""attribute"" could even slightly represent as a word.  Include all synonyms of the word ""attribute"" as well.</em></p>
PostId: 73613",PostDate: 2013-10-23 18:17:15.0,"Text: @glen_b I attempted to add some better explanations at the bottom of my question, let me know if they help at all",YES
8224,"Body: <p>I'm not 100% sure about this either, as the post you link to mentions a lot of subtleties that I haven't considered or studied much yet...but here's an attempt to answer nevertheless.</p>

<ol>
<li><p>As you've stated it, $h(x)$ is your estimator, because you're using it to approximate $f$. ""The process of choosing the linear regression model"" is the process of choosing the estimator function, and using it to estimate $\\hat\\beta$ is just that: using the estimator to produce an estimate. That is, no, these are not components of the estimator itself; these are the process from which the estimator originates, and the process it serves, respectively.</p></li>
<li><p>The limited way of testing estimator bias with which I am amateurishly familiar is simulation testing. In such a special circumstance as this, we may actually know $\\hat\\beta_{true}$ (probably should in many such cases). Furthermore, by simulating data according to predefined parameters, we can test how different values affect the errors of our estimators. As I understand it, these systematic errors are the primary concerns in considerations of estimator bias. </p>

<p>For example, one usually wants an estimator for which the accuracy depends minimally on sample size, or at least won't lose accuracy as sample sizes increase. I think this is sometimes a problem in significance testing, in that some tests will reject the null too often when the null is actually true and the sample size is very large (e.g., the Shapiro-Wilk test). Another example of estimator bias (I think...another place where I might be mistaken) might be your typical parametric test when used in conditions that violate its assumptions. Non-normal distributions can bias parametric tests that assume normally distributed data, whereas nonparametric tests are often relatively unbiased estimators.</p>

<p>Sometimes biasing is more complex and even interactive. For example, I recently read that substituting polychoric correlations for Pearson's $r$ correlations in a matrix on which confirmatory factor analysis is to be performed can inflate (bias) standard errors of parameter estimates and $\\chi^2$ goodness-of-fit when using maximum likelihood estimation (Babakus, 1985). The choice of estimator really starts to get hairy in latent factor modeling...</p>

<p>In any case, problems like these are often discovered by simulation testing, wherein the true parameters are designated and altered systematically, random data are generated based on these settings, and estimates are found to deviate from the true values to different degrees depending on the parameters of the simulated distributions. The extent of that dependence on the distributional parameters is the estimator's sensitivity to those parameters; if the sensitivity is non-negligible, the estimator is biased when the parameters to which it is sensitive enter certain ranges. These are often not the parameters it is used to estimate! OLS multiple regression is sensitive to multicollinearity in regressors, for another example, whereas ridge regression can correct for bias somewhat when regressors are strongly related (collinear).</p></li>
</ol>
PostId: 80860",PostDate: 2013-12-30 09:46:23.0,"Text: Maybe I was a little circuitous, because I also addressed your misconception about bias (it is not usually a function of the estimand, as in your *B*(*β*) example)...but the answer I provided (one of many possible answers) is simulation testing. When you can define all the parameters for yourself, you can test the estimator's sensitivity to them across multiple simulated samples. If you find that errors change in relation to a certain parameter as you change it across simulated samples, you may have found bias. All of this info is in my answer or common practice in sim testing.",YES
8414,"Body: <p>I'm trying to do what I thought was a basic problem but doesn't seem to be working out properly.  I'm looking at disproving a claim that tides cause earthquakes, where the main mechanism that people claim is that it happens during a new or full moon, and especially during a perigee moon (when it's closest to Earth).</p>

<p>People like to give ±1 week windows which is basically half a lunar cycle, so it seems basic that the probability an earthquake would happen, given random chance, would be 50% that it would happen within a week of, e.g., a full moon.  Similarly, since perigee/apogee happens roughly on the same timescale, it seems as though it would be 50% chance that an earthquake would happen within 1 week of a perigee moon.  Put the two together and you have a 25% chance that an earthquake would happen within both a perigee and full moon, correct?</p>

<p>(Important note, <em>updated</em>:  Actual period between new/full moons averages 29.52 days, actual time between perigee moons averages 27.56 days.  <strong>However</strong>, apogee and perigee are NOT normally distributed.  Apogee happens a mode of 27.78 days (mean is 27.55±0.27), while perigee is much more asymmmetric, having a mode of 28.4 days at the peak of approximately a Lorentzian.  Half the max of the Lorentzian is ±0.08 days.  But, the mean is 27.56 with a standard deviation of 1.12; the range is 24.6-28.6 days.  I'm thinking this could throw off the modeling?)</p>

<p>Assuming that's correct, I seem to be running into issues when trying to figure out the probability that an earthquake would happen by chance within ±X days of BOTH a perigee and full moon.  I thought the equation would be simply (2*X/(# days in lunar month, 29.5))*(2*X/(# days between perigee, 27.5)).</p>

<p>However, when I do a monte carlo simulation with 500,000 randomly chosen dates within the time period of 1933 to 2012 (just happens to be when I have earthquake data), the fractions do not line up.  For example, the simulation shows that 14.4% should be within 5 days of both a full and perigee moon, but my above math says it's only 12.3%.</p>

<p>I have checked the results of my simulation against days from perigee, apogee, new moon, and full moon times.  As expected from a random distribution with a large $N$, the number of times that the simulated earthquake is a given time period away from maximum perigee/apogee/new/full moon is even.  Except for perigee, where I see a fall-off for $&gt;|±12.5|$ days from when it's closest to perigee.  I'm thinking this has to do with the non-Gaussian distribution of perigee times?  And could that account for the 2% difference at the 5-day example?</p>

<p>Is the best way to approach this, because these perigee times are a bit crazy, to simply go with the Monte Carlo results?</p>

<p>P.S. This has been updated to better reflect a correction I made in my data.  When I initially posted this, I had some incorrect full/new moon dates in my table that were throwing some results off.</p>
PostId: 35279",PostDate: 2012-08-28 20:54:01.0,Text: @StuartRobbins  How did you get 50% by chance in the first place.  Wouldn't chance occurrence be based on the expected time between earthquakes?  Also are you looking at a specific location or anywhere on earth?,YES
2722,"Body: <p>Consider the context of a dendrogram clustering. Let us call <em>original dissimilarities</em> the distances between the individuals. After constructing the dendrogram we define the <em>cophenetic dissimilarity</em> between two individuals as the distance between the clusters to which these individuals belong.  </p>

<p>Some people consider that the correlation between the original dissimilarities and the cophenetic dissimilarities (called <em>cophenetic correlation</em>) is a ""suitability index"" of the classification. This sounds totally puzzling to me. My objection does not rely on the particular choice of the Pearson correlation, but on the general idea that any link between the original dissimilarities and the cophenetic dissimilarities could be related to the suitability of the classification. </p>

<p>Do you agree with me, or could you present some argument supporting the use of the cophenetic correlation as a suitability index for the dendrogram classification ?</p>
PostId: 33066",PostDate: 2012-07-26 09:20:09.0,"Text: @Stephane, I _did_ appreciate your joke, and found it silly and irrelevant (excuse me). It was wicked, not offensive to me.",YES
14772,"Body: <p>[<strong>Response rewritten</strong>]</p>

<p>I think I was too confusing, I apologize for that. Now I am trying to give a proper answer.</p>

<p>We know that median minimize the $L_1$ norm. The formula is
$$ L_1 = \\underset{y \\in \\mathbb{R}}{\\operatorname{arg\\,min}}\\sum_{i=1}^{n}|x_i-y|$$
Also the mean minimize the $L_2$ norm. Again, the formula is
$$ L_2 = \\underset{y \\in \\mathbb{R}}{\\operatorname{arg\\,min}}\\sum_{i=1}^{n}(x_i-y)^2$$
In plain English we say that the median minimize the sum of distances and the mean minimize the sum of squares of those distances. We note also that we are in $\\mathbb{R}$. </p>

<p>My idea is that because we are in $\\mathbb{R}$, the distance function can be any particular case of <em>p-norm</em>, the result would be the same. So I generalize by saying that the distance is <em>p-norm</em> (it might be any type of distance in fact) and to finish quicker we move to $\\mathbb{R}^m$ at the same time
$$L_1 = \\underset{y \\in \\mathbb{R}^m}{\\operatorname{arg\\,min}}\\sum_{i=1}^{n}|(\\sum_{j=1}^{m}(x_{i,j}^p-y_j^p))^\\frac{1}{p}| = \\underset{y \\in \\mathbb{R}^m}{\\operatorname{arg\\,min}}\\sum_{i=1}^{n}\\|x_i-y\\|_p $$
What is important here is that it does not matter what is the value for $p$, it will be an $L_1$. [Note, as suggested by @amoeba, there are two norms, one inside another; the first one is $L_1$ applied on distances, and a nested one applied on the elements of the vectors in $\\mathbb{R}^m$].</p>

<p>Going back to your original question, the <em>geometric median</em> is defined as the point in Euclidean space which minimize the sum of distances. I believe the reason for the word geometric comes from Euclidean space and Euclidean distance (which is $\\|.\\|_2$) and minimize the sum of distances (not the squares as in the case of an $L_2$ estimator), so
$$GM = L_1 = \\underset{y \\in \\mathbb{R}^m}{\\operatorname{arg\\,min}}\\sum_{i=1}^{n}\\|x_i-y\\|_2 $$
As a final note we might choose to minimize:</p>

<ul>
<li>the sum of Manhattan distances ($L_1$ and $\\|.\\|_1$ as distance)</li>
<li>the sum of squares of Manhattan distances ($L_2$ and $\\|.\\|_1$ as distance)</li>
<li>the sum of Euclidean distances (geometric median)</li>
<li>the sum of squares of Euclidean distances ($L_2$ and $\\|.\\|_2$ as distance)
and so on.</li>
</ul>
PostId: 113245",PostDate: 2014-08-26 06:27:11.0,"Text: @whuber Okay, I agree. But here we are not minimizing the $L_1$ norm. The geometric median minimizes the $L_2$ norm. To my question, $L_2$ is relevant and rather $L_1$ seems irrelevant.",YES
7821,"Body: <p>I would like to infer the correlation between random variables $Q$ and $R$, however, I have access only to the distribution of $Q$ and the distribution of $P=Q+R$. We can see how Pearson's $\\rho$ appears here:</p>

<p>$\\sigma^2_P=\\sigma^2_Q+\\sigma^2_R+2\\sigma_Q \\sigma_R \\rho$</p>

<p>I'm basically sure with that information alone, it's not possible to estimate $\\rho$. But I thought of a cool trick. There's a treatment of $Q$ such that for an initial distribution $Q_0$, the new distribution is scaled by a constant $k_Q$ so that $k_Q Q_0=Q_k$. We can compute $k_Q$ from the ratio of $\\mathrm{E}[Q_k]/\\mathrm{E}[Q_0]$. The question is whether $R$ is also affected, and of course, we can't measure $R$ directly. We <em>can</em> measure the mean effect on $R$.</p>

<p>$\\mathrm{E}[R]=\\mathrm{E}[Q+R]-\\mathrm{E}[Q]$</p>

<p>for treated and untreated. If I make one assumption: that treatment also affects $R$ by a constant $k_R$ so that $k_R R_0=R_k$, we get:</p>

<p>$\\sigma^2_P=k_Q^2\\sigma^2_Q+k_R\\sigma^2_R+2k_Q k_R\\sigma_Q \\sigma_R \\rho$</p>

<p>We have two equations and two sets of measurements... the distribution of $Q_0$ and $Q_0+R_0$, and then $Q_k$ and $Q_k+R_k$, from which we can compute the two variances, and $k_Q$, $k_R$ from differences in the means. So we can then solve for $\\sigma_R$ and $\\rho$ using the two variance equations. And I've done that. and it gives a reasonable output...</p>

<p>The question: is there any way, given the data I have to test whether my assumption about $R$ scaling is reasonable? Again, this is based on experimental observations. I <em>cannot</em> measure the joint distribution.</p>
PostId: 114496",PostDate: 2014-09-05 23:55:44.0,"Text: Think you had it originally Glen. I can measure only $P=Q+R$ and $Q$. Edited to clarify, think the first sentence was confusing..",YES
5529,"Body: <p>I have a data set with a binary response variable as non-zero survival cells (y=0) or zero survival cells (y=1), and I would like to build a model with explanatory variable X. I know that the most straightforward method is to use logistic regression. However, in my case, it is more biologically sound to use a Poisson model. Thus, the number of survived cells follows a Poisson distribution $f(k,\\lambda)$, and the response (probability of having zero survival cell) is the Poisson distribution at $k=0$.</p>

<p>Therefore, my model can be written as $y=f(0,\\lambda)=e^{-\\lambda}$, where $\\lambda=g(\\beta,x)$. $\\lambda$ is a function of $x$ with parameter $\\beta$. I am trying to estimate $\\beta$ using MLE. </p>

<p>Now I am bit confused about which likelihood function I should use. Shall I assume that $y$ follows a binomial distribution: $y\\sim B(1,e^{-\\lambda})$, so that I could conduct MLE on the likelihood function from this binomial distribution? What about the Poisson distribution? It is not used in the estimation? Suppose $\\beta^{*}$ is the estimated parameter, which gives $\\lambda^{*}$. This means that the probability of having zero survival cells follows a $B(1,e^{-\\lambda^{*}})$ distribution and the survival cells follows a Poisson $f(k,\\lambda^{*})$ distribution?</p>
PostId: 73402",PostDate: 2013-10-21 18:55:31.0,"Text: $exp^{−\\lambda}$ is the probability of counting zero survival cells, given the individual cell survival probability $exp^{−\\beta x}$ and the expected number of counted survived cell as $Nexp^{−\\beta x}$. Are these two definitions the same? I am confused.",YES
856,"Body: <p>**<strong>Edit:</strong> (10/26/13) More clear (hopefully) mini-rewrites added at the bottom**</p>

<p><em><strong>I'm asking this from a theoretical/general standpoint - not one that applies to a specific use case.</em></strong></p>

<p>I was thinking about this today:</p>

<p><strong>Assuming the data does not contain any measurement errors, if you're looking at a specific observation in your data and one of the measurements you recorded contains what could be considered an outlier, does this increase the probability (above that of the rest of the observations that do not contained measured outliers) that the same observation will contain another outlier in another measurement?</strong></p>

<p><em><strong>For my answer I'm looking for some sort of theorem, principle, etc. that states what I'm trying to communicate here much more elegantly.  For clearer explanations see Gino and Behacad's answers.</em></strong></p>

<p>Example: </p>

<p>Let's say you're measuring the height and circumference of a certain type of plant.  Each observation corresponds to 1 plant (you're only doing this once).</p>

<p>For height, you measure:</p>

<pre>
Obs 1  |   10 cm
Obs 2  |   9 cm
Obs 3  |   11 cm
Obs 4  |   22 cm
Obs 5  |   10 cm
Obs 6  |   9 cm
Obs 7  |   11 cm
Obs 8  |   10 cm
Obs 9  |   11 cm
Obs 10  |   9 cm
Obs 11  |   11 cm
Obs 12  |   10 cm
Obs 13  |   9 cm
Obs 14  |   10 cm
</pre>

<p>Since observation 4 contains what could be considered an outlier from the rest of the data, would the probability increase that measured circumference also contains an outlier for observation #4?</p>

<p>I understand my example may be too idealistic but I think it gets the point across...just change the measurements to anything.</p>

<h1><em><strong>Edited in attempts to make more clear:</em></strong></h1>

<p>(10/26/13)</p>

<p><strong>Version 1 Attempt of Abbreviated Question:</strong></p>

<blockquote>
  <p>In nature and in general, is there a tendency (even a weak one) that
  the <strong><em>probability is greater</em></strong> that the ""degree of variance from the
  mean in any attribute(s) of an observation"" will be <em>similar</em> to the
  ""degree of variance from the mean in any other* specific attribute of
  that same observation"" <strong><em>in comparison to the probability</em></strong> that it
  will INSTEAD be <em>more similar</em> to the ""degree of variance from the
  mean in that same* specific attribute of <em>any other observation</em>.""</p>
</blockquote>

<p><em>* next to a word means I was pairing what they reference.<br>
""Quotes"" used above mean nothing and are used simply to help section parts together/off for clarity.</em></p>

<p><strong>Version 2 Attempt of Abbreviated Question:</strong></p>

<blockquote>
  <p>In nature and in general, is variance from the mean across
  observations for one attribute¹ correlated (even with <em>extremely</em>
  loose correlation) to the variance from the mean across observations for all
  attributes¹?</p>
</blockquote>

<p><em>¹Attribute meaning measurement, quality, presence-of-either of these, and/or nearly anything else that the word ""attribute"" could even slightly represent as a word.  Include all synonyms of the word ""attribute"" as well.</em></p>
PostId: 73613",PostDate: 2013-10-23 18:17:15.0,Text: ^...assume you looked at all datapoints except for one just for this example's and explanation of my conjecture's sake.,NO
2474,"Body: <p>Because @zaynah posted in the comments that the data are thought to follow a Weibull distribution, I'm gonna provide a short tutorial on how to estimate the parameters of such a distribution using MLE (Maximum likelihood estimation). There is a <a href=""http://stats.stackexchange.com/questions/21745/how-can-i-perform-weibull-analysis-on-monthly-recorded-data-of-wind-speeds"">similar post</a> about wind speeds and Weibull distribution on the site.</p>

<ol>
<li><a href=""http://cran.r-project.org/"" rel=""nofollow"">Download and install <code>R</code></a>, it's free</li>
<li>Optional: <a href=""http://www.rstudio.com/"" rel=""nofollow"">Download and install RStudio</a>, which is a great IDE for R providing a ton of useful functions such as syntax highlighting and more.</li>
<li>Install the packages <code>MASS</code> and <code>car</code> by typing: <code>install.packages(c(""MASS"", ""car""))</code>. Load them by typing: <code>library(MASS)</code> and <code>library(car)</code>.</li>
<li><a href=""http://cran.r-project.org/doc/manuals/R-data.pdf"" rel=""nofollow"">Import your data into <code>R</code></a>. If you have your data in Excel, for example, save them as delimited text file (.txt) and import them in <code>R</code> with <code>read.table</code>.</li>
<li>Use the function <code>fitdistr</code> to calculate the maximum likelihood estimates of your weibull distribution: <code>fitdistr(my.data, densfun=""weibull"")</code>. To see a fully worked out example, see the link at the bottom of the answer.</li>
<li>Make a QQ-Plot to compare your data with a Weibull distribution with the scale and shape parameters estimated at point 5: <code>qqPlot(my.data, distribution=""weibull"", shape=, scale=)</code></li>
</ol>

<p>The <a href=""http://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf"" rel=""nofollow"">tutorial of Vito Ricci</a> on fitting distribution with <code>R</code> is a good starting point on the matter. And there are <a href=""http://stats.stackexchange.com/search?q=fit+distribution"">numerous posts</a> on this site on the subject (see <a href=""http://stats.stackexchange.com/questions/58220/what-distribution-does-my-data-follow"">this post</a> too).</p>

<p>To see a fully worked out example of how to use <code>fitdistr</code>, have a look at <a href=""http://stackoverflow.com/questions/15303310/how-to-create-a-weibull-cumulative-distribution-function-starting-from-fitdistr"">this post</a>.</p>

<p>Let's look at an example in <code>R</code>:</p>

<pre><code># Load packages

library(MASS)
library(car)

# First, we generate 1000 random numbers from a Weibull distribution with
# scale = 1 and shape = 1.5

rw &lt;- rweibull(1000, scale=1, shape=1.5)

# We can calculate a kernel density estimation to inspect the distribution
# Because the Weibull distribution has support [0,+Infinity), we are truncate
# the density at 0

par(bg=""white"", las=1, cex=1.1)
plot(density(rw, bw=0.5, cut=0), las=1, lwd=2,
xlim=c(0,5),col=""steelblue"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/iso1j.png"" alt=""Weibull KDE""></p>

<pre><code># Now, we can use fitdistr to calculate the parameters by MLE

fitdistr(rw, densfun=""weibull"")

     shape        scale   
  1.56788999   1.01431852 
 (0.03891863) (0.02153039)
</code></pre>

<p>The maximum likelihood estimates are close to those we arbitrarily set in the generation of the random numbers. Let's compare our data using a QQ-Plot with a hypothetical Weibull distribution with the parameters that we've estimated with <code>fitdistr</code>:</p>

<pre><code>qqPlot(rw, distribution=""weibull"", scale=1.014, shape=1.568, las=1, pch=19)
</code></pre>

<p><img src=""http://i.stack.imgur.com/PdfJk.png"" alt=""QQPlot""></p>

<p>The points are nicely aligned on the line and mostly within the 95%-confidence envelope. We would conclude that our data are compatible with a Weibull distribution. This was expected, of course, as we've sampled our values from a Weibull distribution. </p>

<hr>

<h2>Estimating the $k$ (shape) and $c$ (scale) of a Weibull distribution without MLE</h2>

<p><a href=""http://journals.ametsoc.org/doi/abs/10.1175/1520-0450%281978%29017%3C0350%3AMFEWSF%3E2.0.CO%3B2"" rel=""nofollow"">This paper</a> lists five methods to estimate the parameters of a Weibull distribution for wind speeds. I'm gonna explain three of them here.</p>

<h2>From means and standard deviation</h2>

<p>The shape parameter $k$ is estimated as:
$$
k=\\left(\\frac{\\hat{\\sigma}}{\\hat{v}}\\right)^{-1.086}
$$
and the scale parameter $c$ is estimated as:
$$
c=\\frac{\\hat{v}}{\\Gamma(1+1/k)}
$$
with $\\hat{v}$ is the mean wind speed and $\\hat{\\sigma}$ the standard deviation and $\\Gamma$ is the <a href=""http://en.wikipedia.org/wiki/Gamma_function0450%281978%29017%3C0350%3AMFEWSF%3E2.0.CO%3B2"" rel=""nofollow"">Gamma function</a>.</p>

<h2>Least-squares fit to observed distribution</h2>

<p>If the observed wind speeds are divided into $n$ speed interval $0-V_{1},V_{1}-V_{2},\\ldots, V_{n-1}-V_{n}$, having frequencies of occurrence $f_{1}, f_{2},\\ldots,f_{n}$ and cumulative frequencies $p_{1}=f_{1}, p_{2}=f_{1}+f_{2}, \\ldots, p_{n}=p_{n-1}+f_{n}$, then you can fit a linear regression of the form $y=a+bx$ to the values
$$
x_{i} = \\ln(V_{i})
$$
$$
y_{i} = \\ln[-\\ln(1-p_{i})]
$$
The Weibull parameters are related to the linear coefficients $a$ and $b$ by
$$
c=\\exp\\left(-\\frac{a}{b}\\right)
$$
$$
k=b
$$</p>

<h2>Median and quartile wind speeds</h2>

<p>If you don't have the complete observed wind speeds but the median $V_{m}$ and quartiles $V_{0.25}$ and $V_{0.75}$ $\\left[p(V\\leq V_{0.25})=0.25, p(V\\leq V_{0.75})=0.75\\right]$, then $c$ and $k$ can be computed by the relations
$$
k = \\ln\\left[\\ln(0.25)/\\ln(0.75)\\right]/\\ln(V_{0.75}/V_{0.25})\\approx 1.573/\\ln(V_{0.75}/V_{0.25})
$$
$$
c=V_{m}/\\ln(2)^{1/k}
$$</p>

<h2>Comparison of the four methods</h2>

<p>Here is an example in <code>R</code> comparing the four methods:</p>

<pre><code>library(MASS)  # for ""fitdistr""

set.seed(123)
#-----------------------------------------------------------------------------
# Generate 10000 random numbers from a Weibull distribution
# with shape = 1.5 and scale = 1
#-----------------------------------------------------------------------------

rw &lt;- rweibull(10000, shape=1.5, scale=1)

#-----------------------------------------------------------------------------
# 1. Estimate k and c by MLE
#-----------------------------------------------------------------------------

fitdistr(rw, densfun=""weibull"")
shape         scale   
1.515380298   1.005562356 

#-----------------------------------------------------------------------------
# 2. Estimate k and c using the leas square fit
#-----------------------------------------------------------------------------

n &lt;- 100 # number of bins
breaks &lt;- seq(0, max(rw), length.out=n)

freqs &lt;- as.vector(prop.table(table(cut(rw, breaks = breaks))))
cum.freqs &lt;- c(0, cumsum(freqs)) 

xi &lt;- log(breaks)
yi &lt;- log(-log(1-cum.freqs))

# Fit the linear regression
least.squares &lt;- lm(yi[is.finite(yi) &amp; is.finite(xi)]~xi[is.finite(yi) &amp; is.finite(xi)])
lin.mod.coef &lt;- coefficients(least.squares)

k &lt;- lin.mod.coef[2]
k
1.515115
c &lt;- exp(-lin.mod.coef[1]/lin.mod.coef[2])
c
1.006004

#-----------------------------------------------------------------------------
# 3. Estimate k and c using the median and quartiles
#-----------------------------------------------------------------------------

med &lt;- median(rw)
quarts &lt;- quantile(rw, c(0.25, 0.75))

k &lt;- log(log(0.25)/log(0.75))/log(quarts[2]/quarts[1])
k
1.537766
c &lt;- med/log(2)^(1/k)
c
1.004434

#-----------------------------------------------------------------------------
# 4. Estimate k and c using mean and standard deviation.
#-----------------------------------------------------------------------------

k &lt;- (sd(rw)/mean(rw))^(-1.086)
c &lt;- mean(rw)/(gamma(1+1/k))
k
1.535481
c
1.006938
</code></pre>

<p>All methods yield very similar results. The maximum likelihood approach has the advantage that the standard errors of the Weibull parameters are directly given.</p>

<hr>

<h2>Using bootstrap to add pointwise confidence intervals to the PDF or CDF</h2>

<p>We can use a the non-parametric bootstrap to construct pointwise confidence intervals around the PDF and CDF of the estimated Weibull distribution. Here's an <code>R</code> script:</p>

<pre><code>#-----------------------------------------------------------------------------
# 5. Bootstrapping the pointwise confidence intervals
#-----------------------------------------------------------------------------

set.seed(123)

rw.small &lt;- rweibull(100,shape=1.5, scale=1)

xs &lt;- seq(0, 5, len=500)


boot.pdf &lt;- sapply(1:1000, function(i) {
  xi &lt;- sample(rw.small, size=length(rw.small), replace=TRUE)
  MLE.est &lt;- suppressWarnings(fitdistr(xi, densfun=""weibull""))  
  dweibull(xs, shape=as.numeric(MLE.est[[1]][13]), scale=as.numeric(MLE.est[[1]][14]))
}
)

boot.cdf &lt;- sapply(1:1000, function(i) {
  xi &lt;- sample(rw.small, size=length(rw.small), replace=TRUE)
  MLE.est &lt;- suppressWarnings(fitdistr(xi, densfun=""weibull""))  
  pweibull(xs, shape=as.numeric(MLE.est[[1]][15]), scale=as.numeric(MLE.est[[1]][16]))
}
)   

#-----------------------------------------------------------------------------
# Plot PDF
#-----------------------------------------------------------------------------

par(bg=""white"", las=1, cex=1.2)
plot(xs, boot.pdf[, 1], type=""l"", col=rgb(.6, .6, .6, .1), ylim=range(boot.pdf),
     xlab=""x"", ylab=""Probability density"")
for(i in 2:ncol(boot.pdf)) lines(xs, boot.pdf[, i], col=rgb(.6, .6, .6, .1))

# Add pointwise confidence bands

quants &lt;- apply(boot.pdf, 1, quantile, c(0.025, 0.5, 0.975))
min.point &lt;- apply(boot.pdf, 1, min, na.rm=TRUE)
max.point &lt;- apply(boot.pdf, 1, max, na.rm=TRUE)
lines(xs, quants[1, ], col=""red"", lwd=1.5, lty=2)
lines(xs, quants[3, ], col=""red"", lwd=1.5, lty=2)
lines(xs, quants[2, ], col=""darkred"", lwd=2)
#lines(xs, min.point, col=""purple"")
#lines(xs, max.point, col=""purple"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/h1HU5.png"" alt=""Weibull PDF CIs""></p>

<pre><code>#-----------------------------------------------------------------------------
# Plot CDF
#-----------------------------------------------------------------------------

par(bg=""white"", las=1, cex=1.2)
plot(xs, boot.cdf[, 1], type=""l"", col=rgb(.6, .6, .6, .1), ylim=range(boot.cdf),
     xlab=""x"", ylab=""F(x)"")
for(i in 2:ncol(boot.cdf)) lines(xs, boot.cdf[, i], col=rgb(.6, .6, .6, .1))

# Add pointwise confidence bands

quants &lt;- apply(boot.cdf, 1, quantile, c(0.025, 0.5, 0.975))
min.point &lt;- apply(boot.cdf, 1, min, na.rm=TRUE)
max.point &lt;- apply(boot.cdf, 1, max, na.rm=TRUE)
lines(xs, quants[1, ], col=""red"", lwd=1.5, lty=2)
lines(xs, quants[3, ], col=""red"", lwd=1.5, lty=2)
lines(xs, quants[2, ], col=""darkred"", lwd=2)
lines(xs, min.point, col=""purple"")
lines(xs, max.point, col=""purple"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/cQDlv.png"" alt=""Weibull CDF CIs""></p>
PostId: 60530",PostDate: 2013-05-31 14:04:46.0,"Text: but @COOLSerdash there is no name  at all.. like you told me, i had only insert the numbers.. so mydata has only rows n columns of numbers..:(",YES
7532,"Body: <p>I want to generate a spatial data following multivariate Gaussian distribution. </p>

<p>However, I don't want it to be homogeneous, meaning I don't want the correlation/covariance to be homogeneous. I want it to be heterogeneous. </p>

<p>Any suggestions on how to generate such data?</p>

<p>Here is an example of what I mean by heterogeneous covariance matrix</p>

<pre><code>   25.8621   25.1207   24.6305   24.3867
   25.1207   25.3719   24.8768   24.6305
   24.6305   24.8768   25.3719   25.1207
   24.3867   24.6305   25.1207   25.8621
</code></pre>

<p>There are four variables. As you can see the marginal variance is different which means C(0) is dependent on location. So does C(h) for any h. This is one method of generating heterogeneous data.</p>

<p>But I want some spatial data consisting of like land, lakes which have very less covariance between each other.</p>
PostId: 66313",PostDate: 2013-08-02 01:22:48.0,Text: @EngrStudent Matlab,YES
13486,"Body: <p>This problem is special, in the following way.  Label the $2n-1$ non-chair women with the numbers $1, 2, \\ldots, 2n-1$ and label the $2n$ men with the numbers $1, 2, \\ldots, 2n$.  Any all-female committee is determined by the $n$ labels of its members. Corresponding to such a committee are <em>two</em> distinct all-male committees: the one of men having those labels and the one of the men <em>not</em> having those labels.  Furthermore, exactly one of those all-male committees includes the man with label $2n$: call this the ""second"" committee and let the other choice of all-male committee be the ""first"" committee.</p>

<p>Notice that any two distinct ""first"" committees correspond to distinct all-female committees, that any two distinct ""second"" committees correspond to distinct all-female committees, and no ""first"" committee can also be a ""second"" committee or conversely.</p>

<p>As an illustration, consider the case $n=2$.  The $\\binom{2n-1}{n}=3$ all-female committees and their $\\binom{2n}{n}=6$ all-male counterparts are</p>

<p>$$\\eqalign {
\\text{Women} &amp;  &amp; \\text{First men}; &amp; \\text{Second men}\\\\
\\{1,2\\} &amp;\\to &amp;\\{1,2\\}; &amp; \\{3,4\\}\\\\
\\{1,3\\} &amp;\\to &amp;\\{1,3\\}; &amp; \\{2,4\\}\\\\
\\{2,3\\} &amp;\\to &amp;\\{2,3\\}; &amp; \\{1,4\\}.
}$$</p>

<p>Therefore, there are exactly two possible all-male committees for each all-female committee.  If in fact all members of the committee are of the same sex, then it must be exactly twice as likely that they are male than that they are female.  <strong>The answer does not depend on $n$!</strong></p>
PostId: 70151",PostDate: 2013-09-16 16:00:13.0,"Text: @whuber: We don't have to count in the way you suggest correct? The point is that for each all female committee there are $2$ distinct all male committees? So for $n=2$, we could have done the counting as e.g.: $\\{1,2 \\} \\to \\{1,3 \\}, \\{2,4 \\}$, $\\{1,3 \\} \\to \\{1,4 \\}, \\{2,3\\}$, and $\\{2,3\\} \\to \\{2,4 \\}, \\{1,3 \\}$?",YES
10395,"Body: <p>Luckily someone has posted some <a href=""http://www.panix.com/~murphy/bdata.txt"">genuine birthday data</a> with a <a href=""http://www.panix.com/~murphy/bday.html"">bit of discussion</a> of a related question (is the distribution uniform).  We can use this and resampling to show that the answer to your question is apparently 23 - the same as the <a href=""http://en.wikipedia.org/wiki/Birthday_problem"">theoretical answer</a>.</p>

<pre><code>&gt; x &lt;- read.table(""bdata.txt"", header=T)
&gt; birthday &lt;- data.frame(date=as.factor(x$date), count=x$count)
&gt; summary(birthday) 
      date         count     
 101    :  1   Min.   : 325  
 102    :  1   1st Qu.:1266  
 103    :  1   Median :1310  
 104    :  1   Mean   :1314  
 105    :  1   3rd Qu.:1362  
 106    :  1   Max.   :1559  
 (Other):360                 
&gt; results &lt;- rep(0,50)
&gt; reps &lt;-2000 # big number needed as there is some instability otherwise
&gt; for (i in 1:50)
+ {
+ count &lt;- 0
+ for (j in 1:reps)
+ {
+ samp &lt;- sample(birthday$date, i, replace=T, prob=birthday$count)
+ count &lt;- count + 1*(max(table(samp))&gt;1)
+ }
+ results[i] &lt;- count/reps
+ }
&gt; results
 [1] 0.0000 0.0045 0.0095 0.0220 0.0210 0.0395 0.0570 0.0835 0.0890 0.1165
[11] 0.1480 0.1770 0.1955 0.2265 0.2490 0.2735 0.3105 0.3350 0.3910 0.4165
[21] 0.4690 0.4560 0.5210 0.5310 0.5745 0.5975 0.6240 0.6430 0.6950 0.7015
[31] 0.7285 0.7510 0.7690 0.8025 0.8225 0.8280 0.8525 0.8645 0.8685 0.8830
[41] 0.8965 0.9020 0.9240 0.9435 0.9350 0.9465 0.9545 0.9655 0.9600 0.9665
</code></pre>
PostId: 22012",PostDate: 2012-01-31 10:15:50.0,"Text: @Xi'an, check this out and see what you think: `table(replicate(10^5, max(tabulate(sample(1:365,360,rep=TRUE)))))`.",YES
1842,"Body: <p>All models are imperfect representations of reality: the more data you have, the better able you are to detect their imperfections and to take them into account by building better models. So you should <em>expect</em> any kind of goodness-of-fit test to become significant when you increase the sample size enough. You have the choice of deciding that the model performs well enough as it is or of making it more complex to accommodate those previously indiscernible discrepancies.</p>

<p>In this case you might want to first examine carefully the extra 2,000 observations to look for outliers, change-points, &amp;c., then try a model with more GARCH/ARMA parameters as indicated by the auto-correlation functions.</p>
PostId: 78009",PostDate: 2013-11-28 21:32:15.0,"Text: (1) If your fitting algorithm isn't converging that's likely your main problem. (2) For a GARCH model, or any other, to perform well it must make good predictions. Statistically significant correlation in the standardized squared residuals may be tolerable if it's tiny. If you want to improve the model look for signs of mis-specification & consider more parameters for the GARCH part.",YES
11111,"Body: <p>Periodic Table for 100, Alex.  I don't have code for it, though.  :(</p>

<p>One might think that a ""periodic table"" package might already exist in CRAN.  The idea of a coloring scheme and layout of such data could be interesting and useful.</p>

<p>These could be colored by package and sorted vertically by frequency, e.g. in a sample of code on CRAN or as they appear in one's local codebase.</p>
PostId: 14001",PostDate: 2011-08-08 19:26:48.0,"Text: @cbeleites: I'm wary of overthinking the analogy.  :)  Just having a periodic table capability would be nice.  I could see groupings based on source (i.e. package), vertical ordering based on priority within the package, and horizontal ordering based on package name or even priority of the packages (with ""base"" getting a very prominent position on the left or right).",NO
11334,"Body: <p>Say I have multivariate normal $N(\\mu, \\Sigma)$ density.  I want to get the second (partial) derivative w.r.t. $\\mu$.  Not sure how to take derivative of a matrix.</p>

<p>Wiki says take the derivative element by element inside the matrix.  </p>

<p>I am working with Laplace approximation 
$$\\log{P}_{N}(\\theta)=\\log {P}_{N}-\\frac{1}{2}{(\\theta-\\hat{\\theta})}^{T}{\\Sigma}^{-1}(\\theta-\\hat{\\theta}) \\&gt;.$$<br>
The mode is $\\hat\\theta=\\mu$.</p>

<p>I was given $${\\Sigma}^{-1}=-\\frac{{{\\partial }^{2}}}{\\partial {{\\theta }^{2}}}\\log p(\\hat{\\theta }|y),$$ how did this come about?</p>

<p>What I have done:<br>
$$\\log P(\\theta|y)=-\\frac{k}{2}\\log 2\\pi-\\frac{1}{2} \\left| \\Sigma \\right|-\\frac{1}{2}{(\\theta-\\hat \\theta)}^{T}{\\Sigma}^{-1}(\\theta-\\hat\\theta)$$</p>

<p>So, I take derivative w.r.t to $\\theta$, first off, there is a transpose, secondly, it is a matrix.  So, I am stuck.</p>

<p>Note: If my professor comes across this, I am referring to the lecture.</p>
PostId: 27436",PostDate: 2012-05-01 03:24:17.0,"Text: @user As I pointed out, the second derivative of the logarithm *must* have non-positive eigenvalues.  Yes, there are links between variances and negative second partial derivatives, as the theory of maximum likelihood estimation, Fisher information, etc., reveals--Macro has referred to that earlier in these comments.",YES
9075,"Body: <p>If you're asking about where the point of inflexion is on the cubic, it's where $\\frac{d^2y}{dx^2}=6ax+2b$ is zero, which occurs when $x = -b/3a$.</p>

<p><img src=""http://i.stack.imgur.com/TNhht.png"" alt=""enter image description here""></p>

<p>However a fitted model doesn't give you the exact (population) values of $a$ and $b$, just noisy estimates, so you are left to estimate that point. </p>

<p>This is a very similar problem to finding the turning point on a quadratic fit, which has been discussed in a number of posts here. Some of these posts will have some useful information, particularly if you want a confidence interval for the point of inflexion.</p>

<p>For example, see <a href=""http://stats.stackexchange.com/questions/79967/estimating-95-confidence-interval-for-vertex-of-quadratic-fit/"">here</a> and a related question <a href=""http://stats.stackexchange.com/questions/40602/can-i-interpret-the-inclusion-of-a-quadratic-term-in-logistic-regression-as-indi"">here</a>.</p>

<p>Finding the turning point in a logistic model is another common problem on CV which has a number of relevant posts. These can be found with a suitable search.</p>

<p>--</p>

<p>It's not clear that cubics are a good choice of model for this situation, unless you really expect the relationship to reach a maximum and then come back down again as Strain increases further:</p>

<p><img src=""http://i.stack.imgur.com/THOXg.png"" alt=""enter image description here""></p>

<p>If you don't expect that, I'd suggest considering functions that more nearly accord with the physical expectations of the behaviour.</p>

<p>(Perhaps the aforementioned logistic model may be of some relevance to you, I don't know.)</p>

<hr>

<p>Response to updated question:</p>

<blockquote>
  <p>The figure shows that there is a linear relationship at the beginning, but at some point (point of deviation from linearity), the linear relationship disappears and non-linear relationship prevails. I'm looking for a way to determine this point of deviation from linearity quantitatively, not as done by visual inspection. How do I do that?</p>
</blockquote>

<p>If you fit, say a nonlinear logistic model, the inflexion point in the logistic is a parameter of the model. If you fit some other model it will depend on the model.</p>

<hr>

<p>If you really think it's linear for an initial period (rather than just close to linear), you might fit a function that is literally linear and then joins up to a function that has no curvature at the join point, and the same slope, akin to the left end of a natural cubic spline where the knot is a parameter, but instead with a function on the right that flattens out/asymptotes.</p>
PostId: 97459",PostDate: 2014-05-13 06:07:26.0,"Text: @orthopolis If the relationship doesn't come back down, but either reaches or simply approaches a maximum, you should fit a curve that behaves that way. There are many such curves; I mentioned the logistic curve, which is widely used, but there are other choices depending on your needs. Please see my additional information relating to the cubic in my answer.",YES
